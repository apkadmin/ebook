<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Chương 10 (giai đoạn 2) - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/emojify.js/1.1.0/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif;padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;padding:0 15px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc-label{opacity:.3;background-color:#ccc;border:none;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child > ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}.ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}.markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}.ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}.markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}.ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled"><h1 id="Chương-10-giai-đoạn-2"><a class="anchor hidden-xs" href="#Chương-10-giai-đoạn-2" title="Chương-10-giai-đoạn-2"><span class="octicon octicon-link"></span></a>Chương 10 (giai đoạn 2)</h1><p><a href="https://www.deeplearningbook.org/contents/rnn.html" target="_blank" rel="noopener">https://www.deeplearningbook.org/contents/rnn.html</a></p><h1 id="105-Mạng-neuron-truy-hồi-sâu"><a class="anchor hidden-xs" href="#105-Mạng-neuron-truy-hồi-sâu" title="105-Mạng-neuron-truy-hồi-sâu"><span class="octicon octicon-link"></span></a>10.5 Mạng neuron truy hồi sâu</h1><p>Quá trình tính toán trong hầu hết các mạng neuron truy hồi có thể được phân tách thành ba khối tham số và các phép biến đổi tương ứng như sau:</p><ol>
<li>Từ đầu vào tới trạng thái ẩn.</li>
<li>Từ trạng thái ẩn trước tới trạng thái ẩn tiếp theo.</li>
<li>Từ trạng thái ẩn tới đầu ra.</li>
</ol><p><img src="https://i.imgur.com/OsHw6Kq.png" alt="" width="450"></p><blockquote>
<p>Hình 10.13: Một mạng neuron truy hồi có thể được làm sâu thêm theo nhiều cách [Pascanu et al., 2014a]. (a) Trạng thái ẩn truy hồi có thể được chia thành các nhóm được tổ chức theo thứ bậc. (b) Cơ chế tính toán sâu hơn (ví dụ: mạng perceptron đa tầng) có thể được áp dụng cho các khối đầu vào tới trạng thái ẩn, trạng thái ẩn tới trạng thái ẩn, và trạng thái ẩn tới đầu ra, kéo dài đường đi ngắn nhất liên kết các bước thời gian với nhau. © Ta có thể giảm thiểu ảnh hưởng từ các liên kết quá dài bằng cách đưa vào các kết nối nhảy cóc (skip connection).</p>
</blockquote><p>Với kiến trúc mạng neuron truy hồi ở hình 10.13, mỗi trong số ba khối này được liên kết với một ma trận trọng số. Nói cách khác, khi một mạng neuron truy hồi được trải dài, mỗi khối này tương ứng với một phép biến đổi nông, theo nghĩa là một phép biến đổi được biểu diễn bằng một tầng duy nhất trong mạng perceptron đa tầng sâu. Thông thường, đây là một phép biến đổi được biểu diễn bằng một biến đổi affin mà mô hình học được, và một đơn vị phi tuyến cố định tiếp sau đó.</p><p>Một câu hỏi được đặt ra: Tăng độ sâu trong các phép biến đổi liệu có mang lại lợi ích? Các kết quả thực nghiệm [Graves et al., 2013; Pascanu et al., 2014a] khẳng định mạnh mẽ điều đó. Thực nghiệm đã chỉ ra rằng chúng ta cần một độ sâu đủ để thực hiện các ánh xạ cần thiết. Xem thêm [Schmidhuber, 1992], [El Hihiand Bengio, 1996], hoặc [Jaeger, 2007a] về những nghiên cứu trước đây về các mạng truy hồi sâu.</p><p>Graves và cộng sự (2013) là nhóm nghiên cứu đầu tiên cho thấy lợi ích đáng kể khi phân tách trạng thái của mạng neuron truy hồi thành nhiều tầng, như trong hình 10.13a. Ta có thể coi các tầng thấp hơn trong cấu trúc phân cấp được mô tả trong hình 10.13a đóng vai trò biến đổi đầu vào thô ban đầu thành một biểu diễn thích hợp hơn, ở các mức độ cao hơn của trạng thái ẩn. Pascanu và cộng sự (2014a) tiến thêm một bước nữa khi đề xuất sử dụng một perceptron đa tầng riêng biệt (có thể sâu) cho mỗi trong số ba khối tham số trên (như minh hoạ trong hình 10.13b). Một điều cần lưu ý là khi ta muốn tăng khả năng biểu diễn cho mỗi khối tham số bằng cách tăng chiều sâu, nó có thể gây tổn thương cho việc học do quá trình tối ưu hoá khi đó trở nên khó khăn hơn. Nói chung, các kiến trúc nông sẽ dễ tối ưu hơn, trong khi cách bổ sung chiều sâu như trong hình 10.13b lại kéo dài đường đi ngắn nhất từ một biến tại bước thời gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">t</span></span></span><script type="math/tex" id="MathJax-Element-1">t</script></span> tới một biến tại bước thời gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-3"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">t</span><span class="MJXp-mo" id="MJXp-Span-5" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mn" id="MJXp-Span-6">1</span></span></span><script type="math/tex" id="MathJax-Element-2">t + 1</script></span>. Chẳng hạn như nếu quá trình chuyển trạng thái sử dụng một perceptron đa tầng với một tầng ẩn duy nhất, chúng ta đã tăng gấp đôi chiều dài của đường đi ngắn nhất giữa các biến trong hai bước thời gian khác nhau so với mạng neuron truy hồi thông thường ở hình 10.3. Tuy nhiên, theo lập luận của Pascanu và cộng sự (2014a), điều này có thể được giảm thiểu bằng cách áp dụng các kết nối nhảy cóc vào đường đi từ <em>trạng thái ẩn tới trạng thái ẩn</em> tiếp theo, như minh họa trong hình 10.13c.</p><h1 id="106-Các-mạng-neuron-đệ-quy"><a class="anchor hidden-xs" href="#106-Các-mạng-neuron-đệ-quy" title="106-Các-mạng-neuron-đệ-quy"><span class="octicon octicon-link"></span></a>10.6 Các mạng neuron đệ quy</h1><p>Các mạng neuron đệ quy đại diện cho một cách khái quát hóa khác của mạng neuron truy hồi, với một loại đồ thị tính toán khác, có cấu trúc như một cây sâu (deep tree), thay vì giống như chuỗi của mạng neuron truy hồi. Đồ thị tính toán điển hình cho mạng neuron đệ quy được minh họa trong hình 10.14. Mạng neuron đệ quy được đề xuất bởi Pollack (1990), và Bottou (2011) đã mô tả tiềm năng của chúng trong việc học để suy luận. Các mạng đệ quy đã được áp dụng thành công để xử lý các cấu trúc dữ liệu là đầu vào cho mạng neuron [Frasconi et al., 1997, 1998], trong xử lý ngôn ngữ tự nhiên [Socher et al., 2011a,c, 2013a], cũng như trong thị giác máy tính [Socher et al., 2011b].</p><p>Một lợi thế rõ ràng của mạng đệ quy so với mạng truy hồi đó là với một chuỗi có cùng độ dài <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-7"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">τ</span></span></span><script type="math/tex" id="MathJax-Element-3">\tau</script></span>, độ sâu (được đo bằng số phép hợp thành các biến đổi phi tuyến) có thể được giảm đáng kể từ \tau xuống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-9"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-10">O</span><span class="MJXp-mo" id="MJXp-Span-11" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi" id="MJXp-Span-12">log</span><span class="MJXp-mo" id="MJXp-Span-13" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">τ</span><span class="MJXp-mo" id="MJXp-Span-15" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-4">O(\log \tau)</script></span>, điều này có thể giúp giải quyết vấn đề phụ thuộc dài hạn. Một câu hỏi mở được đặt ra: làm thế nào để cấu trúc cây một cách tốt nhất? Một lựa chọn đó là một cấu trúc cây không phụ thuộc vào dữ liệu, chẳng hạn như cây nhị phân cân bằng (balanced binary tree). Trong một số lĩnh vực ứng dụng, các phương pháp bên ngoài có thể đề xuất cấu trúc cây thích hợp cho từng bài toán cụ thể. Ví dụ, khi xử lý các câu ngôn ngữ tự nhiên, cấu trúc cây của mạng đệ quy có thể được cố định thành cấu trúc cây cú pháp của câu, sinh bởi bởi trình phân tích cú pháp ngôn ngữ tự nhiên [Socher et al., 2011a, 2013a]. Trong trường hợp lý tưởng nhất, chúng ta sẽ muốn chương trình học tự khám phá và suy ra cấu trúc cây thích hợp cho bất kỳ đầu vào cụ thể nào, như đề xuất của [Bottou, 2011].</p><blockquote>
<p><em><strong>ND</strong></em>: vấn đề phụ thuộc dài hạn (long-term dependencies) là một vấn đề khó trong xử lý ngôn ngữ tự nhiên. Một ví dụ đơn giản là vấn đề hỏi-đáp với đầu vào là 1 văn bản và 1 câu hỏi. Nhiệm vụ của hệ thống AI cần đọc hiểu văn bản và đưa ra đáp án cho câu hỏi. Chẳng hạn văn bản đầu vào là <code>"Tintin cầm quả bóng đi vào phòng khách. Cậu ấy ngồi xuống ghế, đặt quả bóng xuống, và đi ăn cơm"</code>. Câu hỏi AI cần trả lời là <code>"quả bóng đang ở phòng nào?"</code>. Nếu là người bình thường hiểu tiếng Việt thì vấn đề đọc và trả lời câu hỏi trên không khó. Nhưng với AI, ta cần có cơ chế giúp AI lưu và liên kết thông tin từ <code>Tintin</code> tới <code>cậu ấy</code> và <code>đặt quả bóng</code>. Chú ý là bạn có thể làm cho sự phụ thuộc này <code>dài hạn</code> hơn bằng cách có nhiều hành động giữa <code>Tintin</code> và <code>đặt quả bóng</code>. Như thế vấn đề sẽ càng khó giải quyết hơn đối với AI nhưng với người bình thường, việc này không gây cản trở gì lớn.</p>
</blockquote><p><img src="https://i.imgur.com/Hj84a89.png" alt="" width="350"></p><blockquote>
<p>Hình 10.14: Một mạng đệ quy có đồ thị tính toán khái quát hoá đồ thị tính toán của mạng neuron truy hồi từ một chuỗi tính toán thành một cây tính toán. Một chuỗi có độ dài biến thiên <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-16"><span class="MJXp-msubsup" id="MJXp-Span-17"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-18" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-19" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-20">(</span><span class="MJXp-mn" id="MJXp-Span-21">1</span><span class="MJXp-mo" id="MJXp-Span-22">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-24"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-25" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-26" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-27">(</span><span class="MJXp-mn" id="MJXp-Span-28">2</span><span class="MJXp-mo" id="MJXp-Span-29">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-30" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-31" style="margin-left: 0em; margin-right: 0em;">…</span><span class="MJXp-mo" id="MJXp-Span-32" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-33"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-34" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-35" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-36">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37">t</span><span class="MJXp-mo" id="MJXp-Span-38">)</span></span></span></span></span><script type="math/tex" id="MathJax-Element-5">\boldsymbol x^{(1)},\boldsymbol x^{(2)}, \dots , \boldsymbol x^{(t)}</script></span> có thể được ánh xạ thành một biểu diễn có kích thước cố định (đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-39"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-40">o</span></span></span><script type="math/tex" id="MathJax-Element-6">\boldsymbol o</script></span>), với một tập các tham số cố định (các ma trận trọng số <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-41"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-42">U</span><span class="MJXp-mo" id="MJXp-Span-43" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-44">V</span><span class="MJXp-mo" id="MJXp-Span-45" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-46">W</span></span></span><script type="math/tex" id="MathJax-Element-7">\boldsymbol U,\boldsymbol V,\boldsymbol W</script></span>). Hình trên minh họa một trường hợp học giám sát trong đó nhãn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-47"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-48">y</span></span></span><script type="math/tex" id="MathJax-Element-8">\boldsymbol y</script></span> tương ứng với toàn bộ chuỗi đầu vào.</p>
</blockquote><p>any variants of the recursive net idea are possible. For example, Frasconiet al. (1997) and Frasconi et al. (1998) associate the data with a tree structure,and associate the inputs and targets with individual nodes of the tree. Thecomputation performed by each node does not have to be the traditional artiﬁcialneuron computation (aﬃne transformation of all inputs followed by a monotonenonlinearity). For example, Socher et al. (2013a) propose using tensor operationsand bilinear forms, which have previously been found useful to model relationshipsbetween concepts (Weston et al., 2010; Bordes et al., 2012) when the concepts arerepresented by continuous vectors (embeddings)</p><p>Có thể có nhiều biến thể của mạng đệ quy. Ví dụ, Frasconi và đồng nghiệp (1997, 1998) kết hợp dữ liệu với cấu trúc cây, và kết hợp các đầu vào và nhãn với các nút riêng lẻ của cây. Việc tính toán được thực hiện trên mỗi nút không nhất thiết là các neuron tính toán truyền thống (phép biến đổi affin của tất cả các đầu vào theo sau bởi một đơn vị phi tuyến đơn điệu). Ví dụ, Socher và đồng nghiệp (2013a) đề xuất sử dụng các phép toán tensor và các dạng song tuyến, trước đây đã được chứng minh là có ích trong việc mô hình hóa các mối quan hệ giữa các khái niệm [Weston et al., 2010; Bordes et al., 2012] khi các khái niệm được biểu diễn bằng các vector liên tục (dạng nhúng).</p><h1 id="107-Thách-thức-của-phụ-thuộc-dài-hạn"><a class="anchor hidden-xs" href="#107-Thách-thức-của-phụ-thuộc-dài-hạn" title="107-Thách-thức-của-phụ-thuộc-dài-hạn"><span class="octicon octicon-link"></span></a>10.7 Thách thức của phụ thuộc dài hạn</h1><p>Thách thức về mặt toán học của phụ thuộc dài hạn trong các mạng truy hồi đã được giới thiệu trong phần 8.2.5. Vấn đề cơ bản là các giá trị gradient sau khi lan truyền qua nhiều giai đoạn có xu hướng hoặc tiêu biến (đa phần) hoặc bùng nổ (hiếm khi, nhưng lại gây ảnh hướng lớn đến quá trình tối ưu hóa). Ngay cả khi ta giả định rằng các tham số là bộ tham số giúp mạng truy hồi có tính ổn định (có thể lưu trữ các ký ức, với các giá trị gradient không bùng nổ), khó khăn trong xử lý phụ thuộc dài hạn phát sinh từ các trọng số nhỏ dần theo cấp số nhân gán cho  các tương tác dài hạn (liên quan tới phép nhân của nhiều ma trận Jacobi). Nhiều nghiên cứu đã cung cấp các giải pháp sâu hơn [Hochreiter, 1991; Doya, 1993; Bengio et al., 1994; Pascanu et al., 2013]. Trong phần này, ta sẽ mổ xẻ vấn đề một cách chi tiết hơn. Những phần còn lại sẽ mô tả các hướng tiếp cận để khắc phục vấn đề.</p><p>Các mạng truy hồi là phép hợp của cùng một hàm được lặp lại nhiều lần tại mỗi bước thời gian. Các phép hợp này có thể dẫn đến hành vi cực kì phi tuyến, minh họa trong hình 10.15.</p><p>Cụ thể, phép hợp các hàm số mà các mạng neuron truy hồi sử dụng tương đồng với phép nhân ma trận ở một mức độ nào đó. Ta có thể hình dung mối quan hệ truy hồi</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-49"><span class="MJXp-mtable" id="MJXp-Span-50"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-51" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-52" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-53"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-54" style="margin-right: 0.05em;">h</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-55" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-56">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57">t</span><span class="MJXp-mo" id="MJXp-Span-58">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-59" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-60"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-61" style="margin-right: 0.05em;">W</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-62" style="vertical-align: 0.5em;"><span class="MJXp-mi" id="MJXp-Span-63">⊤</span></span></span><span class="MJXp-msubsup" id="MJXp-Span-64"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-65" style="margin-right: 0.05em;">h</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-66" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-67">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68">t</span><span class="MJXp-mo" id="MJXp-Span-69">−</span><span class="MJXp-mn" id="MJXp-Span-70">1</span><span class="MJXp-mo" id="MJXp-Span-71">)</span></span></span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-9"> \boldsymbol h^{(t)} =\boldsymbol W^{\top} \boldsymbol h^{(t-1)} \tag{10.36}</script></span></p><p>như là một mạng neuron truy hồi rất đơn giản, không có các hàm kích hoạt phi tuyến và đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-72"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-73">x</span></span></span><script type="math/tex" id="MathJax-Element-10">\boldsymbol x</script></span>. Như đã thảo luận trong phần 8.2.5, về cơ bản, mối quan hệ truy hồi này mô tả  phương pháp luỹ thừa (power method). Nó có thể được đơn giản hóa thành</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-74"><span class="MJXp-mtable" id="MJXp-Span-75"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-76" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-77" style="text-align: center;"><span class="MJXp-msubsup" id="MJXp-Span-78"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-79" style="margin-right: 0.05em;">h</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-80" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-81">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82">t</span><span class="MJXp-mo" id="MJXp-Span-83">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-84" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo MJXp-bold" id="MJXp-Span-85" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-86"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87" style="margin-right: 0.05em;">W</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-88" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">t</span></span></span><span class="MJXp-msubsup" id="MJXp-Span-90"><span class="MJXp-mo" id="MJXp-Span-91" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-92" style="vertical-align: 0.5em;"><span class="MJXp-mi" id="MJXp-Span-93">⊤</span></span></span><span class="MJXp-msubsup" id="MJXp-Span-94"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-95" style="margin-right: 0.05em;">h</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-96" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-97">(</span><span class="MJXp-mn" id="MJXp-Span-98">0</span><span class="MJXp-mo" id="MJXp-Span-99">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-100" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-11">\boldsymbol h^{(t)} =\boldsymbol (W^{t})^{\top} \boldsymbol h^{(0)},\tag{10.37}</script></span></p><p>và nếu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-101"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-102">W</span></span></span><script type="math/tex" id="MathJax-Element-12">\boldsymbol W</script></span> có thể được chéo hóa thành dạng</p><p><span class="mathjax"><span class="MathJax_Preview">\boldsymbol W=\boldsymbol QΛ\boldsymbol Q ^{\top}
\tag{10.38}</span><script type="math/tex; mode=display">\boldsymbol W=\boldsymbol QΛ\boldsymbol Q ^{\top}
\tag{10.38}</script></span></p><p>với <span class="mathjax"><span class="MathJax_Preview">\boldsymbol Q</span><script type="math/tex">\boldsymbol Q</script></span> là ma trận trực giao, công thức truy hồi có thể được đơn giản hóa hơn nữa thành</p><p><span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}=\boldsymbol Q^{\top} \boldsymbolΛ^{t} \boldsymbol Q \boldsymbol h^{(0)}.
\tag{10.39}</span><script type="math/tex; mode=display">\boldsymbol h^{(t)}=\boldsymbol Q^{\top} \boldsymbolΛ^{t} \boldsymbol Q \boldsymbol h^{(0)}.
\tag{10.39}</script></span></p><p>Các trị riêng tăng theo lũy thừa bậc <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>, khiến các trị riêng với độ lớn nhỏ hơn <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> sẽ giảm dần về <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> và những trị riêng với độ lớn lớn hơn 1 sẽ tăng đến vô cùng. Bất kỳ phần tử nào của <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(0)}</span><script type="math/tex">\boldsymbol h^{(0)}</script></span> không được liên kết với vector riêng lớn nhất cuối cùng sẽ bị loại bỏ.</p><p><img src="https://i.imgur.com/AWJa2wx.png" alt=""></p><blockquote>
<p>Hình 10.15: Thực hiện các phép hợp hàm số nhiều lần. Khi hợp thành nhiều hàm phi tuyến (như tầng tanh-tuyến tính trong hình trên), kết quả thu được sẽ có tính phi tuyến rất mạnh, điển hình là với hầu hết các giá trị ứng với đạo hàm nhỏ, một vài giá trị ứng với đạo hàm lớn, và nhiều sự thay đổi tăng-giảm luân phiên nhau. Trong hình này, chúng tôi biểu diễn một phép chiếu tuyến tính của của một trạng thái ẩn <span class="mathjax"><span class="MathJax_Preview">100</span><script type="math/tex">100</script></span> chiều xuống <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> chiều duy nhất, thể hiện trên trục <span class="mathjax"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>. Trục <span class="mathjax"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> là tọa độ của trạng thái ban đầu dọc theo một hướng ngẫu nhiên trong không gian <span class="mathjax"><span class="MathJax_Preview">100</span><script type="math/tex">100</script></span> chiều. Do đó chúng ta có thể xem đồ thị này như một mặt cắt tuyến tính của một hàm số đa chiều. Các đường với màu sắc khác nhau thể hiện hàm số sau mỗi bước thời gian, hay nói cách khác là sau một số lần hàm chuyển trạng thái được hợp lại.</p>
</blockquote><blockquote>
<p><em><strong>ND</strong></em>: phương pháp luỹ thừa (power method) là một phương pháp rất phổ biến để tìm các trị riêng và vector riêng của ma trận.</p>
</blockquote><p>Đây là một vấn đề riêng biệt của mạng neuron truy hồi. Trong trường hợp vô hướng, hãy tưởng tượng khi nhân nhiều lần trọng số <span class="mathjax"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> với chính nó. Kết quả <span class="mathjax"><span class="MathJax_Preview">w^{t}</span><script type="math/tex">w^{t}</script></span> sẽ tiêu biến hoặc bùng nổ, tùy thuộc vào độ lớn của <span class="mathjax"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>. Tuy nhiên, nếu ta tạo ra một mạng không truy hồi có trọng số <span class="mathjax"><span class="MathJax_Preview">w^{(t)}</span><script type="math/tex">w^{(t)}</script></span> thay đổi ở mỗi bước, mọi chuyện sẽ khác. Khi đó, nếu trạng thái khởi tạo là <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> thì trạng thái ở thời điểm <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> sẽ là <span class="mathjax"><span class="MathJax_Preview">\prod_t w^{(t)}</span><script type="math/tex">\prod_t w^{(t)}</script></span>. Giả sử rằng các giá trị <span class="mathjax"><span class="MathJax_Preview">w^{(t)}</span><script type="math/tex">w^{(t)}</script></span> được sinh ra ngẫu nhiên, độc lập với nhau, với trung bình bằng <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> và phương sai <span class="mathjax"><span class="MathJax_Preview">\upsilon</span><script type="math/tex">\upsilon</script></span>. Phương sai của phép nhân sẽ là <span class="mathjax"><span class="MathJax_Preview">O(\upsilon^{n})</span><script type="math/tex">O(\upsilon^{n})</script></span>. Để đạt được phương sai mong muốn <span class="mathjax"><span class="MathJax_Preview">\upsilon^{*}</span><script type="math/tex">\upsilon^{*}</script></span>, chúng ta có thể chọn những trọng số đặc biệt có phương sai <span class="mathjax"><span class="MathJax_Preview">\upsilon=\sqrt[n]{\upsilon^{*}}</span><script type="math/tex">\upsilon=\sqrt[n]{\upsilon^{*}}</script></span>. Do đó, các mạng lan truyền thuận cực sâu với tỷ lệ được lựa chọn cẩn thận có thể tránh được vấn đề tiêu biến hoặc bùng nổ, như Sussillo (2014) đã đưa ra.</p><p>Vấn đề tiêu biến hoặc bùng nổ gradient trong mạng neuron truy hồi được phát hiện một cách độc lập bởi các nhóm nghiên cứu khác nhau [Hochreiter (1991), Bengio et al., (1993, 1994)]. Người ta hy vọng có thể tránh được vấn đề này bằng một cách đơn giản là chỉ thực hiện trong vùng không gian tham số mà các gradient không tiêu biến hoặc bùng nổ. Thật không may, để lưu trữ ký ức mà không bị ảnh hưởng bởi các nhiễu loạn nhỏ, mạng neuron truy hồi phải sử dụng tham số trong vùng mà gradient có thể tiêu biến [Bengio et al., 1993, 1994]. Cụ thể hơn, khi mô hình có thể biểu diễn các phụ thuộc dài hạn, gradient của một tương tác dài hạn có độ lớn nhỏ hơn theo cấp số mũ so với độ lớn của gradient của một tương tác ngắn hạn. Nói như vậy không có nghĩa là không thể học được những phụ thuộc dài hạn, nhưng nó có thể sẽ tốn rất nhiều thời gian để học bởi vì tín hiệu về những phụ thuộc này sẽ có xu hướng bị lu mờ trước những biến động dù là nhỏ nhất phát sinh từ các phụ thuộc ngắn hạn. Trong thực tế, những thực nghiệm của Bengio và cộng sự (1994) chỉ ra rằng: khi chúng ta tăng không gian phụ thuộc cần nắm bắt, quá trình tối ưu dựa trên gradient sẽ trở nên khó khăn hơn, với xác suất của việc huấn luyện thành công mạng neuron truy hồi truyền thống bằng thuật toán trượt gradient ngẫu nhiên nhanh chóng tiến về <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> đối với các chuỗi có độ dài chỉ <span class="mathjax"><span class="MathJax_Preview">10</span><script type="math/tex">10</script></span> hoặc <span class="mathjax"><span class="MathJax_Preview">20</span><script type="math/tex">20</script></span>.</p><p>Để tham khảo các nghiên cứu sâu hơn về các mạng truy hồi như dưới góc nhìn các hệ thống động học, xem thêm [Doya, 1993; Bengio et al., 1994; Siegelmann and Sontag, 1995], và bài điểm lại các công trình liên quan của Pascanu và cộng sự (2013). Các phần còn lại của chương này sẽ thảo luận về các cách tiếp cận khác nhau đã được đề xuất để giảm thiểu khó khăn trong việc học các phụ thuộc dài hạn (trong một số trường hợp, mạng neuron truy hồi có thể học các phụ thuộc kéo dài qua hàng trăm bước), nhưng vấn đề học các phụ thuộc dài hạn vẫn là một trong những thách thức chính trong học sâu.</p><h1 id="108-Mạng-trạng-thái-vọng-hồi"><a class="anchor hidden-xs" href="#108-Mạng-trạng-thái-vọng-hồi" title="108-Mạng-trạng-thái-vọng-hồi"><span class="octicon octicon-link"></span></a>10.8 Mạng trạng thái vọng hồi</h1><p>Các trọng số truy hồi để  ánh xạ từ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t − 1)}</span><script type="math/tex">\boldsymbol h^{(t − 1)}</script></span> đến <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> và các trọng số đầu vào để ánh xạ từ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol x^{(t)}</span><script type="math/tex">\boldsymbol x^{(t)}</script></span> đến <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> là các tham số khó học nhất trong một mạng truy hồi. Một hướng tiếp cận được đề xuất [Jaeger, 2003; Maass et al., 2002; Jaeger and Haas, 2004; Jaeger, 2007b] để giải quyết vấn đề này là thiết lập các trọng số truy hồi sao cho các đơn vị truy hồi ẩn thực hiện tốt công việc nắm bắt lịch sử của các đầu vào trong quá khứ và <em>chỉ học các trọng số đầu ra</em>. Đây là ý tưởng được đề xuất độc lập cho <em>mạng trạng thái vọng hồi</em> (echo state network - ESN) [Jaeger and Haas, 2004; Jaeger, 2007b], và <em>máy trạng thái lỏng</em> (liquid state machine - LST) [Maass et al., 2002]. Hai hệ thống này là tương đồng với nhau, ngoại trừ việc máy trạng thái lỏng sử dụng các neuron kích thích (với đầu ra nhị phân) thay vì các đơn vị ẩn có giá trị liên tục trong mạng trạng thái vọng hồi. Cả hai đều được gọi là <em>tính toán lưu trữ</em> (reservoir computing) [Lukoševičius and Jaeger 2009] bởi các đơn vị ẩn của chúng tạo thành một kho lưu trữ các đặc trưng tạm thời có thể lưu giữ các khía cạnh khác nhau của lịch sử của đầu vào.</p><p>Một hướng nhìn về các mạng neuron toán truy hồi tính toán lưu trữ này đó là chúng hơi giống với các <em>máy lõi</em> (kernel machine): chúng ánh xạ một chuỗi dài tùy ý (lịch sử của các đầu vào cho đến thời gian <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>) thành một vector độ dài cố định (trạng thái truy hồi <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span>), mà ở đó ta có thể áp dụng một bộ dự đoán tuyến tính (thường là hồi quy tuyến tính) để giải quyết vấn đề ta đang quan tâm. Các tiêu chuẩn huấn luyện sau đó có thể dễ dàng thiết kế để trở thành một hàm lồi của các trọng số đầu ra. Ví dụ, nếu đầu ra bao gồm hồi quy tuyến tính từ đơn vị ẩn tới nhãn đầu ra, và tiêu chuẩn huấn luyện là trung bình bình phương sai số, thì nó là hàm lồi và có thể được giải quyết bằng các thuật toán học tập đơn giản [Jaeger, 2003].</p><p>Vì vậy, một câu hỏi quan trọng được đặt ra: ta phải thiết lập các trọng số đầu vào và trọng số truy hồi như thế nào để trạng thái của mạng neuron truy hồi có thể biểu diễn một lượng lớn thông tin về lịch sử? Câu trả lời được đề xuất trong các công trình về tính toán lữu trữ là coi các mạng truy hồi như một hệ thống động lực và thiết lập các trọng số đầu vào và trọng số truy hồi sao cho hệ thống động lực gần đạt được sự ổn định.</p><p>Ý tưởng ban đầu là làm cho các trị riêng của ma trận Jacobi của hàm chuyển trạng thái gần với 1. Như đã giải thích trong phần 8.2.5, một đặc tính quan trọng của mạng truy hồi là phổ trị riêng của ma trận Jacobi <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J^{(t)} = \frac{\partial s^{(t)}}{\partial s^{(t − 1)}}</span><script type="math/tex">\boldsymbol J^{(t)} = \frac{\partial s^{(t)}}{\partial s^{(t − 1)}}</script></span>. <em>Bán kính phổ</em> (spectral radius) của <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J^{(t)}</span><script type="math/tex">\boldsymbol J^{(t)}</script></span> có vai trò đặc biệt quan trọng, được xác định bằng độ lớn của trị riêng có độ lớn cực đại trong số các trị riêng của nó.</p><p>Để hiểu được ảnh hưởng của bán kính phổ, hãy xem xét trường hợp lan truyền ngược đơn giản với ma trận Jacobi <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J</span><script type="math/tex">\boldsymbol J</script></span> không thay đổi theo <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>. Trường hợp này xảy ra, chẳng hạn, khi mạng là hoàn toàn tuyến tính. Giả sử rằng <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J</span><script type="math/tex">\boldsymbol J</script></span> có vector riêng <span class="mathjax"><span class="MathJax_Preview">{\boldsymbol v}</span><script type="math/tex">{\boldsymbol v}</script></span> và trị riêng tương ứng <span class="mathjax"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>. Xét những gì sẽ xảy ra khi ta lan truyền một vector gradient ngược theo thời gian. Nếu bắt đầu với một vector gradient <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g</span><script type="math/tex">\boldsymbol g</script></span>, thì sau một bước lan truyền ngược, ta sẽ có <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J \boldsymbol g</span><script type="math/tex">\boldsymbol J \boldsymbol g</script></span>, và sau <span class="mathjax"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> bước ta sẽ có <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J^n\boldsymbol g</span><script type="math/tex">\boldsymbol J^n\boldsymbol g</script></span>. Xét trường hợp khi lan truyền ngược một phiên bản <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g</span><script type="math/tex">\boldsymbol g</script></span> chứa nhiễu. Nếu bắt đầu với <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g + \delta\boldsymbol{\boldsymbol v}</span><script type="math/tex">\boldsymbol g + \delta\boldsymbol{\boldsymbol v}</script></span>, thì sau một bước, ta sẽ có <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J(\boldsymbol g + \delta \boldsymbol{\boldsymbol v})</span><script type="math/tex">\boldsymbol J(\boldsymbol g + \delta \boldsymbol{\boldsymbol v})</script></span>. Sau <span class="mathjax"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> bước, ta sẽ có <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J^n (\boldsymbol g + \delta \boldsymbol{\boldsymbol v})</span><script type="math/tex">\boldsymbol J^n (\boldsymbol g + \delta \boldsymbol{\boldsymbol v})</script></span>. Từ đây, ta có thể thấy rằng kết quả của quá trình lan truyền ngược bắt đầu từ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g</span><script type="math/tex">\boldsymbol g</script></span> và bắt đầu từ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g + \delta \boldsymbol{\boldsymbol v}</span><script type="math/tex">\boldsymbol g + \delta \boldsymbol{\boldsymbol v}</script></span> lệch nhau một lượng <span class="mathjax"><span class="MathJax_Preview">\delta \boldsymbol J^n\boldsymbol{\boldsymbol v}</span><script type="math/tex">\delta \boldsymbol J^n\boldsymbol{\boldsymbol v}</script></span> sau <span class="mathjax"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> bước. Nếu <span class="mathjax"><span class="MathJax_Preview">\boldsymbol {\boldsymbol v}</span><script type="math/tex">\boldsymbol {\boldsymbol v}</script></span> được chọn là một vector riêng đơn vị của <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J</span><script type="math/tex">\boldsymbol J</script></span> với trị riêng <span class="mathjax"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>, thì phép nhân với ma trận Jacob chỉ đơn giản là nhân tỷ lệ độ lệch ở mỗi bước. Hai lần thực hiện lan truyền ngược chênh lệch nhau một khoảng <span class="mathjax"><span class="MathJax_Preview">\delta|\lambda|^n</span><script type="math/tex">\delta|\lambda|^n</script></span>. Khi <span class="mathjax"><span class="MathJax_Preview">\boldsymbol {\boldsymbol v}</span><script type="math/tex">\boldsymbol {\boldsymbol v}</script></span> là vector riêng ứng với trị riêng có giá trị tuyệt đối <span class="mathjax"><span class="MathJax_Preview">|\lambda|</span><script type="math/tex">|\lambda|</script></span> lớn nhất, giá trị nhiễu này sẽ đạt khoảng cách lớn nhất có thể khi nhiễu ban đầu có độ lớn <span class="mathjax"><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>.</p><p>Khi <span class="mathjax"><span class="MathJax_Preview">| \lambda | &gt; 1</span><script type="math/tex">| \lambda | > 1</script></span>, độ lệch <span class="mathjax"><span class="MathJax_Preview">\delta | \lambda |^n</span><script type="math/tex">\delta | \lambda |^n</script></span> tăng theo cấp số nhân. Khi <span class="mathjax"><span class="MathJax_Preview">| \lambda |&lt;1</span><script type="math/tex">| \lambda |<1</script></span>, độ lệch nhỏ đi theo cấp số nhân.</p><p>Tất nhiên, ví dụ này giả định rằng ma trận Jacob không thay đổi sau mỗi bước thời gian, nghĩa là mạng truy hồi không chứa thành phần phi tuyến. Khi sử dụng đơn vị phi tuyến, đạo hàm của hàm phi tuyến sẽ tiến tới <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> ở nhiều bước thời gian và giúp ngăn chặn sự bùng nổ đạo hàm do bán kính phổ lớn gây ra. Thật vậy, các công trình nghiên cứu gần đây nhất về mạng trạng thái vọng hồi sử dụng bán kính phổ lớn hơn nhiều so với giá trị <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> [Yildiz et al., 2012; Jaeger, 2012].</p><p>Chú ý rằng, tất cả những tính chất ta phát biểu cho phép lan truyền ngược bằng các phép nhân ma trận lặp lại cũng cũng đúng cho lan truyền thuận trong một mạng không có thành phần phi tuyến, trong đó trạng thái <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t + 1)} = \boldsymbol h^{(t)\top} \boldsymbol  W</span><script type="math/tex">\boldsymbol h^{(t + 1)} = \boldsymbol h^{(t)\top} \boldsymbol  W</script></span>.</p><p>Khi một ánh xạ tuyến tính <span class="mathjax"><span class="MathJax_Preview">\boldsymbol W ^ {\top}</span><script type="math/tex">\boldsymbol W ^ {\top}</script></span> luôn thu nhỏ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h</span><script type="math/tex">\boldsymbol h</script></span> theo chuẩn <span class="mathjax"><span class="MathJax_Preview">L^2</span><script type="math/tex">L^2</script></span>, ta nói rằng ánh xạ này có <em>tính co</em>. Khi bán kính phổ nhỏ hơn <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>, ánh xạ từ <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> đến <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h ^{(t + 1)}</span><script type="math/tex">\boldsymbol h ^{(t + 1)}</script></span> sẽ là ánh xạ co, vì vậy một thay đổi nhỏ sẽ trở nên nhỏ hơn sau mỗi bước thời gian. Điều này khiến cho mạng quên đi thông tin về quá khứ khi ta lưu trữ vector trạng thái với mức độ chính xác hữu hạn (chẳng hạn như các số nguyên 32 bit).</p><p>Ma trận Jacobi cho ta biết <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> thay đổi một lượng như thế nào sau một bước lan truyền thuận về phía trước, hay nói cách khác, nó cho biết cách gradient trên <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h ^{(t + 1)}</span><script type="math/tex">\boldsymbol h ^{(t + 1)}</script></span> lan truyền một bước về phía sau trong quá trình lan truyền ngược như thế nào. Lưu ý rằng cả <span class="mathjax"><span class="MathJax_Preview">\boldsymbol W</span><script type="math/tex">\boldsymbol W</script></span> và <span class="mathjax"><span class="MathJax_Preview">\boldsymbol J</span><script type="math/tex">\boldsymbol J</script></span> đều không nhất thiết phải là ma trận đối xứng (mặc dù chúng là các ma trận vuông có giá trị thực), do đó chúng có thể có trị riêng và vector riêng có giá trị phức, với các thành phần ảo có khả năng dao động (nếu cùng một ma trận Jacobi được áp dụng lặp đi lặp lại). Mặc dù <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> hoặc một biến thể thay đổi nhỏ của <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> mà ta xét trong lan truyền ngược có giá trị thực, chúng cũng có thể được biểu diễn trong một cơ sở có giá trị phức như trên. Ta cần quan tâm đến độ lớn (trị tuyệt đối của giá trị phức) của các hệ số cơ sở có thể nhận giá trị phức này khi nhân ma trận với vector. Một trị riêng có độ lớn lớn hơn một trị riêng nào đó tương ứng với phép phóng đại (tăng theo cấp số nhân,nếu được áp dụng lặp đi lặp lại), và tương ứng với phép co (giảm theo cấp số nhân nếu được áp dụng lặp đi lặp lại) nếu có độ lớn nhỏ hơn một.</p><p>Với ánh xạ phi tuyến, ma trận Jacob được tự do thay đổi ở mỗi bước. Do đó động tính sẽ trở nên phức tạp hơn. Tuy nhiên, một điều vẫn đúng đó là một sự biến thiên nhỏ ban đầu có thể trở nên lớn sau một vài bước. Một khác biệt giữa trường hợp tuyến tính thuần túy và trường hợp phi tuyến là việc sử dụng một hàm <em>ép</em> (squashing) phi tuyến như <span class="mathjax"><span class="MathJax_Preview">\tanh</span><script type="math/tex">\tanh</script></span> có thể làm các động tính truy hồi bị giới hạn. Lưu ý rằng có thể xảy ra trường hợp động tính trở nên vô hạn qua quá trình lan truyền ngược ngay cả khi bước lan truyền thuận có động tính giới hạn, chẳng hạn, khi một chuỗi các đơn vị <span class="mathjax"><span class="MathJax_Preview">\tanh</span><script type="math/tex">\tanh</script></span> nằm toàn bộ ở giữa chế độ tuyến tính của chúng và được liên kết bởi các ma trận trọng số với bán kính phổ lớn hơn <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>. Tuy nhiên, hiếm khi tất cả các đơn vị <span class="mathjax"><span class="MathJax_Preview">\tanh</span><script type="math/tex">\tanh</script></span> đều đồng thời nằm ở điểm kích hoạt tuyến tính của chúng.</p><p>Chiến lược của mạng trạng thái vọng hồi chỉ đơn giản là cố định các trọng số để đạt được bán kính phổ nào đó, chẳng hạn như <span class="mathjax"><span class="MathJax_Preview">3</span><script type="math/tex">3</script></span>, trong đó thông tin được chuyển tiếp qua thời gian nhưng không bùng nổ nhờ tính ổn định của các đơn vị phi tuyến bão hòa như <span class="mathjax"><span class="MathJax_Preview">\tanh</span><script type="math/tex">\tanh</script></span>.</p><p>Gần đây, các kỹ thuật thiết lập trọng số trong mạng trạng thái vọng hồi đã được chứng minh rằng cũng có thể được sử dụng để <em>khởi tạo</em> các trọng số trong một mạng truy hồi đầy đủ (với các trọng số truy hồi giữa các tầng ẩn được học bằng lan truyền ngược theo thời gian) để giúp học các phụ thuộc dài hạn [Sutskever, 2012; Sutskeveret al., 2013]. Trong thiết lập này, một bán kính phổ ban đầu là <span class="mathjax"><span class="MathJax_Preview">1.2</span><script type="math/tex">1.2</script></span> hoạt động tốt, kết hợp với chiến lược khởi tạo thưa được mô tả trong phần 8.4.</p><h1 id="109-Các-đơn-vị-rò-rỉ-và-các-chiến-lược-khác-cho-nhiều-thang-thời-gian"><a class="anchor hidden-xs" href="#109-Các-đơn-vị-rò-rỉ-và-các-chiến-lược-khác-cho-nhiều-thang-thời-gian" title="109-Các-đơn-vị-rò-rỉ-và-các-chiến-lược-khác-cho-nhiều-thang-thời-gian"><span class="octicon octicon-link"></span></a>10.9 Các đơn vị rò rỉ và các chiến lược khác cho nhiều thang thời gian</h1><p>Một cách giải quyết vấn đề phụ thuộc dài hạn là thiết kế một mô hình thực thi ở nhiều thang thời gian, để cho một số phần của mô hình thực thi tại các thang thời gian mịn hơn (fine-grained) và có thể xử lý các chi tiết nhỏ, trong khi các phần khác thực thi ở thang thời gian thô hơn (coarse) và truyền thông tin từ quá khứ xa đến hiện tại hiệu quả hơn. Có nhiều chiến lược khác nhau để xây dựng cả thang thời gian mịn và thô. Có thể kể đến như bổ sung các kết nối nhảy cóc qua thời gian,  các “đơn vị rò rỉ” lấy tích phân các tín hiệu theo các hằng số thời gian khác nhau, hay loại bỏ một số kết nối được sử dụng để mô hình hóa các thang thời gian mịn hơn.</p><h2 id="1091-Thêm-các-kết-nối-nhảy-cóc-qua-thời-gian"><a class="anchor hidden-xs" href="#1091-Thêm-các-kết-nối-nhảy-cóc-qua-thời-gian" title="1091-Thêm-các-kết-nối-nhảy-cóc-qua-thời-gian"><span class="octicon octicon-link"></span></a>10.9.1 Thêm các kết nối nhảy cóc qua thời gian</h2><p>Một cách để thu được các thang thời gian thô là thêm các kết nối trực tiếp từ các biến trong quá khứ xa đến các biến ở hiện tại. Ý tưởng sử dụng các kết nối nhảy cóc như vậy đề xuất bởi Lin và cộng sự (1996), xuất phát từ ý tưởng kết hợp các độ trễ trong các mạng neuron lan truyền thuận [Lang and Hinton, 1988]. Trong một mạng truy hồi thông thường, một kết nối truy hồi xuất phát từ một đơn vị tại thời điểm <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> đến một đơn vị tại thời điểm <span class="mathjax"><span class="MathJax_Preview">t + 1</span><script type="math/tex">t + 1</script></span>. Ta có thể xây dựng các mạng truy hồi với độ trễ lớn hơn [Bengio, 1991].</p><p>Như đã thấy trong phần 8.2.5, các gradient có thể tiêu biến hoặc bùng nổ với cấp số nhân <em>theo số lượng bước thời gian</em>. Lin và cộng sự (1996) đã giới thiệu các kết nối truy hồi với độ trễ <span class="mathjax"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> để giảm thiểu vấn đề này. Các gradient khi đó giảm dần theo cấp số nhân, như một hàm số của <span class="mathjax"><span class="MathJax_Preview">\dfrac{\tau}{d}</span><script type="math/tex">\dfrac{\tau}{d}</script></span> thay vì <span class="mathjax"><span class="MathJax_Preview">\tau</span><script type="math/tex">\tau</script></span>. Vì có cả kết nối trễ và kết nối từng bước, các gradient có thể vẫn bùng nổ theo cấp số nhân theo <span class="mathjax"><span class="MathJax_Preview">\tau</span><script type="math/tex">\tau</script></span>. Điều này cho phép thuật toán học tập nắm bắt được các phụ thuộc dài hơn, mặc dù không phải toàn bộ các phụ thuộc dài hạn đều có thể được biểu diễn tốt theo cách này.</p><h2 id="1092-Các-đơn-vị-rò-rỉ-và-phổ-của-các-thang-thời-gian-khác-nhau"><a class="anchor hidden-xs" href="#1092-Các-đơn-vị-rò-rỉ-và-phổ-của-các-thang-thời-gian-khác-nhau" title="1092-Các-đơn-vị-rò-rỉ-và-phổ-của-các-thang-thời-gian-khác-nhau"><span class="octicon octicon-link"></span></a>10.9.2 Các đơn vị rò rỉ và phổ của các thang thời gian khác nhau</h2><p>Một cách khác để thu được các đường đi mà trên đó tích của các đạo hàm có giá trị gần với <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> đó là sử dụng các đơn vị tự kết nối <em>tuyến tính</em> (linear self-connections) có trọng số gần với <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>.</p><p>Khi ta cộng dồn một trung bình di động <span class="mathjax"><span class="MathJax_Preview">\mu^{(t)}</span><script type="math/tex">\mu^{(t)}</script></span> của giá trị <span class="mathjax"><span class="MathJax_Preview">\upsilon^{(t)}</span><script type="math/tex">\upsilon^{(t)}</script></span> nào đó bằng cập nhật <span class="mathjax"><span class="MathJax_Preview">\mu^{(t)} \leftarrow \alpha\mu^{(t-1)} + (1-\alpha)\upsilon^{(t)}</span><script type="math/tex">\mu^{(t)} \leftarrow \alpha\mu^{(t-1)} + (1-\alpha)\upsilon^{(t)}</script></span>, tham số <span class="mathjax"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> là một ví dụ cho đơn vị tự kết nối tuyến tính từ <span class="mathjax"><span class="MathJax_Preview">\mu^{(t-1)}</span><script type="math/tex">\mu^{(t-1)}</script></span> đến <span class="mathjax"><span class="MathJax_Preview">\mu^{(t)}</span><script type="math/tex">\mu^{(t)}</script></span>. Khi <span class="mathjax"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> gần với <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>, giá trị trung bình di động này ghi nhớ các thông tin về quá khứ trong một thời gian dài, còn khi <span class="mathjax"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> gần với <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>, thông tin về quá khứ nhanh chóng bị loại bỏ. Các đơn vị ẩn có các đơn vị tự kết nối tuyến tính có thể vận hành tương tự như các trung bình di động nói trên. Những đơn vị ẩn như vậy được gọi là <em>đơn vị rò rỉ</em> (leaky unit).</p><p>Các kết nối nhảy cóc qua <span class="mathjax"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> bước thời gian là một phương pháp đảm bảo rằng một đơn vị luôn có thể học để chịu ảnh hưởng bởi một giá trị từ <span class="mathjax"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> bước thời gian trước đó. Sử dụng các đơn vị tự kết nối tuyến tính với trọng số gần <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> là một cách khác để đảm bảo rằng đơn vị có thể sử dụng các giá trị từ quá khứ. Phương pháp tự kết nối tuyến tính cho phép sử dụng ảnh hưởng quá khứ một cách trơn tru và linh động hơn bằng cách điều chỉnh <span class="mathjax"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> (giá trị thực) thay vì điều chỉnh độ dài bước nhảy cóc (giá trị nguyên).</p><p>Những ý tưởng này được đề xuất bởi Mozer (1992) và El Hihi, Bengio (1996). Các đơn vị rò rỉ cũng cho thấy sự hữu dụng trong mạng trạng thái vọng hồi [Jaeger et al., 2007].</p><p>Có hai chiến lược cơ bản để thiết lập các hằng số thời gian được sử dụng bởi các đơn vị rò rỉ. Một chiến lược là cố định chúng một cách thủ công thành các giá trị hằng số, chẳng hạn, bằng cách lấy mẫu các giá trị từ một phân phối nào đó tại thời điểm khởi tạo. Một chiến lược khác là tạo ra những hằng số thời gian là các tham số tự do và học chúng. Việc sử dụng đơn vị rò rỉ ở các thang thời gian khác nhau có vẻ như sẽ hữu ích cho việc học phụ thuộc dài hạn [Mozer, 1992; Pascanu et al., 2013].</p><h2 id="1093-Loại-bỏ-kết-nối"><a class="anchor hidden-xs" href="#1093-Loại-bỏ-kết-nối" title="1093-Loại-bỏ-kết-nối"><span class="octicon octicon-link"></span></a>10.9.3 Loại bỏ kết nối</h2><p>Một cách tiếp cận khác để xử lý các phụ thuộc dài hạn là ý tưởng tổ chức trạng thái của mạng neuron truy hồi tại nhiều thang thời gian [El Hihi and Bengio, 1996], với luồng thông tin được truyền đi dễ dàng hơn qua khoảng cách dài ở các thang thời gian chậm hơn.</p><p>Ý tưởng này khác với kết nối nhảy cóc qua thời gian đã được thảo luận trước đó bởi vì nó chủ động <em>loại bỏ</em> các kết nối dài có độ dài <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> và thay thế chúng bằng các kết nối dài hơn. Các đơn vị được điều chỉnh theo cách như vậy buộc phải thực thi trên phạm vi thời gian dài. Các kết nối nhảy cóc qua thời gian <em>bổ sung</em> thêm các cạnh. Các đơn vị nhận thêm những kết nối mới như vậy có thể học cách thực thi trên một phạm vi thời gian dài nhưng cũng có thể chọn cách tập trung vào các kết nối ngắn hạn khác của chúng.</p><p>Có nhiều cách khác nhau để ép một nhóm các đơn vị truy hồi phải hoạt động ở các mức thời gian khác nhau. Một cách là làm các đơn vị truy hồi có tính rò rỉ, nhưng có các nhóm đơn vị khác nhau ứng với các thang thời gian khác nhau. Đây là đề xuất trong [Mozer ,1992] và đã được sử dụng thành công trong [Pascanu et al., 2013]. Một phương án khác là có các cập nhật tường minh và rời rạc ở những thời điểm khác nhau, với tần suất khác nhau cho các nhóm đơn vị khác nhau. Đây là cách tiếp cận của El Hihi &amp; Bengio (1996) và Koutnik &amp; cộng sự (2014). Nó hoạt động tốt trên nhiều tập dữ liệu đánh giá tiêu chuẩn.</p><h1 id="1010-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn-và-các-mạng-neuron-truy-hồi-khác-sử-dụng-cổng"><a class="anchor hidden-xs" href="#1010-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn-và-các-mạng-neuron-truy-hồi-khác-sử-dụng-cổng" title="1010-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn-và-các-mạng-neuron-truy-hồi-khác-sử-dụng-cổng"><span class="octicon octicon-link"></span></a>10.10 Bộ nhớ ngắn hạn hướng dài hạn và các mạng neuron truy hồi khác sử dụng cổng</h1><p>Tính đến thời điểm chúng tôi viết chương này, mô hình chuỗi hiệu quả nhất được sử dụng trong các ứng dụng thực tế là các <em>mạng neuron truy hồi sử dụng cổng</em>. Chúng bao gồm mạng <em>bộ nhớ ngắn hạn hướng dài hạn</em> (long short-term memory) và các <em>đơn vị truy hồi có cổng</em> (gated recurren unit).</p><p>Giống như các đơn vị rò rỉ, các mạng neuron truy hồi sử dụng cổng dựa trên ý tưởng tạo ra các đường đi có đạo hàm không tiêu biến cũng như không bùng nổ qua thời gian. Các đơn vị rò rỉ thực hiện điều đó với các trọng số kết nối được chọn một cách thủ công là các hằng số hoặc tham số. Các mạng neuron truy hồi sử dụng cổng đã tổng quát hóa ý tưởng này thành các trọng số kết nối có thể thay đổi ở mỗi bước thời gian.</p><p>Các đơn vị rò rỉ cho phép mạng <em>tích lũy</em> thông tin (chẳng hạn như dấu hiệu cho một đặc trưng hoặc một phạm trù cụ thể) trong một khoảng thời gian dài. Tuy nhiên, khi thông tin đó đã được sử dụng, mạng neuron đôi khi nên <em>quên</em> đi trạng thái cũ. Ví dụ, nếu một chuỗi được cấu thành từ các chuỗi con thành phần và ta mong muốn một đơn vị rò rỉ tích luỹ các dấu hiệu bên trong mỗi chuỗi con của các chuỗi thành phần, ta cần một cơ chế để quên trạng thái cũ bằng cách thiết lập nó bằng <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>. Thay vì quyết định thời điểm thiết lập trạng thái 0  một cách thủ công, ta cần mạng neuron học cách quyết định khi nào cần xoá đi trạng thái cũ. Mạng neuron truy hồi sử dụng cổng giúp chúng ta thực hiện điều này.</p><h2 id="10101-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn"><a class="anchor hidden-xs" href="#10101-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn" title="10101-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn"><span class="octicon octicon-link"></span></a>10.10.1 Bộ nhớ ngắn hạn hướng dài hạn</h2><p>Ý tưởng khéo léo về việc sử dụng các vòng tự lặp để tạo ra các đường đi mà trong đó gradient có thể truyền trong thời gian dài là một đóng góp cốt lõi của mô hình <em>bộ nhớ ngắn hạn hướng dài hạn</em> (LSTM) ban đầu [Hochreiter and Schmidhuber, 1997]. Một bổ sung quan trọng là làm cho trọng số của vòng tự lặp này phụ thuộc điều kiện theo ngữ cảnh, thay vì cố định chúng [Gers et al., 2000]. Bằng cách kiểm soát trọng số của vòng tự lặp bằng cổng (được điều khiển bởi một đơn vị ẩn khác), thang thời gian của phép tích phân có thể được thay đổi linh hoạt. Trong trường hợp này, ý ta là ngay cả đối với một LSTM với tham số cố định, thang thời gian của phép tích phân có thể thay đổi tuỳ theo chuỗi đầu vào, vì các hằng số thời gian là đầu ra của chính mô hình đó. LSTM đạt được sự thành công tuyệt vời trong nhiều ứng dụng, như nhận dạng chữ viết tay không ràng buộc [Graves et al., 2009], nhận dạng giọng nói [Graves et al., 2013; Graves and Jaitly, 2014], sinh chữ viết tay [Graves, 2013], dịch máy [Sutskever et al., 2014], chú thích hình ảnh [Kiros et al., 2014b; Vinyals et al., 2015; Xu et al., 2015] và phân tích cú pháp [Vinyals et al., 2014a].</p><p><img src="https://i.imgur.com/lascyxk.png" alt=""></p><blockquote>
<p>Hình 10.16: Sơ đồ khối của một “tế bào” mạng truy hồi LSTM. Các tế bào được kết nối truy hồi với nhau, thay thế các đơn vị ẩn thông thường của các mạng truy hồi nguyên thuỷ. Một đặc trưng đầu vào được tính toán bằng một đơn vị neuron nhân tạo thông thường. Giá trị của nó có thể được tích lũy vào trạng thái nếu cổng đầu vào dạng chữ S cho phép. Đơn vị trạng thái có một vòng tự lặp tuyến tính với trọng số được kiểm soát bởi cổng quên. Đầu ra của tế bào có thể bị khoá bởi cổng đầu ra. Toàn bộ đơn vị đóng vai trò cổng đều có một hàm phi tuyến chữ S, trong khi đơn vị đầu vào có thể sử dụng bất kỳ hàm ép (squashing) phi tuyến nào. Đơn vị trạng thái cũng có thể được sử dụng làm đầu vào bổ sung cho các đơn vị cổng. Hình vuông màu đen biểu thị độ trễ của một bước thời gian.</p>
</blockquote><p>Sơ đồ khối của LSTM được minh họa trong hình 10.16. Chúng tôi đưa ra các phương trình lan truyền thuận tương ứng cho một kiến trúc mạng truy hồi nông ở phía dưới đây. Các kiến trúc sâu hơn cũng đã được sử dụng thành công [Graves et al., 2013; Pascanu et al., 2014a]. Thay vì một đơn vị chỉ đơn giản áp dụng hàm phi tuyến theo từng phần tử lên kết quả của phép biến đổi affin cho đầu vào và các đơn vị truy hồi, các mạng truy hồi LSTM sử dụng các “tế bào LSTM” có tính truy hồi nội tại (một vòng tự lặp) bên cạnh tính truy hồi bên ngoài của mạng neuron truy hồi. Mỗi tế bào có các đầu vào và đầu ra tương tự như một mạng truy hồi truyền thống, nhưng có nhiều tham số hơn và sử dụng một hệ thống các đơn vị cổng giúp kiểm soát dòng thông tin. Thành phần quan trọng nhất của tế bào LSTM là đơn vị trạng thái <span class="mathjax"><span class="MathJax_Preview">s^{(t)}_i</span><script type="math/tex">s^{(t)}_i</script></span>, với một vòng tự lặp tuyến tính giống như các đơn vị rò rỉ được mô tả trong phần trước. Tuy nhiên, ở đây trọng số của vòng tự lặp (hoặc hằng số thời gian tương ứng) được điều khiển bởi một đơn vị <em>cổng quên</em> (forget gate) <span class="mathjax"><span class="MathJax_Preview">f^{(t)}_i</span><script type="math/tex">f^{(t)}_i</script></span> (cho bước thời gian <span class="mathjax"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> và tế bào <span class="mathjax"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>) giúp thiết lập trọng số này thành một giá trị trong khoảng từ <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> đến <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> bằng một đơn vị chữ S.</p><p><span class="mathjax"><span class="MathJax_Preview"> f^{(t)}_i = \sigma \left( b^f_i + \sum_j U^f_{i,j}x^{(t)}_j + \sum_j W^f_{i,j}h^{(t-1)}_j \right),
\tag{10.40}</span><script type="math/tex; mode=display"> f^{(t)}_i = \sigma \left( b^f_i + \sum_j U^f_{i,j}x^{(t)}_j + \sum_j W^f_{i,j}h^{(t-1)}_j \right),
\tag{10.40}</script></span></p><p>trong đó <span class="mathjax"><span class="MathJax_Preview">\boldsymbol x^{(t)}</span><script type="math/tex">\boldsymbol x^{(t)}</script></span> là vector đầu vào hiện tại và <span class="mathjax"><span class="MathJax_Preview">\boldsymbol h^{(t)}</span><script type="math/tex">\boldsymbol h^{(t)}</script></span> là vector tầng ẩn hiện tại, bao gồm các đầu ra toàn bộ tế bào LSTM. <span class="mathjax"><span class="MathJax_Preview">\boldsymbol b^f</span><script type="math/tex">\boldsymbol b^f</script></span>, <span class="mathjax"><span class="MathJax_Preview">\boldsymbol U^f</span><script type="math/tex">\boldsymbol U^f</script></span>, <span class="mathjax"><span class="MathJax_Preview">\boldsymbol W^f</span><script type="math/tex">\boldsymbol W^f</script></span> lần lượt là hệ số tự do, trọng số đầu vào và trọng số truy hồi của cổng quên. Do đó, trạng thái nội tại của tế bào LSTM được cập nhật như sau, với trọng số vòng tự lặp có điều kiện <span class="mathjax"><span class="MathJax_Preview">f^{(t)}_i</span><script type="math/tex">f^{(t)}_i</script></span>:</p><p><span class="mathjax"><span class="MathJax_Preview"> s^{(t)}_i = f^{(t)}_i s^{(t-1)}_i + g^{(t)}_i \sigma \left( b_i + \sum_j U_{i,j}x^{(t)}_j + \sum_j W_{i,j}h^{(t-1)}_j \right),
\tag{10.41}</span><script type="math/tex; mode=display"> s^{(t)}_i = f^{(t)}_i s^{(t-1)}_i + g^{(t)}_i \sigma \left( b_i + \sum_j U_{i,j}x^{(t)}_j + \sum_j W_{i,j}h^{(t-1)}_j \right),
\tag{10.41}</script></span></p><p>trong đó <span class="mathjax"><span class="MathJax_Preview">\boldsymbol b</span><script type="math/tex">\boldsymbol b</script></span>, <span class="mathjax"><span class="MathJax_Preview">\boldsymbol U</span><script type="math/tex">\boldsymbol U</script></span> và <span class="mathjax"><span class="MathJax_Preview">\boldsymbol W</span><script type="math/tex">\boldsymbol W</script></span> lần lượt biểu thị hệ số tự do, trọng số đầu vào và trọng số truy hồi trong tế bào LSTM. Đơn vị <em>cổng đầu vào bên ngoài</em> (external input gate unit) <span class="mathjax"><span class="MathJax_Preview">g^{(t)}_i</span><script type="math/tex">g^{(t)}_i</script></span> được tính toán giống như cổng quên (bằng một đơn vị chữ S để đạt được giá trị lựa chọn nằm trong khoảng <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> và <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>), nhưng với các tham số của riêng nó:</p><p><span class="mathjax"><span class="MathJax_Preview"> g^{(t)}_i = \sigma \left( b^g_i + \sum_j U^g_{i,j} x^{(t)}_j +\sum_j W^g_{i,j} h^{(t-1)}_j \right).
\tag{10.42}</span><script type="math/tex; mode=display"> g^{(t)}_i = \sigma \left( b^g_i + \sum_j U^g_{i,j} x^{(t)}_j +\sum_j W^g_{i,j} h^{(t-1)}_j \right).
\tag{10.42}</script></span></p><p>Đầu ra <span class="mathjax"><span class="MathJax_Preview">h^{(t)}_i</span><script type="math/tex">h^{(t)}_i</script></span> của tế bào LSTM có thể bị khoá thông qua <em>cổng đầu ra</em> (output gate) <span class="mathjax"><span class="MathJax_Preview">q^{(t)}_i</span><script type="math/tex">q^{(t)}_i</script></span>, cổng này cũng sử dụng khoá là một đơn vị chữ S<br>
<span class="mathjax"><span class="MathJax_Preview"> h^{(t)}_i = \tanh \left( s^{(t)}_i \right) q^{(t)}_i,
\tag{10.43}</span><script type="math/tex; mode=display"> h^{(t)}_i = \tanh \left( s^{(t)}_i \right) q^{(t)}_i,
\tag{10.43}</script></span><br>
<span class="mathjax"><span class="MathJax_Preview"> q^{(t)}_i = \sigma \left( b^o_i + \sum_j U^o_{i,j} x^{(t)}_j + \sum_j W^o_{i,j} h^{(t-1)}_j \right),
\tag{10.44}</span><script type="math/tex; mode=display"> q^{(t)}_i = \sigma \left( b^o_i + \sum_j U^o_{i,j} x^{(t)}_j + \sum_j W^o_{i,j} h^{(t-1)}_j \right),
\tag{10.44}</script></span></p><p>với các tham số <span class="mathjax"><span class="MathJax_Preview">\boldsymbol b^o</span><script type="math/tex">\boldsymbol b^o</script></span>, <span class="mathjax"><span class="MathJax_Preview">\boldsymbol U^o</span><script type="math/tex">\boldsymbol U^o</script></span>, <span class="mathjax"><span class="MathJax_Preview">\boldsymbol W^o</span><script type="math/tex">\boldsymbol W^o</script></span> lần lượt là hệ số tự do, trọng số đầu vào, trọng số truy hồi. Trong số các biến thể, ta có thể lựa chọn sử dụng trạng thái của tế bào <span class="mathjax"><span class="MathJax_Preview">s^{(t)}_j</span><script type="math/tex">s^{(t)}_j</script></span> như một đầu vào bổ sung (với trọng số của nó) cho ba cổng của đơn vị thứ <span class="mathjax"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>, giống như trong hình 10.16. Khi đó ta cần bổ sung thêm ba tham số.</p><p>Các mạng LSTM đã cho thấy khả năng học các phụ thuộc dài hạn dễ dàng hơn các kiến trúc truy hồi đơn giản, đầu tiên là trên các tập dữ liệu nhân tạo được thiết kế để kiểm tra khả năng học phụ thuộc dài hạn [Bengio et al., 1994; Hochreiter and Schmidhuber, 1997; Hochreiter et al., 2001], và sau đó đạt được hiệu suất tốt nhất trên các tác vụ xử lý chuỗi đầy thách thức [Graves, 2002; Graves et al., 2013; Sutskever et al., 2014]. Các biến thể và phương án thay thế của LSTM đã được nghiên cứu và sử dụng sẽ được đề cập ở phần sau.</p><p>— Trang Le thay Tran Luong kết thúc dịch từ đây (cuối trang 406)—</p><p>—Trang Le bắt đầu dịch từ đây (trang 407) —</p><h2 id="10102-Các-mạng-truy-hồi-sử-dụng-cổng-khác"><a class="anchor hidden-xs" href="#10102-Các-mạng-truy-hồi-sử-dụng-cổng-khác" title="10102-Các-mạng-truy-hồi-sử-dụng-cổng-khác"><span class="octicon octicon-link"></span></a>10.10.2 Các mạng truy hồi sử dụng cổng khác</h2><p>Những phần nào của kiến trúc LSTM thực sự cần thiết? Liệu có thể thiết kế những kiến trúc nào khác thành công trong việc cho phép mạng linh hoạt kiểm soát thang thời gian và quên đi hành vi của các đơn vị khác?</p><p>Một số câu trả lời cho câu hỏi này đã được trình bày trong những nghiên cứu gần đây về mạng neuron truy hồi sử dụng cổng sử dụng các đơn vị còn được gọi là đơn vị cổng truy hồi hay <em>GRU</em> (gated recurrent unit)[Cho et al., 2014b; Chung et al., 2014, 2015a; Jozefowicz et al., 2015 ; Chrupala et al., 2015]. Điểm khác biệt chính của kiến trúc này với LSTM là nó dùng một đơn vị cổng duy nhất đồng thời làm nhiệm vụ kiểm soát hệ số quên và quyết định cập nhật đơn vị trạng thái như thế nào. Các phương trình cập nhật như sau:</p><p><span class="mathjax"><span class="MathJax_Preview"> h_i^{(t)} = u_i^{(t-1)}h_i^{(t-1)} + (1 - u_i^{(t-1)})\sigma\left( b_i + \displaystyle \sum_j U_{i,j} x_j^{(t-1)} + \displaystyle \sum_j W_{i,j} \,r_j^{(t-1)} h_j^{(t-1)} \right),
\tag {10.45}</span><script type="math/tex; mode=display"> h_i^{(t)} = u_i^{(t-1)}h_i^{(t-1)} + (1 - u_i^{(t-1)})\sigma\left( b_i + \displaystyle \sum_j U_{i,j} x_j^{(t-1)} + \displaystyle \sum_j W_{i,j} \,r_j^{(t-1)} h_j^{(t-1)} \right),
\tag {10.45}</script></span></p><p>Trong đó <span class="mathjax"><span class="MathJax_Preview">\boldsymbol u</span><script type="math/tex">\boldsymbol u</script></span> là đại diện cho cổng “cập nhật” và <span class="mathjax"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> đại diện cho cổng “thiết lập lại” (reset). Giá trị của chúng được xác định như sau:</p><p><span class="mathjax"><span class="MathJax_Preview"> u_i^{(t)} = \sigma \left( b_i^u + \displaystyle\sum_j U_{i,j}^u \,x_j^{(t)} + \displaystyle\sum_j W_{i,j}^u \,h_j^{(t)}\right)
\tag {10.46}</span><script type="math/tex; mode=display"> u_i^{(t)} = \sigma \left( b_i^u + \displaystyle\sum_j U_{i,j}^u \,x_j^{(t)} + \displaystyle\sum_j W_{i,j}^u \,h_j^{(t)}\right)
\tag {10.46}</script></span><br>
và<br>
<span class="mathjax"><span class="MathJax_Preview"> r_i^{(t)} = \sigma \left( b_i^r + \displaystyle\sum_j U_{i,j}^r \,x_j^{(t)} + \displaystyle\sum_j W_{i,j}^r \,h_j^{(t)}\right).
\tag{10.47}</span><script type="math/tex; mode=display"> r_i^{(t)} = \sigma \left( b_i^r + \displaystyle\sum_j U_{i,j}^r \,x_j^{(t)} + \displaystyle\sum_j W_{i,j}^r \,h_j^{(t)}\right).
\tag{10.47}</script></span></p><p>--------- Hung Le ------</p><p>Cổng tái thiết và cổng cập nhật có thể “bỏ qua” các phần của vector trạng thái. Các cổng cập nhật hoạt động giống như các bộ tích hợp có điều kiện có thể điều chỉnh một cách tuyến tính bất kì kích thước nào, nhờ đó quyết định lựa chọn sao chép nó (khi kết quả hàm chữ S nằm tại một cực) hay là hoàn toàn bỏ qua nó (khi kết quả hàm chữ S là cực còn lại) bằng cách thay thế nó bằng giá trị “trạng thái đích” mới (mà bộ tích hợp rò rỉ muốn hội tụ về trạng thái đó). Các cổng thiết lập lại kiểm soát các phần của trạng thái được sử dụng để tính toán trạng thái đích tiếp theo, bổ sung một hiệu ứng phi tuyến trong mối liên hệ giữa trạng thái quá khứ và trạng thái tương lai.</p><p>Ngoài ra, còn nhiều biến thể khác của cơ chế này đã được thiết kế. Ví dụ như đầu ra của cổng thiết lập lại (hoặc cổng quên) có thể được chia sẻ (dùng chung) qua nhiều đơn vị ẩn. Ngoài ra, tích của cổng toàn cục (bao gồm toàn bộ nhóm các đơn vị, chẳng hạn như toàn bộ một tầng) và cổng cục bộ (trên mỗi đơn vị) có thể được sử dụng để kết hợp kiểm soát toàn cục và kiểm soát cục bộ. Tuy nhiên, nhiều nghiên cứu về các kiến trúc biến thể của LSTM và GRU cho thấy rằng chưa có biến thể nào vượt qua hai loại kiến trúc này trên phạm vi lớn các tác vụ [Greff et al., 2015; Jozefowicz et al., 2015]. Greff và cộng sự (2015) chỉ ra rằng cổng quên là một thành phần quan trọng, trong khi Jozefowicz và cộng sự (2015) đề xuất thêm một hệ số tự do bằng <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> vào cổng quên của LSTM, một thực nghiệm lấy cảm hứng từ [Gers et al.,2000], giúp LSTM trở thành biến thể tốt nhất trong các kiến trúc đã được khám phá.</p><h1 id="1011-Tối-ưu-hóa-cho-các-phụ-thuộc-dài-hạn"><a class="anchor hidden-xs" href="#1011-Tối-ưu-hóa-cho-các-phụ-thuộc-dài-hạn" title="1011-Tối-ưu-hóa-cho-các-phụ-thuộc-dài-hạn"><span class="octicon octicon-link"></span></a>10.11 Tối ưu hóa cho các phụ thuộc dài hạn</h1><blockquote>
<p>ND:<br>
<strong>Vanishing gradient</strong> là hiện tượng gradient có giá trị nhỏ dần khi đi xuống các tầng thấp hơn. Kết quả là các cập nhật thực hiện bởi dốc theo gradient không làm thay đổi nhiều trọng số của các tầng đó, khiến chúng không thể hội tụ và mô hình sẽ không thu được kết quả tốt. Trong phạm vi bản dịch, chúng tôi gọi hiện tượng này là sự tiêu biến gradient<br>
<strong>Exloding gradient</strong> là hiện tượng các gradient có thể có giá trị lớn hơn trong quá trình lan truyền ngược, khiến một số tầng có giá trị cập nhật cho trọng số quá lớn khiến chúng phân kỳ (phân rã), khiến cho kết quả thu được qua quá trình huấn luyện không như mong muốn. Trong phạm vi bản dịch này, chúng tôi gọi hiện tượng này là sự bùng nổ gradient.</p>
</blockquote><p>Phần 8.2.5 và phần 10.7 đã mô tả vấn đề tiêu biến và bùng nổ gradient xảy ra khi tối ưu hóa các mạng neuron truy hồi qua nhiều bước thời gian.</p><p>Năm 2011, Martens và Sutskever đã đề xuất một ý tưởng thú vị: các đạo hàm bậc hai có thể tiêu biến cùng lúc với đạo hàm bậc một. Các thuật toán tối ưu hóa bậc hai có thể được hiểu đại khái là chia đạo hàm bậc nhất cho đạo hàm bậc hai (trong trường hợp số chiều lớn sẽ là nhân gradient với nghịch đảo ma trận Hesse). Nếu đạo hàm bậc hai nhỏ đi theo cùng tỷ lệ với đạo hàm bậc một, thì tỷ số giữa đạo hàm bậc một và bậc hai có thể vẫn gần như không đổi. Không may là các phương pháp tối ưu hóa bậc hai có nhiều hạn chế, bao gồm: chi phí tính toán lớn, cần một lô nhỏ có kích thước lớn và có xu hướng bị thu hút vào các điểm yên ngựa. Martens và Sutskever (2011) đã tìm ra những kết quả đầy hứa hẹn bằng cách sử dụng các phương pháp tối ưu hóa bậc hai. Sau đó, Sutskever và cộng sự (2013) đã tìm ra các phương pháp đơn giản hơn và có thể đạt được hiệu quả tương tự như sử dụng động lượng Nesterov với sự chú trọng trong quá trình khởi tạo tham số. Xem [Sutskever, 2012] để biết thêm chi tiết. Cả hai cách tiếp cận này phần lớn đã được thay thế bằng cách áp dụng trượt gradient ngẫu nhiên (thậm chí không có động lượng) cho mạng LSTM. Thiết kế một mô hình dễ tối ưu hóa là dễ dàng hơn nhiều so với thiết kế một thuật toán tối ưu hóa mạnh mẽ.</p><p><img src="https://i.imgur.com/TLQWQsx.png" alt=""></p><blockquote>
<p>Hình 10.17: Ví dụ về hiệu ứng của việc xén gradient trong một mạng truy hồi với hai tham số <span class="mathjax"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> và <span class="mathjax"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span>. Xén gradient có thể làm cho trượt gradient thực hiện hợp lý hơn trong vùng lân cận của những vách đứng rất dốc. Những vách đứng dốc này thường xuất hiện trong các mạng truy hồi hoạt động gần như tuyến tính. Các vách đứng dốc theo cấp số nhân theo các bước thời gian vì ma trận trọng số được nhân với chính nó trong mỗi bước thời gian. (<em>Hình bên trái</em>) Trượt gradient không sử dụng xén gradient đã văng khỏi của vùng vực, sau đó nhận một gradient rất lớn từ mặt vách. Các gradient lớn đẩy các thông số một cách thê thảm ra khỏi các trục của đồ thị. (<em>Hình bên phải</em>) Trượt gradient có sử dụng xén gradient có một phản ứng hợp lý hơn với các vách đứng. Dù nó trèo lên trên mặt vách, kích thước bước sẽ bị giới hạn để nó không thể bị đẩy ra khỏi vùng dốc gần với nghiệm. Hình minh họa đã được điều chỉnh với sự cho phép của Pascanu và đồng nghiệp (2013).</p>
</blockquote><h2 id="10111-Xén-gradient"><a class="anchor hidden-xs" href="#10111-Xén-gradient" title="10111-Xén-gradient"><span class="octicon octicon-link"></span></a>10.11.1 Xén gradient</h2><p>Như thảo luận trong phần 8.2.4, các hàm có tính phi tuyến mạnh, chẳng hạn như các hàm được tính toán bởi mạng truy hồi qua nhiều bước thời gian, có xu hướng có các đạo hàm có giá trị rất lớn hoặc rất nhỏ. Điều này đã được minh họa trong hình 8.3 và hình 10.17, ta thấy rằng hàm mục tiêu (một hàm của các tham số) có một “quang cảnh” mà trong đó có một “vách đứng”: các vùng rộng và khá phẳng được phân tách bởi các vùng nhỏ mà ở đó hàm mục tiêu thay đổi nhanh chóng, tạo thành một loại vách đứng.</p><p>Khó khăn phát sinh khi gradient của tham số có giá trị rất lớn, việc cập nhật các tham số khi trượt gradient có thể ném các tham số rất xa vào một vùng nơi hàm mục tiêu lớn hơn, phá hủy phần lớn công việc đã được thực hiện để đạt được nghiệm hiện tại. Các gradient biểu thị hướng giảm mạnh nhất trong một khu vực rất nhỏ quanh các tham số hiện tại. Bên ngoài vùng rất nhỏ này, hàm chi phí có thể bắt đầu cong lên trên. Giá trị cập nhật được phải được lựa chọn đủ nhỏ để tránh làm cho độ cong tăng lên quá nhiều. Chúng tôi thường sử dụng các tốc độ học suy giảm đủ chậm để các bước liên tiếp có tốc độ học xấp xỉ nhau. Kích thước bước thích hợp với một phần tương đối tuyến tính của cảnh quan thường không phù hợp và gây ra chuyển động ngược lên dốc khi ta tiến vào phần cong hơn của cảnh quan ở bước tiếp theo.</p><p><em>Xén gradient</em> là một loại giải pháp đơn giản, đã được sử dụng bởi các chuyên gia trong nhiều năm. Mikolov (2012), Pascanu và cộng sự (2013) đã đưa ra những trường hợp khác nhau của ý tưởng này. Mikolov (2012) chọn cách xén gradient tham số trên một lô nhỏ theo từng phần tử ngay trước khi cập nhật tham số. Trong khi đó, Pascanu và cộng sự (2013) chọn cách <em>xén chuẩn <span class="mathjax"><span class="MathJax_Preview">||\boldsymbol g||</span><script type="math/tex">||\boldsymbol g||</script></span> của gradient</em> <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g</span><script type="math/tex">\boldsymbol g</script></span> ngay trước khi cập nhật tham số.</p><p><span class="mathjax"><span class="MathJax_Preview">\text{if}\, ||\boldsymbol g|| &gt; v
\tag{10.48}</span><script type="math/tex; mode=display">\text{if}\, ||\boldsymbol g|| > v
\tag{10.48}</script></span><br>
<span class="mathjax"><span class="MathJax_Preview">\boldsymbol g \leftarrow \dfrac{\boldsymbol g\upsilon}{||\boldsymbol g||},
\tag{10.49}</span><script type="math/tex; mode=display">\boldsymbol g \leftarrow \dfrac{\boldsymbol g\upsilon}{||\boldsymbol g||},
\tag{10.49}</script></span></p><p>trong đó <span class="mathjax"><span class="MathJax_Preview">\upsilon</span><script type="math/tex">\upsilon</script></span> là một ngưỡng chọn trước và <span class="mathjax"><span class="MathJax_Preview">\boldsymbol g</span><script type="math/tex">\boldsymbol g</script></span> được sử dụng để cập nhật tham số. Bởi vì gradient của tất cả tham số (bao gồm các nhóm tham số khác nhau, ví dụ như trọng số và hệ số tự do) được tái chuẩn hóa với một hệ số tỷ lệ duy nhất, phương pháp thứ hai có lợi thế là đảm bảo rằng mỗi bước vẫn đi theo hướng của gradient, tuy nhiên những thực nghiệm cho thấy cả hai hình thức đều hoạt động tương tự nhau. Mặc dù hướng cập nhật tham số là giống với hướng của gradient thực sự (gradient được tính toán từ toàn bộ mẫu huấn luyện), nhưng với việc xén chuẩn của gradient, chuẩn của vector cập nhật tham số đã bị giới hạn. Giá trị gradient giới hạn này giúp tránh được việc thực hiện một bước cập nhật không tốt trong trường hợp bùng nổ gradient. Trên thực tế, khi độ lớn gradient vượt quá một ngưỡng nào đó, việc chỉ thực hiện một <em>bước ngẫu nhiên</em> cũng thường có xu hướng mang lại kết quả tốt. Nếu sự bùng nổ nghiêm trọng đến mức gradient đạt tới giá trị <em>Inf</em> (vô cùng) hoặc <em>Nan</em> (Not-a-number: không-phải-một-số), thì một bước ngẫu nhiên có kích thước <span class="mathjax"><span class="MathJax_Preview">\upsilon</span><script type="math/tex">\upsilon</script></span> có thể được thực hiện và thường sẽ di chuyển gradient ra khỏi vùng không ổn định. Xén chuẩn gradient theo lô nhỏ không làm thay đổi hướng của gradient của từng lô nhỏ. Tuy nhiên, phép lấy trung bình các gradient bị xén chuẩn từ nhiều lô nhỏ không tương đương với xén chuẩn của gradient thực sự. Các mẫu có chuẩn gradient lớn, cũng như các mẫu nằm trong cùng lô nhỏ với chúng sẽ giảm bớt đóng góp đối với hướng sau cùng của gradient. Việc này trái ngược với trượt gradient truyền thống theo lô nhỏ, ở đó hướng của gradient thực sự giống với hướng của trung bình của tất cả các gradient trên các lô nhỏ. Nói cách khác, trượt gradient ngẫu nhiên truyền thống sử dụng một ước lượng không chệch của gradient, trong khi trượt gradient với xén chuẩn đưa vào một độ chệch dựa trên thực nghiệm. Với việc xén từng phần tử, hướng cập nhật tham số sẽ không được căn chỉnh tương ứng với hướng của gradient thực sự hoặc gradient theo lô nhỏ, nhưng nó vẫn tiến theo hướng giảm. Graves (2013) cũng đề xuất việc xén gradient sau khi lan truyền ngược (theo các đơn vị ẩn), nhưng không có sự so sánh nào giữa các biến thể này; chúng tôi phỏng đoán rằng tất cả các phương pháp này hoạt động tương tự như nhau.</p><h2 id="10112-Cơ-chế-kiểm-soát-để-thúc-đẩy-luồng-thông-tin"><a class="anchor hidden-xs" href="#10112-Cơ-chế-kiểm-soát-để-thúc-đẩy-luồng-thông-tin" title="10112-Cơ-chế-kiểm-soát-để-thúc-đẩy-luồng-thông-tin"><span class="octicon octicon-link"></span></a>10.11.2 Cơ chế kiểm soát để thúc đẩy luồng thông tin</h2><p>Xén gradient giúp giải quyết vấn đề bùng nổ gradient, nhưng nó không hữu dụng với vấn đề tiêu biến gradient. Để giải quyết sự tiêu biến gradient và nắm bắt các phụ thuộc dài hạn tốt hơn, chúng tôi đã thảo luận về ý tưởng tạo ra các đường dẫn trong đồ thị tính toán của kiến trúc truy hồi trải dài mà trên đó tích của các gradient ứng với các cung gần bằng <span class="mathjax"><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>. Một hướng tiếp cận để đạt được điều này là sử dụng LSTM, cũng như các cơ chế vòng tự lặp và cơ chế cổng khác, đã được mô tả trong phần 10.10. Một ý tưởng khác là kiểm soát hoặc ràng buộc các tham số để thúc đẩy “ luồng thông tin”. Cụ thể, chúng ta muốn vector gradient <span class="mathjax"><span class="MathJax_Preview">\boldsymbol\nabla_{\boldsymbol h^{(t)}}L</span><script type="math/tex">\boldsymbol\nabla_{\boldsymbol h^{(t)}}L</script></span> được lan truyền ngược để bảo toàn độ lớn của nó, ngay cả khi hàm mất mát chỉ phạt đầu ra ở cuối chuỗi. Ta cần</p><p><span class="mathjax"><span class="MathJax_Preview"> (\boldsymbol\nabla{\boldsymbol h^{(t)}}L) \dfrac{\partial \boldsymbol h^{(t)}}{\partial \boldsymbol h^{(t-1)}}
\tag{10.50}</span><script type="math/tex; mode=display"> (\boldsymbol\nabla{\boldsymbol h^{(t)}}L) \dfrac{\partial \boldsymbol h^{(t)}}{\partial \boldsymbol h^{(t-1)}}
\tag{10.50}</script></span></p><p>có độ lớn bằng</p><p><span class="mathjax"><span class="MathJax_Preview"> \boldsymbol\nabla{\boldsymbol h^{(t)}}L.
\tag{10.51}</span><script type="math/tex; mode=display"> \boldsymbol\nabla{\boldsymbol h^{(t)}}L.
\tag{10.51}</script></span></p><p>Với mục tiêu đó, Pascanu và cộng sự (2013) đề xuất bộ kiểm soát sau:</p><p><span class="mathjax"><span class="MathJax_Preview"> \Omega = \sum_t \left( \dfrac{\left\|(\boldsymbol\nabla{\boldsymbol h^{(t)}}L) \dfrac{\partial \boldsymbol h^{(t)}}{\partial \boldsymbol h^{(t-1)}} 
\right\|}{\left\|\boldsymbol\nabla{\boldsymbol h^{(t)}}L\right\|} - 1 \right)^2.
\tag{10.52}</span><script type="math/tex; mode=display"> \Omega = \sum_t \left( \dfrac{\left\|(\boldsymbol\nabla{\boldsymbol h^{(t)}}L) \dfrac{\partial \boldsymbol h^{(t)}}{\partial \boldsymbol h^{(t-1)}} 
\right\|}{\left\|\boldsymbol\nabla{\boldsymbol h^{(t)}}L\right\|} - 1 \right)^2.
\tag{10.52}</script></span></p><p>Tính toán gradient của bộ kiểm soát này có vẻ sẽ gặp khó khăn, nhưng Pascanu và cộng sự (2013) đề xuất một xấp xỉ trong đó coi các vector được lan truyền ngược <span class="mathjax"><span class="MathJax_Preview">\boldsymbol\nabla{\boldsymbol h^{(t)}}L</span><script type="math/tex">\boldsymbol\nabla{\boldsymbol h^{(t)}}L</script></span> như các hằng số (cho mục đích của bộ kiểm soát này, do đó không cần phải lan truyền ngược qua chúng). Các thực nghiệm với bộ kiểm soát này chỉ ra rằng, nếu kết hợp với xén chuẩn (để xử lý bùng nổ gradient), bộ kiểm soát có thể tăng đáng kể phạm vi khoảng phụ thuộc mà mạng neuron truy hồi có thể nắm bắt. Bởi các biến động của mạng neuron truy hồi được giữ trên cạnh của các gradient bùng nổ, nên việc xén gradient là rất quan trọng. Nếu không xén gradient, vấn đề bùng nổ gradient khiến quá trình học của mạng neuron truy hồi thất bại.</p><p>Điểm yếu chính của phương pháp này là nó không hiệu quả như LSTM cho các tác vụ có dữ liệu phong phú, chẳng hạn như mô hình hóa ngôn ngữ.</p><p>–Trang Le kết thúc phần dịch tại đây(trang 411)----</p><p>— HieuDC bắt đầu dịch từ đây (trang 412)—</p><h1 id="1012-Bộ-nhớ-tường-minh"><a class="anchor hidden-xs" href="#1012-Bộ-nhớ-tường-minh" title="1012-Bộ-nhớ-tường-minh"><span class="octicon octicon-link"></span></a>10.12 Bộ nhớ tường minh</h1><p>Trí tuệ được tạo nên từ tri thức, và việc tiếp thu tri thức được thực hiện thông qua quá trình học tập, đây là yếu tố thúc đẩy sự phát triển của các kiến trúc đa tầng quy mô lớn. Có nhiều loại tri thức khác nhau. Một số loại tri thức thuộc về tiềm thức và khó mô tả bằng lời, ví dụ như cách đi bộ, hay sự khác biệt giữa chó và mèo. Một số loại khác lại khá rõ ràng, tường minh và có thể mô tả được bằng ngôn ngữ, trong đó có những kiến thức phổ thông, như “mèo là một loài động vật” hay những sự kiện rất cụ thể mà bạn cần quan tâm để hoàn thành mục tiêu hiện tại, ví dụ như “cuộc họp với nhóm nhân viên bán hàng vào lúc 3 giờ chiều ở phòng 141”.</p><p>Mạng neuron vốn vượt trội về khả năng lưu trữ tri thức tiềm ẩn, nhưng lại gặp khó khăn khi cần ghi nhớ sự kiện. Thuật toán trượt gradient ngẫu nhiên cần nhiều cách biểu diễn của cùng một đầu vào trước khi nó có thể được lưu vào các tham số của mạng neuron, và ngay cả khi đó, dữ liệu đầu vào cũng không được lưu trữ hoàn toàn chính xác. Graves và cộng sự, (2014b) đã đặt ra giả thuyết rằng điều này là do các mạng neuron thiếu sự tương thích của hệ thống <em>bộ nhớ hoạt động</em> (working memory), thứ cho phép con người nắm giữ và xử lý những mảnh thông tin rời rạc một cách tường minh để đạt được mục đích nhất định. Những thành phần bộ nhớ tường minh như vậy cho phép các hệ thống của chúng ta không những lưu trữ và khôi phục những sự kiện chi tiết một cách nhanh chóng và “chủ động” mà còn có thể sử dụng thông tin này để suy luận một cách tuần tự. Sự cần thiết của những mạng neuron xử lý thông tin tuần tự theo các bước, thay đổi cách thức đầu vào được đưa vào mạng ở mỗi bước từ lâu đã được đánh giá là quan trọng đối với khả năng suy luận hơn là tạo ra các đáp ứng tự động, theo trực giác đối với đầu vào [Hinton, 1990].</p><p>Để giải quyết vấn đề này, Weston và cộng sự (2014) đã đưa ra các <em>mạng bộ nhớ</em> (memory network), trong đó chứa một tập các tế bào bộ nhớ có thể được truy cập thông qua cơ chế đánh địa chỉ. Ban đầu, các mạng bộ nhớ cần một tín hiệu giám sát để chỉ dẫn cho chúng cách sử dụng các tế bào bộ nhớ. Sau đó, Graves và cộng sự (2014b) đưa ra mô hình <em>máy neuron Turing</em>, thứ có thể học để đọc và ghi dữ liệu tùy ý từ tế bào bộ nhớ mà không cần chỉ dẫn tường minh nào về việc thực hiện hành động gì, cho phép huấn luyện từ đầu đến cuối (end-to-end) mà không cần tín hiệu chỉ dẫn thông qua việc sử dụng một cơ chế chú ý mềm dẻo (soft attention) dựa theo nội dung (xem [Bahdanau et al., 2015] và mục 12.4.5.1). Cơ chế đánh địa chỉ mềm dẻo này đã trở thành tiêu chuẩn cùng với các kiến trúc liên quan, từ đó mô phỏng các cơ chế thuật toán theo hướng vẫn cho phép tối ưu hóa dựa trên gradient [Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Kumar et al., 2015; Vinyals et al., 2015a; Grefenstette et al., 2015].</p><p>Mỗi tế bào bộ nhớ có thể được coi là một mở rộng của tế bào bộ nhớ trong LSTM và GRU. Điểm khác biệt là mạng xuất ra một trạng thái nội tại, lựa chọn tế bào nào để đọc ra hay ghi vào, giống như việc máy tính truy cập bộ nhớ bằng cách đọc ra hay ghi vào một địa chỉ cụ thể.</p><p>Việc tối ưu các hàm tạo giá trị địa chỉ số nguyên chính xác để truy cập ô nhớ là khá khó khăn. Để giảm bớt vấn đề này, các máy neuron Turing sẽ đọc hoặc ghi đồng thời nhiều tế bào bộ nhớ. Để đọc ra, máy neuron Turing lấy trung bình có trọng số của nhiều tế bào; còn khi ghi vào, giá trị trên các tế bào sẽ được thay đổi với các lượng khác nhau. Các hệ số của các thao tác này được chọn để tập trung vào một lượng nhỏ các tế bào, ví dụ, được chọn thông qua hàm softmax. Sử dụng các trọng số này với các đạo hàm khác <span class="mathjax"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> cho phép các hàm điều khiển truy cập tới bộ nhớ được tối ưu hóa bằng trượt gradient. Gradient trên những hệ số này cho biết nên tăng hay giảm chúng. Thông thường, gradient sẽ chỉ lớn đối với các địa chỉ bộ nhớ có hệ số lớn.</p><p>Những tế bào bộ nhớ này thường được tăng cường để chứa một vector, thay vì một số vô hướng duy nhất như trong LSTM hay GRU. Có hai lí do để tăng kích thước của tế bào bộ nhớ. Thứ nhất, chúng ta đã tăng chi phí truy cập tế bào bộ nhớ. Chúng ta bỏ ra chi phí tính toán để tạo ra một hệ số cho nhiều tế bào nhưng lại kì vọng các hệ số này tập trung thành cụm quanh một lượng nhỏ các tế bào. Bằng cách đọc giá trị vector thay vì giá trị vô hướng, ta có thể phần nào giảm đi sự phức tạp này. Thứ hai, việc sử dụng tế bào bộ nhớ có giá trị là vector cho phép <em>đánh địa chỉ dựa theo nội dung</em> (content-based addressing), tức là trọng số dùng để đọc hoặc ghi từ một tế bào là một hàm của chính tế bào đó. Tế bào mang giá trị vector cho phép ta có thể khôi phục một bộ nhớ có giá trị vector hoàn chỉnh nếu ta có thể tạo ra một mô thức khớp với một phần nào đó nhưng không khớp với toàn bộ phần tử của nó, tương tự như việc bạn có thể nhớ lại lời một bài hát dựa trên một vài từ ngữ trong đó. Có thể coi chỉ dẫn đọc dựa theo nội dung giống như là “Hãy nhớ lại lời một bài hát có đoạn điệp khúc là ‘We alllive in a yellow submarine.’” Đánh địa chỉ dựa theo nội dung sẽ hữu ích hơn khi đối tượng cần khôi phục có kích thước lớn; trong trường hợp mọi chữ cái trong lời bài hát đều được lưu trên một tế bào bộ nhớ riêng biệt thì ta không thể tìm kiếm theo cách này được. Để so sánh, cơ chế <em>đánh địa chỉ dựa theo vị trí</em> (location-based addressing) không cho phép tham chiếu đến nội dung của bộ nhớ. Ta có thể lấy ví dụ về đánh địa chỉ dựa theo vị trí như sau “Hãy nhớ lại lời bài hát thứ 347 trong danh sách nhạc của bạn”. Cơ chế đánh địa chỉ dựa theo vị trí này thường có thể phù hợp một cách hoàn hảo ngay cả khi các tế bào bộ nhớ có kích thước nhỏ.</p><p>Nếu nội dung chứa trong tế bào bộ nhớ được sao chép (không bị quên) tại hầu hết các bước thời điểm thì thông tin mà nó lưu trữ có thể được lan truyền thuận theo thời gian, và gradient được lan truyền ngược theo thời gian mà không bị tiêu biến hay bùng nổ.</p><p>Hướng tiếp cận bằng bộ nhớ tường minh được minh họa trong hình 10.18, trong đó ta thấy một “mạng neuron tác vụ” được ghép với một bộ nhớ. Dù mạng neuron tác vụ đó có thể là mạng lan truyền thuận hay truy hồi thì hệ thống tổng quát vẫn là mạng truy hồi. Mạng tác vụ có thể lựa chọn việc đọc ra hay ghi vào một địa chỉ bộ nhớ nhất định. Có vẻ như bộ nhớ tường minh cho phép các mô hình học các tác vụ mà các mạng neuron truy hồi hay LSTM thông thường không học được. Một lý do cho ưu điểm này có thể là việc thông tin và gradient có thể lan truyền (thuận và ngược theo thời gian) trong một khoảng thời gian rất dài.</p><p><img src="https://i.imgur.com/9GdHPjt.jpg" alt=""></p><blockquote>
<p>Hình 10.18: Sơ đồ của một mạng chứa bộ nhớ hữu hình, thể hiện một số yếu tố thiết kế quan trọng của máy neuron Turing. Ở đây ta phân biệt được phần “biểu diễn” của mô hình (“mạng tác vụ”, ở đây là mạng truy hồi nằm phía dưới hình) với phần “bộ nhớ” (tập các tế bào) lưu trữ các sự kiện. Mạng tác vụ sẽ học cách “điều khiển” bộ nhớ, quyết định khi nào đọc và ghi vào bộ nhớ (thông qua các cơ chế đọc và ghi, được thể hiện bằng các mũi tên đậm trỏ tới các địa chỉ cần đọc hay ghi).</p>
</blockquote><p>Như một sự thay thế của lan truyền ngược thông qua trung bình trọng số của tế bào bộ nhớ, ta có thể diễn giải các hệ số đánh địa chỉ bộ nhớ như là các giá trị xác suất và chỉ đọc ngẫu nhiên một tế bào [Zaremba and Sutskever, 2015]. Các mô hình tối ưu hóa đưa ra các quyết định rời rạc cần có các thuật toán tối ưu chuyên biệt, được mô tả ở mục 20.9.1. Cho đến nay, việc huấn luyện các kiến trúc ngẫu nhiên đưa ra các quyết định rời rạc vẫn khó khăn hơn việc huấn luyện các thuật toán tất định để đưa ra các quyết định mềm dẻo.</p><p>Dù là mềm dẻo (cho phép lan truyền ngược) hay ngẫu nhiên và cứng rắn, cơ chế lựa chọn địa chỉ vẫn có dạng đồng nhất với <em>cơ chế chú ý</em> (attention) đã từng được giới thiệu trước đây trong phạm vi dịch máy [Bahdanau et al., 2015] và sẽ được đề cập đến trong mục 12.4.5.1. Ý tưởng của cơ chế chú ý đối với các mạng neuron thậm chí đã được đưa ra trước đó trong bài toán tạo chữ viết tay [Graves, 2013], trong đó cơ chế chú ý được giới hạn để chỉ dịch chuyển thuận theo thời gian qua chuỗi. Đối với dịch máy và mạng bộ nhớ, tại mỗi bước, trọng tâm của sự chú ý có thể dịch chuyển đến một vị trí hoàn toàn khác so với bước trước đó.</p><p>Các mạng neuron truy hồi cung cấp một phương pháp để mở rộng các phương pháp học sâu để xử lý dữ liệu tuần tự. Chúng là công cụ quan trọng cuối cùng trong các công cụ học sâu mà ta có. Tiếp theo, chúng tôi sẽ thảo luận về cách lựa chọn và sử dụng các công cụ trên trong các tác vụ thực tế.</p><p>—HieuDC kết thúc dịch từ đây—</p><p>– Lh Long đã review tới đây –</p><p>– LA đã review tới đây –</p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-10-giai-đoạn-2" title="Chương 10 (giai đoạn 2)">Chương 10 (giai đoạn 2)</a></li>
<li><a href="#105-Mạng-neuron-truy-hồi-sâu" title="10.5 Mạng neuron truy hồi sâu">10.5 Mạng neuron truy hồi sâu</a></li>
<li><a href="#106-Các-mạng-neuron-đệ-quy" title="10.6 Các mạng neuron đệ quy">10.6 Các mạng neuron đệ quy</a></li>
<li><a href="#107-Thách-thức-của-phụ-thuộc-dài-hạn" title="10.7 Thách thức của phụ thuộc dài hạn">10.7 Thách thức của phụ thuộc dài hạn</a></li>
<li><a href="#108-Mạng-trạng-thái-vọng-hồi" title="10.8 Mạng trạng thái vọng hồi">10.8 Mạng trạng thái vọng hồi</a></li>
<li><a href="#109-Các-đơn-vị-rò-rỉ-và-các-chiến-lược-khác-cho-nhiều-thang-thời-gian" title="10.9 Các đơn vị rò rỉ và các chiến lược khác cho nhiều thang thời gian">10.9 Các đơn vị rò rỉ và các chiến lược khác cho nhiều thang thời gian</a><ul class="nav">
<li><a href="#1091-Thêm-các-kết-nối-nhảy-cóc-qua-thời-gian" title="10.9.1 Thêm các kết nối nhảy cóc qua thời gian">10.9.1 Thêm các kết nối nhảy cóc qua thời gian</a></li>
<li><a href="#1092-Các-đơn-vị-rò-rỉ-và-phổ-của-các-thang-thời-gian-khác-nhau" title="10.9.2 Các đơn vị rò rỉ và phổ của các thang thời gian khác nhau">10.9.2 Các đơn vị rò rỉ và phổ của các thang thời gian khác nhau</a></li>
<li><a href="#1093-Loại-bỏ-kết-nối" title="10.9.3 Loại bỏ kết nối">10.9.3 Loại bỏ kết nối</a></li>
</ul>
</li>
<li><a href="#1010-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn-và-các-mạng-neuron-truy-hồi-khác-sử-dụng-cổng" title="10.10 Bộ nhớ ngắn hạn hướng dài hạn và các mạng neuron truy hồi khác sử dụng cổng">10.10 Bộ nhớ ngắn hạn hướng dài hạn và các mạng neuron truy hồi khác sử dụng cổng</a><ul class="nav">
<li><a href="#10101-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn" title="10.10.1 Bộ nhớ ngắn hạn hướng dài hạn">10.10.1 Bộ nhớ ngắn hạn hướng dài hạn</a></li>
<li><a href="#10102-Các-mạng-truy-hồi-sử-dụng-cổng-khác" title="10.10.2 Các mạng truy hồi sử dụng cổng khác">10.10.2 Các mạng truy hồi sử dụng cổng khác</a></li>
</ul>
</li>
<li><a href="#1011-Tối-ưu-hóa-cho-các-phụ-thuộc-dài-hạn" title="10.11 Tối ưu hóa cho các phụ thuộc dài hạn">10.11 Tối ưu hóa cho các phụ thuộc dài hạn</a><ul class="nav">
<li><a href="#10111-Xén-gradient" title="10.11.1 Xén gradient">10.11.1 Xén gradient</a></li>
<li><a href="#10112-Cơ-chế-kiểm-soát-để-thúc-đẩy-luồng-thông-tin" title="10.11.2 Cơ chế kiểm soát để thúc đẩy luồng thông tin">10.11.2 Cơ chế kiểm soát để thúc đẩy luồng thông tin</a></li>
</ul>
</li>
<li><a href="#1012-Bộ-nhớ-tường-minh" title="10.12 Bộ nhớ tường minh">10.12 Bộ nhớ tường minh</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;"  >
        <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-10-giai-đoạn-2" title="Chương 10 (giai đoạn 2)">Chương 10 (giai đoạn 2)</a></li>
<li><a href="#105-Mạng-neuron-truy-hồi-sâu" title="10.5 Mạng neuron truy hồi sâu">10.5 Mạng neuron truy hồi sâu</a></li>
<li><a href="#106-Các-mạng-neuron-đệ-quy" title="10.6 Các mạng neuron đệ quy">10.6 Các mạng neuron đệ quy</a></li>
<li><a href="#107-Thách-thức-của-phụ-thuộc-dài-hạn" title="10.7 Thách thức của phụ thuộc dài hạn">10.7 Thách thức của phụ thuộc dài hạn</a></li>
<li><a href="#108-Mạng-trạng-thái-vọng-hồi" title="10.8 Mạng trạng thái vọng hồi">10.8 Mạng trạng thái vọng hồi</a></li>
<li><a href="#109-Các-đơn-vị-rò-rỉ-và-các-chiến-lược-khác-cho-nhiều-thang-thời-gian" title="10.9 Các đơn vị rò rỉ và các chiến lược khác cho nhiều thang thời gian">10.9 Các đơn vị rò rỉ và các chiến lược khác cho nhiều thang thời gian</a><ul class="nav">
<li><a href="#1091-Thêm-các-kết-nối-nhảy-cóc-qua-thời-gian" title="10.9.1 Thêm các kết nối nhảy cóc qua thời gian">10.9.1 Thêm các kết nối nhảy cóc qua thời gian</a></li>
<li><a href="#1092-Các-đơn-vị-rò-rỉ-và-phổ-của-các-thang-thời-gian-khác-nhau" title="10.9.2 Các đơn vị rò rỉ và phổ của các thang thời gian khác nhau">10.9.2 Các đơn vị rò rỉ và phổ của các thang thời gian khác nhau</a></li>
<li><a href="#1093-Loại-bỏ-kết-nối" title="10.9.3 Loại bỏ kết nối">10.9.3 Loại bỏ kết nối</a></li>
</ul>
</li>
<li><a href="#1010-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn-và-các-mạng-neuron-truy-hồi-khác-sử-dụng-cổng" title="10.10 Bộ nhớ ngắn hạn hướng dài hạn và các mạng neuron truy hồi khác sử dụng cổng">10.10 Bộ nhớ ngắn hạn hướng dài hạn và các mạng neuron truy hồi khác sử dụng cổng</a><ul class="nav">
<li><a href="#10101-Bộ-nhớ-ngắn-hạn-hướng-dài-hạn" title="10.10.1 Bộ nhớ ngắn hạn hướng dài hạn">10.10.1 Bộ nhớ ngắn hạn hướng dài hạn</a></li>
<li><a href="#10102-Các-mạng-truy-hồi-sử-dụng-cổng-khác" title="10.10.2 Các mạng truy hồi sử dụng cổng khác">10.10.2 Các mạng truy hồi sử dụng cổng khác</a></li>
</ul>
</li>
<li><a href="#1011-Tối-ưu-hóa-cho-các-phụ-thuộc-dài-hạn" title="10.11 Tối ưu hóa cho các phụ thuộc dài hạn">10.11 Tối ưu hóa cho các phụ thuộc dài hạn</a><ul class="nav">
<li><a href="#10111-Xén-gradient" title="10.11.1 Xén gradient">10.11.1 Xén gradient</a></li>
<li><a href="#10112-Cơ-chế-kiểm-soát-để-thúc-đẩy-luồng-thông-tin" title="10.11.2 Cơ chế kiểm soát để thúc đẩy luồng thông tin">10.11.2 Cơ chế kiểm soát để thúc đẩy luồng thông tin</a></li>
</ul>
</li>
<li><a href="#1012-Bộ-nhớ-tường-minh" title="10.12 Bộ nhớ tường minh">10.12 Bộ nhớ tường minh</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
