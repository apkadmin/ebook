<div id="doc" class="markdown-body container-fluid comment-enabled"><h1 id="Chương-11-Phương-pháp-luận-thực-tế">Chương 11: Phương pháp luận thực tế</h1><p>Để áp dụng thành công các kỹ thuật học sâu, người thực hiện cần nhiều kỹ năng hơn là chỉ đơn thuần hiểu về thuật toán và nguyên tắc hoạt động của chúng. Đối với học máy, một người nghiên cứu giỏi cần biết cách lựa chọn thuật toán phù hợp với mỗi ứng dụng cụ thể, đồng thời biết cách giám sát và điều chỉnh dựa trên kết quả thực nghiệm thu được để cải thiện hệ thống học máy. Trong quá trình phát triển hệ thống học máy,  người nghiên cứu cần quyết định khi nào cần: thu thập thêm dữ liệu, tăng hay giảm dung lượng mô hình, thêm hay bớt các đặc trưng kiểm soát, cải thiện quá trình tối ưu hóa của mô hình, cải thiện quá trình suy luận xấp xỉ của mô hình, hay sửa lỗi khi triển khai cài đặt phần mềm cho mô hình. Tất cả những thao tác này ít nhất cũng cần tốn thời gian để thử, vì vậy việc xác định đúng hướng hành động thay vì suy đoán theo cảm tính là rất quan trọng.</p><p>Phần lớn nội dung cuốn sách này được dành để nói về các mô hình học máy, các thuật toán huấn luyện, và các hàm mục tiêu khác nhau. Điều này có thể khiến cho bạn tin rằng yếu tố quan trọng nhất để trở thành một chuyên gia học máy là biết thật nhiều kỹ thuật học máy và kiến thức toán học. Trong thực tế, người ta thường có thể đạt kết quả tốt hơn nhiều khi áp dụng một thuật toán thông thường một cách đúng đắn, thay vì sử dụng một thuật toán phức tạp mà không hiểu rõ về nó. Và may thay, để sử dụng thuật toán một cách đúng đắn, ta chỉ cần nắm được một số phương pháp khá đơn giản. Nhiều lời khuyên trong chương này được tinh chỉnh từ nghiên cứu của Andrew Ng (2015).</p><p>Chúng tôi đề xuất quy trình thiết kế trong thực tế như sau:</p><ul>
<li>Xác định các mục tiêu của bạn - sử dụng thang đo sai số nào, và giá trị mục tiêu (nhãn) ứng với thang đo sai số ấy. Những mục tiêu và thang đo sai số này nên được điều chỉnh tuỳ theo vấn đề mà ứng dụng cần giải quyết.</li>
<li>Thiết lập một đường ống hoạt động <em>đầu-cuối</em> (end-to-end pipeline) càng sớm càng tốt, bao gồm cả việc ước lượng các thang đo hiệu suất phù hợp.</li>
<li>Trang bị cho hệ thống công cụ phù hợp để xác định các điểm nghẽn (bottleneck) trong hiệu năng. Chẩn đoán xem những thành phần nào đang hoạt động kém hơn kì vọng, và liệu nguyên nhân là do quá khớp, vị khớp, hay do lỗi trong dữ liệu hoặc phần mềm.</li>
<li>Lặp lại các thay đổi mang tính cải thiện như: thu thập dữ liệu mới, tinh chỉnh các siêu tham số, hoặc thay đổi thuật toán, dựa trên kết quả cụ thể từ các thực nghiệm của bạn.</li>
</ul><p>Để minh họa, ta sử dụng hệ thống phiên mã địa chỉ Street View [Goodfellow và cộng sự, 2014d]. Mục đích của ứng dụng này là thêm các toà nhà vào Google Maps. Những ô tô phục vụ chương trình Street View sẽ chụp ảnh các toà nhà và ghi lại tọa độ GPS được ứng với mỗi bức ảnh. Một mạng tích chập được sử dụng để nhận diện số nhà trong mỗi ảnh, cho phép cơ sở dữ liệu của Google Maps thêm địa chỉ đó vào đúng vị trí trên bản đồ. Câu chuyện về quá trình phát triển ứng dụng thương mại này cho ta một ví dụ về việc làm thế nào để tuân thủ phương pháp thiết kế mà ta đã lựa chọn từ đầu.</p><p>Chúng tôi sẽ mô tả từng bước của quá trình này.</p><h1 id="111-Các-thang-đo-hiệu-suất">11.1 Các thang đo hiệu suất</h1><p>Xác định các mục tiêu, xét theo nghĩa thang đo sai số mà bạn sẽ sử dụng, là bước cần thiết đầu tiên, bởi thang đo sai số sẽ là kim chỉ nam cho tất cả các thao tác trong tương lai. Bạn cũng nên định hình một mức hiệu suất mà bạn mong muốn đạt được.</p><p>Hãy luôn nhớ rằng đối với phần lớn các ứng dụng, việc đạt được giá trị sai số tuyệt đối bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mn" id="MJXp-Span-2">0</span></span></span><script type="math/tex" id="MathJax-Element-1">0</script></span> là bất khả thi. Sai số Bayes là một giá trị xác định tỷ lệ sai số nhỏ nhất mà bạn có thể kỳ vọng đạt được, ngay cả khi bạn có lượng dữ liệu huấn luyện vô hạn và có thể khôi phục phân phối xác suất thực tế của dữ liệu. Điều này có thể là do các đặc trưng đầu vào của bạn không hàm chứa đầy đủ thông tin về biến đầu ra, hoặc bản chất hệ thống vốn mang tính ngẫu nhiên. Bạn cũng sẽ chỉ có một lượng dữ liệu huấn luyện hữu hạn.</p><p>Lượng dữ liệu huấn luyện có thể bị giới hạn bởi một vài lý do. Khi mục đích của bạn là xây dựng sản phẩm hoặc dịch vụ tốt nhất có thể trong thực tế, bạn có thể thu thập nhiều dữ liệu hơn, nhưng cần phải xác định được giá trị sai số giảm đi khi có thêm dữ liệu và cân đối nó với chi phí thu thập dữ liệu. Việc thu thập dữ liệu có thể cần thời gian, tiền bạc, và sức chịu đựng của con người (ví dụ như khi quá trình thu thập dữ liệu bao gồm đến việc thực hiện các xét nghiệm xâm lấn trong y khoa). Khi mục đích của bạn là xác định xem thuật toán nào sẽ thực hiện tốt hơn trên một thang tiêu chuẩn cố định, đặc tả của tiêu chuẩn này thường xác định rõ tập dữ liệu huấn luyện và bạn không được phép thu thập thêm dữ liệu.</p><p>Làm thế nào để ta xác định được mức hiệu suất kỳ vọng hợp lý? Thông thường, trong bối cảnh học thuật, chúng ta sẽ có một ước lượng về tỷ lệ sai số có thể đạt được dựa trên những kết quả đã được công bố trước đó. Đối với các ứng dụng thực tế, ta cần đưa ra một ước lượng nào đó về tỷ lệ sai số cần thiết để một ứng dụng được coi là an toàn, hiệu quả về mặt chi phí, hay hấp dẫn với khách hàng. Khi đã xác định được tỷ lệ sai số mong muốn thực tế, những quyết định thiết kế của bạn sẽ đi theo hướng để có thể đạt được tỷ lệ này.</p><p>Một điều quan trọng khác cần cân nhắc bên cạnh giá trị mục tiêu của thang đo hiệu suất là việc lựa chọn sử dụng thang đo nào. Một số thang đo hiệu suất khác nhau có thể được dùng để đo lường hiệu quả của một ứng dụng hoàn chỉnh chứa các thành phần học máy. Những thang đo hiệu suất này thường khác với hàm chi phí được sử dụng để huấn luyện mô hình. Như đã mô tả ở phần 5.1.2, người ta thường đo lường độ chính xác, hay nói cách khác, chính là tỷ lệ sai số của một hệ thống.</p><p>Tuy nhiên, có nhiều ứng dụng yêu cầu những thang đo phức tạp hơn để đánh giá hiệu suất.</p><p>Đôi khi, mức độ thiệt hại khi gây ra một loại lỗi nào đó sẽ cao hơn nhiều so với khi gây ra các loại lỗi khác. Ví dụ, một hệ thống phát hiện thư rác có thể gặp phải hai loại lỗi: phân loại sai một thư hợp lệ thành thư rác, và cho phép một thư rác xuất hiện trong hộp thư hợp lệ. Có thể thấy việc loại bỏ một thư hợp lệ là điều tồi tệ hơn nhiều so với chấp nhận một tin nhắn đáng ngờ. Vì vậy, thay vì đánh giá tỷ lệ sai số của một bộ phân loại thư rác, ta mong muốn có thể đánh giá một dạng chi phí tổng hợp nào đó, mà chi phí của việc chặn những tin nhắn hợp lệ cao hơn chi phí của việc cho phép những tin rác xuất hiện.</p><p>Đôi khi ta cần huấn luyện một bộ phân loại nhị phân, dùng để phát hiện một số sự kiện hiếm hoi. Ví dụ, ta có thể thiết kế một bài kiểm tra y khoa cho một bệnh dịch hiếm. Giả sử rằng trung bình chỉ có một người mắc bệnh này trên mỗi một triệu người. Chúng ta có thể dễ dàng đạt được tỷ lệ phát hiện chính xác <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-3"><span class="MJXp-mn" id="MJXp-Span-4">99.9999</span><span class="MJXp-mi" id="MJXp-Span-5">%</span></span></span><script type="math/tex" id="MathJax-Element-2">99.9999\%</script></span> bằng cách lập trình cho bộ phân loại luôn báo cáo kết quả là âm tính. Rõ ràng, độ chính xác là một thang đo tồi để đánh giá hiệu quả của một hệ thống như thế này. Một cách để giải quyết vấn đề này là đánh giá độ <em>chính xác</em> (precision) và <em>độ nhạy</em> (recall). Trong đó, độ chính xác là tỷ lệ những phát hiện bởi mô hình mà đúng, độ nhạy là tỷ lệ mà những sự kiện đúng đã được phát hiện. Một máy phát hiện luôn trả về kết quả âm tính sẽ đạt được độ chính xác hoàn hảo, nhưng có độ nhạy bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-6"><span class="MJXp-mn" id="MJXp-Span-7">0</span></span></span><script type="math/tex" id="MathJax-Element-3">0</script></span>. Ngược lại, nếu nó chỉ ra tất cả mọi người đều bị bệnh thì sẽ đạt được độ nhạy hoàn hảo, nhưng độ chính xác lại bằng phần trăm số người bị bệnh (<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-8"><span class="MJXp-mn" id="MJXp-Span-9">0.0001</span><span class="MJXp-mi" id="MJXp-Span-10">%</span></span></span><script type="math/tex" id="MathJax-Element-4">0.0001\%</script></span> trong ví dụ này). Khi sử dụng độ chính xác và độ nhạy, người ta thường biểu diễn một <em>đường cong PR</em> (Precision-Recall curve, viết tắt là PR curve), với độ chính xác ứng với trục <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12">y</span></span></span><script type="math/tex" id="MathJax-Element-5">y</script></span> và độ nhạy ứng với trục <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-13"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">x</span></span></span><script type="math/tex" id="MathJax-Element-6">x</script></span> của hệ trục toạ độ. Bộ phân loại cho ra điểm số cao hơn nếu sự kiện được phát hiện là có xảy ra. Ví dụ, một mạng lan truyền thuận được thiết kế để phát hiện một loại bệnh, với đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-15"><span class="MJXp-mrow" id="MJXp-Span-16"><span class="MJXp-munderover" id="MJXp-Span-17"><span><span class="MJXp-over"><span style="margin-bottom: -1.17em;"><span class="MJXp-mo" id="MJXp-Span-19" style="margin-left: 0px; margin-right: 0px;">ˆ</span></span><span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">y</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21">P</span><span class="MJXp-mo" id="MJXp-Span-22" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23">y</span><span class="MJXp-mo" id="MJXp-Span-24" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-25">1</span><span class="MJXp-mrow" id="MJXp-Span-26"><span class="MJXp-mo" id="MJXp-Span-27" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-28">x</span><span class="MJXp-mo" id="MJXp-Span-29" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-7">\hat{y} = P(y = 1 | \boldsymbol{x})</script></span> ước lượng xác suất một người với bệnh án được mô tả bằng các đặc trưng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-30"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-31">x</span></span></span><script type="math/tex" id="MathJax-Element-8">\boldsymbol{x}</script></span> là có bệnh. Chúng ta sẽ báo cáo kết quả dương tính khi điểm số này vượt qua một ngưỡng nào đó. Bằng cách thay đổi ngưỡng cho trước, ta có thể đánh đổi giữa độ chính xác và độ nhạy. Trong nhiều trường hợp, ta hy vọng có thể tổng kết hiệu quả của bộ phân loại bằng một con số cụ thể, thay vì là một đường cong. Để được vậy, ta chuyển độ chính xác <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-32"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33">p</span></span></span><script type="math/tex" id="MathJax-Element-9">p</script></span> và độ nhạy <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-34"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-35">r</span></span></span><script type="math/tex" id="MathJax-Element-10">r</script></span> thành một <em>điểm số F</em> (F-score) được tính theo công thức:</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-36"><span class="MJXp-mtable" id="MJXp-Span-37"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-38" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-39" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40">F</span><span class="MJXp-mo" id="MJXp-Span-41" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mfrac" id="MJXp-Span-42" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-43">2</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45">r</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46">p</span><span class="MJXp-mo" id="MJXp-Span-47" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48">r</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-49" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-11">F = \frac{2pr}{p+r}. \tag{11.1}</script></span></p><p>Ngoài ra, ta cũng có thể xem xét tổng diện tích nằm dưới đường cong PR để đánh giá.</p><p>Trong một số ứng dụng, hệ thống học máy còn có thể từ chối đưa ra quyết định. Điều này hữu ích khi thuật toán học máy có khả năng ước lượng độ tự tin của nó khi đưa ra quyết định, đặc biệt là trong những trường hợp mà một quyết định sai có thể gây ra tổn thất lớn và việc ra quyết định có thể được con người tiếp nhận. Hệ thống phiên mã địa chỉ Street View là một ví dụ như vậy. Tác vụ cần thực hiện là phiên mã số nhà từ một hình ảnh để liên kết với địa chỉ thực tế bên ngoài trên bản đồ. Bởi giá trị của bản đồ sẽ giảm đi đáng kể nếu nó không chính xác, nên việc chỉ thêm địa chỉ mới vào hệ thống khi nó khớp với thực tế là rất quan trọng. Nếu hệ thống học máy nghĩ rằng nó không làm được điều đó chính xác như con người thì tốt nhất là để con người làm. Tất nhiên, hệ thống học máy chỉ hữu dụng nếu nó có thể làm giảm đi đáng kể lượng hình ảnh mà con người phải trực tiếp xử lý. Một thang đo hiệu suất tự nhiên được sử dụng trong trường hợp này là <em>độ phủ</em> (coverage). Độ phủ là tỷ lệ các mẫu mà hệ thống học máy có khả năng đưa ra kết quả phân loại. Ta có thể cân nhắc đánh đổi giữa độ phủ và độ chính xác. Một hệ thống có thể đạt <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-50"><span class="MJXp-mn" id="MJXp-Span-51">100</span><span class="MJXp-mi" id="MJXp-Span-52">%</span></span></span><script type="math/tex" id="MathJax-Element-12">100\%</script></span> độ chính xác bằng cách không xử lý bất cứ ví dụ nào, nhưng điều này đồng nghĩa với việc giảm độ phủ xuống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-53"><span class="MJXp-mn" id="MJXp-Span-54">0</span><span class="MJXp-mi" id="MJXp-Span-55">%</span></span></span><script type="math/tex" id="MathJax-Element-13">0\%</script></span>. Đối với trường hợp của Street View, mục tiêu của dự án là đạt tới độ chính xác ngang với trình độ con người khi duy trì độ phủ ở mức <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-56"><span class="MJXp-mn" id="MJXp-Span-57">95</span><span class="MJXp-mi" id="MJXp-Span-58">%</span></span></span><script type="math/tex" id="MathJax-Element-14">95\%</script></span>. Hiệu suất của con người trong tác vụ này đạt độ chính xác <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-59"><span class="MJXp-mn" id="MJXp-Span-60">98</span><span class="MJXp-mi" id="MJXp-Span-61">%</span></span></span><script type="math/tex" id="MathJax-Element-15">98\%</script></span>.</p><p>Ta còn có thể sử dụng nhiều thang đó khác. Ví dụ: đo tỷ lệ nhấp chuột, thu thập khảo sát về mức độ hài lòng của người dùng, v.v. Nhiều ứng dụng chuyên biệt còn có các tiêu chuẩn đặc thù riêng.</p><p>Điều quan trọng là xác định thang đo hiệu suất nào cần cải thiện trước tiên, sau đó tập trung vào cải thiện nó. Nếu không xác định mục tiêu rõ ràng, thật khó để nói rằng liệu những thay đổi của hệ thống học máy có tạo ra tiến triển hay không.</p><h1 id="112-Các-mô-hình-cơ-sở-mặc-định">11.2 Các mô hình cơ sở mặc định</h1><p>Sau khi lựa chọn thang đo hiệu suất và mục tiêu, bước tiếp theo trong bất kỳ ứng dụng thực tế nào là thiết lập một hệ thống đầu-cuối hợp lý càng sớm càng tốt. Trong mục này, chúng tôi đưa ra các khuyến nghị về việc nên dùng thuật toán nào để làm mô hình cơ sở đầu tiên trong các trường hợp khác nhau. Hãy nhớ rằng các nghiên cứu về học sâu tiến triển rất nhanh, vậy nên một số thuật toán tốt hơn có thể xuất hiện ngay trong lúc chúng tôi viết cuốn sách này.</p><p>Tùy thuộc vào độ phức tạp của vấn đề, bạn thậm chí có thể bắt đầu mà không cần sử dụng học sâu. Nếu vấn đề của bạn có thể được giải quyết chỉ bằng cách chọn một vài trọng số tuyến tính hợp lý thì bạn có thể bắt đầu với một mô hình thống kê đơn giản như hồi quy logit.</p><p>Nếu bạn thấy vấn đề của mình thuộc nhóm <a href="https://en.wikipedia.org/wiki/AI-complete" target="_blank" rel="noopener">“AI-complete”</a> như nhận dạng đối tượng, nhận dạng giọng nói, dịch máy, v.v. thì nhiều khả năng mọi việc sẽ tiến triển tốt nếu bạn bắt đầu với một mô hình học sâu phù hợp.</p><p>Trước tiên, ta lựa chọn dạng mô hình tổng quát dựa trên cấu trúc của dữ liệu. Nếu bạn muốn thực hiện học có giám sát với đầu vào là các vector kích thước cố định, hãy dùng mạng lan truyền thuận với các tầng kết nối đầy đủ. Nếu như đầu vào có cấu trúc topo biết trước (chẳng hạn, đầu vào là hình ảnh), hãy dùng mạng tích chập. Trong những trường hợp này, bạn nên bắt đầu bằng việc sử dụng một số loại đơn vị tuyến tính theo từng khoảng (với mỗi khoảng giá trị của đầu vào, đơn vị sẽ có đầu ra khác nhau) như ReLUs hoặc các biến thể của chúng như Leaky ReLU, PreLU hoặc maxout. Còn nếu đầu vào hoặc đầu ra là một chuỗi, hãy sử dụng một mạng truy hồi có cổng (LSTM hoặc GRU).</p><p>Một lựa chọn hợp lý cho thuật toán tối ưu là SGD có động lượng với tốc độ học suy giảm (các chiến lược suy giảm tốc độ học phổ biến - có thể vận hành tốt hơn hoặc tệ hơn tùy thuộc vào các bài toán khác nhau - bao gồm: suy giảm tuyến tính cho đến khi đạt được tốc độ học cực tiểu định trước, giảm tỷ lệ theo cấp số nhân hoặc giảm tỷ lệ học theo hệ số từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-62"><span class="MJXp-mn" id="MJXp-Span-63">2</span></span></span><script type="math/tex" id="MathJax-Element-16">2</script></span> đến <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-64"><span class="MJXp-mn" id="MJXp-Span-65">10</span></span></span><script type="math/tex" id="MathJax-Element-17">10</script></span> khi sai số trên tập xác thực tiến đến vùng bão hòa). Một lựa chọn thay thế hợp lý khác là Adam. <em>Chuẩn hóa theo lô</em> (batch normalization) có thể mang lại hiệu quả đáng kể cho hiệu suất tối ưu hóa, đặc biệt là đối với các mạng tích chập và mạng neuron với hàm kích hoạt phi tuyến chữ S. Mặc dù bỏ qua việc chuẩn hóa theo lô trong mô hình cơ sở đầu tiên là khá hợp lý, nhưng chiến lược này sẽ được sớm được dùng đến nếu quá trình tối ưu hóa xuất hiện vấn đề.</p><p>Nếu tập huấn luyện của bạn không chứa mười triệu mẫu hoặc nhiều hơn, bạn nên thực hiện một số chuẩn hoá đơn giản ngay từ đầu. Kết thúc sớm nên được sử dụng gần như mọi lúc. <em>Tắt ngẫu nhiên</em> (dropout) là một bộ kiểm soát tuyệt vời, dễ triển khai và tương thích với nhiều mô hình và thuật toán huấn luyện. Chuẩn hoá theo lô đôi khi cũng làm giảm sai số tổng quát hóa và cho phép bỏ qua tắt ngẫu nhiên, bởi nhiễu trong việc ước lượng thống kê đã được sử dụng để chuẩn hoá mỗi biến số.</p><p>Nếu bài toán của bạn giống với một bài toán khác đã được nghiên cứu kỹ, việc sao chép mô hình và thuật toán sẵn có sẽ giúp ích rất nhiều. Thậm chí bạn có thể sao chép một mô hình đã được huấn luyện trước cho bài toán đó. Ví dụ, việc sử dụng các đặc trưng từ một mạng tích chập được huấn luyện trên tập ImageNet để giải quyết các bài toán thị giác máy tính khác là rất thông dụng [Girshick el at., 2015].</p><p>Một câu hỏi thường gặp là chúng ta có nên bắt đầu bằng cách sử dụng mô hình học không giám sát, được mô tả trong phần III. Điều này tùy thuộc vào lĩnh vực ta cần giải quyết. Một số lĩnh vực, như xử lý ngôn ngữ tự nhiên, đã hưởng lợi rất nhiều từ các kỹ thuật học không giám sát, chẳng hạn như phép <em>nhúng từ</em> (word embedding). Trong các lĩnh vực khác, như thị giác máy tính, các kỹ thuật học không giám sát hiện tại chưa mang lại lợi ích, ngoại trừ các mô hình bán giám sát, khi số lượng các mẫu đã được gán nhãn là rất nhỏ [Kingma el at., 2014; Rasmus el at., 2015]. Nếu ứng dụng của bạn thuộc lĩnh vực mà việc học không giám sát đã được chứng minh là quan trọng, bạn nên sử dụng nó trong mô hình cơ sở đầu tiên. Nếu không, hãy chỉ sử dụng học không giám sát trong lần thử nghiệm đầu tiên nếu tác vụ bạn muốn giải quyết thuộc dạng không giám sát. Bạn luôn luôn có thể thử bổ sung các mô hình học không giám sát vào sau nếu thấy rằng mô hình cơ sở ban đầu quá khớp.</p><h1 id="113-Cân-nhắc-việc-thu-thập-thêm-dữ-liệu">11.3 Cân nhắc việc thu thập thêm dữ liệu.</h1><p>Sau khi hệ thống đầu-cuối đầu tiên được thiết lập, đã đến lúc đánh giá hiệu suất của thuật toán và xác định cách cải thiện nó. Nhiều người mới tiếp cận học máy thường nỗ lực cải thiện kết quả bằng cách thử nhiều thuật toán khác nhau. Tuy nhiên, thu thập thêm dữ liệu thường mang lại kết quả tốt hơn so với cải thiện thuật toán.</p><p>Nhưng làm thế nào để quyết định khi nào nên thu thập thêm dữ liệu? Đầu tiên, ta cần xác định xem hiệu suất trên tập huấn luyện có là chấp nhận được hay không. Hiệu suất trên tập huấn luyện thấp có nghĩa là thuật toán học tập đã không sử dụng huấn luyện có sẵn, vì vậy không có lý do gì để thu thập thêm dữ liệu. Thay vào đó, hãy thử tăng kích thước của mô hình bằng cách thêm vào nhiều tầng hơn hoặc thêm vào mỗi tầng nhiều đơn vị hơn. Đồng thời thử cải thiện thuật toán học, chẳng hạn, bằng cách điều chỉnh siêu tham số như tỷ lệ học. Nếu các mô hình lớn và các thuật toán tối ưu đã được tinh chỉnh một cách cẩn thận mà vẫn không mang lại kết quả tốt, thì vấn đề có thể là do <em>chất lượng</em> của tập dữ liệu huấn luyện. Dữ liệu có thể có quá nhiều nhiễu hoặc không chứa các đầu vào cần thiết cho việc dự đoán kết quả đầu ra mong muốn. Điều này gợi ý rằng ta bắt đầu lại từ đầu, thu thập dữ liệu ít nhiễu hơn, hoặc thu thập một tập đặc trưng phong phú hơn.</p><p>Nếu hiệu suất trên tập huấn luyện là chấp nhận được, ta tiếp tục đánh giá hiệu suất trên tập kiểm thử. Nếu hiệu suất trên tập kiểm thử này cũng là chấp nhận được, ta không cần thực hiện thêm điều gì nữa. Nếu hiệu suất trên tập kiểm thử kém hơn nhiều so với trên tập huấn luyện, thu thập thêm dữ liệu là một trong những giải pháp hữu hiệu nhất. Những yếu tố ta cần quan tâm đặc biệt là chi phí và tính khả thi của việc thu thập thêm dữ liệu, chi phí và tính khả thi của việc giảm sai số kiểm thử bằng các phương pháp khác, lượng dữ liệu được cho là cần thiết để cải thiện hiệu suất trên bộ kiểm thử một cách đáng kể. Ở các công ty internet lớn với hàng triệu hay hàng tỷ người dùng, việc thu thập các tập dữ liệu lớn là khả thi, chi phí cho việc này ít hơn đáng kể so với chi phí của các giải pháp khác, vì thế câu trả lời gần như luôn luôn là thu thập thêm nhiều dữ liệu huấn luyện hơn. Ví dụ, sự phát triển của các tập dữ liệu lớn đã được gán nhãn là một trong những yếu tố quan trọng trong việc giải quyết bài toán nhận dạng đối tượng. Trong các trường hợp khác, chẳng hạn như ứng dụng trong y học, việc thu thập thêm dữ liệu sẽ là rất tốn kém hoặc không khả thi. Một phương pháp thay thế đơn giản hơn đó là giảm kích thước của mô hình hoặc cải thiện cơ chế kiểm soát, bằng cách điều chỉnh các siêu tham số, như các <em>hệ số suy giảm trọng số</em>, hoặc bằng cách thêm các <em>cơ chế kiểm soát</em> như <em>tắt ngẫu nhiên</em>. Nếu bạn nhận thấy cách biệt giữa hiệu suất của tập huấn luyện và tập kiểm thử vẫn là không thể chấp nhận được, kể cả sau khi đã tinh chỉnh các siêu tham số kiểm soát, lời khuyên là hãy thu thập thêm nhiều dữ liệu hơn nữa.</p><p>Khi quyết định nên thu thập thêm dữ liệu, việc xác định lượng dữ liệu cần thu thập thêm cũng là rất cần thiết. Việc vẽ biểu đồ các đường cong biểu thị mối quan hệ giữa kích thước tập huấn luyện và sai số tổng quát hóa là rất hữu ích, như  trong hình 5.4. Bằng cách ngoại suy các đường cong, ta có thể đoán được lượng dữ liệu huấn luyện cần thêm vào để đạt được mức hiệu suất nhất định nào đó. Thường thì việc thêm một lượng mẫu có tỷ lệ nhỏ so với tổng số lượng mẫu đã có sẽ không ảnh hưởng đáng kể tới sai số tổng quát hóa. Bởi vậy, bạn nên thử nghiệm với kích thước tập huấn luyện theo tỉ lệ logarit, ví dụ như tăng gấp đôi số lượng mẫu giữa các lần thử nghiệm liên tiếp nhau.</p><p>Nếu việc thu thập thêm dữ liệu là không khả thi, cách duy nhất để cải thiện sai số tổng quát hóa là cải thiện chính thuật toán học tập. Chủ đề này thuộc về lĩnh vực nghiên cứu và không phải là lĩnh vực nên lưu tâm đối với những người theo hướng thực hành ứng dụng.</p><h1 id="114-Lựa-chọn-siêu-tham-số">11.4. Lựa chọn siêu tham số</h1><p>Hầu hết các thuật toán học sâu đều có một bộ siêu tham số đi kèm, giúp điều khiển nhiều khía cạnh trong hành vi của thuật toán. Một vài trong số chúng ảnh hưởng đến chi phí về thời gian và bộ nhớ của quá trình thực thi thuật toán. Một số khác lại ảnh hưởng đến chất lượng mô hình thu được qua giai đoạn huấn luyện và khả năng suy luận kết quả của nó khi được áp dụng trên các đầu vào mới.</p><p>Có <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-66"><span class="MJXp-mn" id="MJXp-Span-67">02</span></span></span><script type="math/tex" id="MathJax-Element-18">02</script></span> cách tiếp cận cơ bản để lựa chọn các siêu tham số này: chọn thủ công và chọn tự động. Cách chọn các thủ công yêu cầu sự hiểu biết về vai trò của những tham số và cách mô hình học máy đạt được tính tổng quát hóa tốt. Các thuật toán chọn tham số tự động giúp giảm đáng kể yêu cầu hiểu biết về những ý tưởng này, nhưng chi phí tính toán do đó cũng thường lớn hơn nhiều.</p><h2 id="1141-Tinh-chỉnh-thủ-công-các-siêu-tham-số">11.4.1. Tinh chỉnh thủ công các siêu tham số</h2><p>Để thiết lập siêu tham số một cách thủ công, ta cần hiểu mối quan hệ giữa các siêu tham số, sai số huấn luyện, sai số tổng quát hóa và tài nguyên tính toán (bộ nhớ và thời gian chạy). Điều này yêu cầu một nền tảng vững chắc về các ý tưởng liên quan đến dung lượng hiệu dụng của một thuật toán học tập, như đã mô tả trong chương 5.</p><p>Mục đích của việc tìm kiếm thủ công các siêu tham số thường là để tìm kiếm sai số tổng quát hóa thấp nhất trong giới hạn thời gian chạy và bộ nhớ cho trước. Chúng tôi sẽ không thảo luận về cách xác định ảnh hưởng về mặt bộ nhớ và thời gian chạy của các siêu tham số ở đây, bởi vì điều này phụ thuộc nhiều vào nền tảng phần cứng.</p><p>Mục tiêu chính của việc tìm kiếm thủ công siêu tham số là điều chỉnh dung lượng hiệu dụng của mô hình sao cho phù hợp với độ phức tạp của tác vụ. Dung lượng hiệu dụng này bị ràng buộc bởi 3 yếu tố: khả năng biểu diễn của mô hình, khả năng của thuật toán học tập trong việc tối thiểu hóa hàm chi phí được dùng để huấn luyện mô hình, và mức độ mà hàm chi phí và quá trình huấn luyện kiểm soát mô hình. Một mô hình có nhiều tầng và nhiều đơn vị ẩn trên mỗi tầng hơn sẽ có dung lượng biểu diễn lớn hơn - nghĩa là có khả năng biểu diễn được các hàm phức tạp hơn. Mặc dù vậy, trên thực tế, nó có thể không nhất thiết phải học được tất cả các hàm số, nếu thuật toán huấn luyện không thể khám phá ra các hàm thực thi tốt việc tối thiểu hóa chi phí huấn luyện, hoặc nếu các cơ chế kiểm soát như suy giảm trọng số ngăn cản mô hình học được một vài trong số những hàm này.</p><p>Sai số tổng quát hóa thường tuân theo một đường cong chữ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-68"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69">U</span></span></span><script type="math/tex" id="MathJax-Element-19">U</script></span> khi được biểu diễn như một hàm của một trong số những siêu tham số, như được minh hoạ trong hình 5.3. Ở một cực của đường cong chữ U, giá trị siêu tham số tương ứng với dung lượng thấp, và sai số tổng quát hóa cao bởi vì sai số huấn luyện cao. Đây là vùng vị khớp. Tương tự, ở cực còn lại, giá trị siêu tham số tương ứng với dung lượng cao, có sai số tổng quát hóa cao bởi vì khoảng cách giữa sai số huấn luyện và sai số kiểm thử là cao. Dung lượng tối ưu nằm đâu đó ở giữa, nơi đạt được sai số tổng quát hóa thấp nhất bằng cách thêm một khoảng cách khái quát trung bình vào lượng trung bình của sai số huấn luyện.</p><p>Với một vài siêu tham số, quá khớp xảy ra khi chúng có giá trị lớn. Số đơn vị ẩn trong một tầng là một ví dụ, vì việc tăng các đơn vị ẩn làm tăng dung lượng của mô hình. Với nhiều siêu tham số khác, quá khớp xảy ra khi chúng có giá trị nhỏ. Ví dụ, hệ số suy giảm trọng số nhỏ nhất có thể là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-70"><span class="MJXp-mn" id="MJXp-Span-71">0</span></span></span><script type="math/tex" id="MathJax-Element-20">0</script></span>, tương ứng với dung lượng hiệu dụng cực đại của thuật toán học tập.</p><p>Cần lưu ý rằng, không phải mọi siêu tham số đều có khả năng khám phá toàn bộ đường cong chữ U. Có nhiều siêu tham số là rời rạc, như số đơn vị trong một tầng hoặc số lượng mảnh tuyến tính trong một đơn vị <em>cực đại đầu ra</em> (maxout), vì vậy nó chỉ có thể chạm đến một vài điểm dọc theo đường cong. Đôi khi ta cũng có những siêu tham số ở dạng nhị phân, thường được dùng làm công tắc để quyết định liệu có sử dụng một số thành phần không bắt buộc nào đó của thuật toán học tập hay không, chẳng hạn như một bước tiền xử lý dùng để chuẩn hóa các đặc trưng đầu vào bằng cách trừ đi trung bình và chia cho độ lệch chuẩn của chúng. Những siêu tham số này có thể chỉ khám phá được <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-72"><span class="MJXp-mn" id="MJXp-Span-73">2</span></span></span><script type="math/tex" id="MathJax-Element-21">2</script></span> điểm trên đường cong. Các siêu tham số khác có thể có giá trị cực đại hoặc cực tiểu để ngăn chúng khám phá phần nào đó của đường cong. Ví dụ, hệ số suy giảm trọng số có giá trị cực tiểu là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-74"><span class="MJXp-mn" id="MJXp-Span-75">0</span></span></span><script type="math/tex" id="MathJax-Element-22">0</script></span>. Có nghĩa là nếu mô hình bị vị khớp ngay cả khi không suy giảm trong số, ta sẽ không thể tiến vào miền quá khớp bằng cách điều chỉnh hệ số suy giảm trọng số. Nói cách khác, một vài siêu tham số chỉ có thể làm giảm dung lượng của mô hình. Để làm điều ngược lại, ta cần điều chỉnh các siêu tham số khác, chẳng hạn như số đơn vị trong một tầng hoặc số tầng mạng.</p><p>Có lẽ, tốc độ học là siêu tham số quan trọng nhất. Nếu bạn chỉ có thời gian để tinh chỉnh một siêu tham số, hãy tập trung vào tốc độ học. Nó kiểm soát dung lượng hiệu dụng của mô hình theo một cách phức tạp hơn các siêu tham số khác - dung lượng hiệu dụng của mô hình đạt cực đại khi tốc độ học là <em>chính xác</em> cho bài toán tối ưu, chứ không phải khi tốc độ học là đặc biệt lớn hay đặc biệt nhỏ. Tốc độ học có một đường cong chữ U cho sai số <em>huấn luyện</em>, được mô tả trong hình 11.1. Khi tốc độ học quá lớn, phép trượt gradient có thể làm tăng sai số huấn luyện thay vì giảm nó đi. Trong trường hợp hàm bậc hai lý tưởng hóa, điều này xảy ra nếu tốc độ học lớn hơn ít nhất 2 lần giá trị tối ưu của nó (Lecun et al., 1998s). Ngược lại, khi tốc độ học quá nhỏ, quá trình huấn luyện không những chậm hơn mà còn có thể bị mắc kẹt vĩnh viễn tại một vùng có sai số huấn luyện cao. Ảnh hưởng này thường ít được biết đến (nó sẽ không xảy ra trong trường hợp hàm mất mát là lồi).</p><p><img src="https://i.imgur.com/1GFSqzQ.png" alt=""></p><blockquote>
<p>Hình 11.1: Mối quan hệ điển hình giữa tốc độ học và sai số huấn luyện. Lưu ý sự gia tăng mạnh của sai số khi tốc độ học vượt qua giá trị tối ưu. Điều này xảy ra trong một khoảng thời gian huấn luyện cố định, bởi tốc độ học nhỏ hơn đôi khi làm chậm quá trình huấn luyện theo một hệ số tỉ lệ thuận với sự suy giảm tốc độ học. Sai số tổng quát hóa có thể tuân theo đường cong này hoặc trở nên phức tạp hơn do ảnh hưởng của các cơ chế kiểm soát phát sinh khi tỷ lệ học quá lớn hoặc quá nhỏ, bởi vì sự tối ưu hóa tồi, ở một mức độ nào đó, có thể làm giảm hoặc ngăn chặn <em>quá khớp</em> và thậm chí các điểm có sai số huấn luyện bằng nhau có thể có sai số tổng quát hóa khác nhau.</p>
</blockquote><p>Việc tinh chỉnh các tham số khác yêu cầu kiểm soát cả sai số huấn luyện và kiểm thử để chẩn đoán xem liệu mô hình của bạn có <em>quá khớp</em> hay <em>vị khớp</em> hay không, sau đó điều chỉnh dung lượng của nó một cách thích hợp.</p><p>Nếu sai số trên tập huấn luyện cao hơn tỷ lệ sai số mục tiêu của bạn, cách duy nhất là tăng dung lượng mô hình. Nếu bạn đang không sử dụng cơ chế kiểm soát và tự tin rằng thuật toán tối ưu hóa của mình đang hoạt động một cách chính xác, thì bạn phải thêm nhiều tầng hơn vào hệ thống mạng của bạn hoặc thêm nhiều đơn vị ẩn hơn vào mỗi tầng. Không may là điều này sẽ làm tăng chi phí tình toán của mô hình.</p><p>Nếu sai số trên tập kiểm thử cao hơn tỷ lệ sai số mục tiêu của bạn, bạn có hai phương án. Sai số kiểm thử này là tổng của sai số huấn luyện và khoảng cách giữa sai số huấn luyện và sai số kiểm thử. Sai số kiểm thử tối ưu được tìm ra bằng cách đánh đổi giữa các đại lượng này. Các mạng neuron thường thực hiện tốt nhất khi sai số huấn luyện rất thấp (và do đó, dung lượng của mô hình cao) và sai số kiểm thử chủ yếu được quyết định bởi khoảng cách giữa sai số huấn luyện và kiểm thử. Mục tiêu của bạn là làm giảm khoảng cách này sao cho sai số huấn luyện không tăng nhanh hơn so với tốc độ giảm của nó. Để giảm khoảng cách này, hãy thay đổi các siêu tham số kiểm soát để giảm dung lượng mô hình hiệu dụng, chẳng hạn như bằng cách thêm cơ chế tắt ngẫu nhiên hoặc suy giảm trọng số. Thường thì kết quả tốt nhất đến từ một mô hình lớn đã được kiểm soát hoá tốt, ví dụ bằng cách sử dụng tắt ngẫu nhiên.</p><p>Hầu hết siêu tham số có thể được thiết lập bằng cách xét xem liệu chúng làm tăng hay làm giảm dung lượng của mô hình. Một số ví dụ sẽ được mô tả trong Bảng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-76"><span class="MJXp-mn" id="MJXp-Span-77">11.1</span></span></span><script type="math/tex" id="MathJax-Element-23">11.1</script></span>.</p><table>
<thead>
<tr>
<th>Siêu tham số</th>
<th>Tăng dung lượng khi…</th>
<th>Lý do</th>
<th>Nhược điểm</th>
</tr>
</thead>
<tbody>
<tr>
<td>Số lượng đơn vị ẩn</td>
<td>Tăng lên</td>
<td>Tăng số lượng đơn vị ẩn làm tăng dung lượng biểu diễn của mô hình</td>
<td>Tăng số lượng đơn vị ẩn làm tăng thời gian và bộ nhớ cần thiết cho mọi phép toán trên mô hình</td>
</tr>
<tr>
<td>Tốc độ học</td>
<td>Được tinh chỉnh một cách tối ưu</td>
<td>Tốc độ học không phù hợp, quá cao hoặc quá thấp, khiến mô hình có dung lượng hiệu dụng thấp vì không được tối ưu hoá</td>
<td></td>
</tr>
<tr>
<td>Chiều rộng của hàm lõi tích chập</td>
<td>Tăng lên</td>
<td>Tăng chiều rộng của hàm lõi làm tăng số lượng tham số trong mô hình</td>
<td>Một hàm lõi rộng hơn dẫn đến sự thu hẹp kích thước của đầu ra, làm giảm độ hiệu quả của mô hình nếu bạn không sử dụng kỹ thuật thêm vùng đệm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-78"><span class="MJXp-mn" id="MJXp-Span-79">0</span></span></span><script type="math/tex" id="MathJax-Element-24">0</script></span>. Lõi rộng hơn sẽ yêu cầu nhiều bộ nhớ hơn để lưu trữ các tham số, đồng thời làm tăng thời gian chạy. Tuy nhiên, một đầu ra nhỏ hơn sẽ giảm bớt chi phí bộ nhớ.</td>
</tr>
<tr>
<td>Thêm vùng đệm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-80"><span class="MJXp-mn" id="MJXp-Span-81">0</span></span></span><script type="math/tex" id="MathJax-Element-25">0</script></span></td>
<td>Tăng lên</td>
<td>Thêm vùng đệm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-82"><span class="MJXp-mn" id="MJXp-Span-83">0</span></span></span><script type="math/tex" id="MathJax-Element-26">0</script></span> trước phép tích chập khiến kích thước của biểu diễn giữ ở mức lớn</td>
<td>Tăng chi phí thời gian và bộ nhớ của hầu hết các phép toán.</td>
</tr>
<tr>
<td>Hệ số suy giảm trọng số</td>
<td>Giảm đi</td>
<td>Giảm hệ số suy giảm trọng số giải phóng các tham số của mô hình, khiến chúng trở nên lớn hơn</td>
<td></td>
</tr>
<tr>
<td>Tỉ lệ tắt ngẫu nhiên</td>
<td>Giảm đi</td>
<td>Khi tắt đi ít đơn vị hơn, giúp các đơn vị có nhiều cơ hội “hợp sức” với nhau hơn để khớp với tập huấn luyện</td>
<td></td>
</tr>
</tbody>
</table><blockquote>
<p>Bảng 11.1: Ảnh hưởng của các siêu tham số lên độ hiệu quả của mô hình</p>
</blockquote><p>Khi tinh chỉnh thủ công các siêu tham số, đừng bao giờ quên mục đích cuối cùng của bạn: kết quả tốt trên tập kiểm thử. Thêm các cơ chế kiểm soát chỉ là một trong những cách để đạt được mục tiêu này. Chừng nào sai số trên tập huấn luyện vẫn thấp, thì bạn vẫn luôn có thể giảm sai số tổng quát hóa bằng cách thu thập thêm dữ liệu huấn luyện. Một phương pháp vét cạn để đảm bảo khả năng thành công là liên tục tăng dung lượng mô hình và kích thước tập huấn luyện cho đến khi tác vụ được giải quyết. Cách tiếp cận này dĩ nhiên sẽ làm tăng chi phí tính toán cho quá trình huấn luyện và suy luận, vì vậy nó chỉ khả thi khi thực hiện được cung cấp nguồn lực phù hợp. Về cơ bản, cách tiếp cận này có thể thất bại bởi vì những khó khăn khi tối ưu hóa. Tuy nhiên, trong nhiều bài toán mà việc tối ưu hóa không phải là rào cản đáng kể, sử dụng cách này sẽ giúp mô hình được lựa chọn một cách chính xác.</p><h2 id="1142-Các-thuật-toán-tối-ưu-hoá-siêu-tham-số-tự-động">11.4.2 Các thuật toán tối ưu hoá siêu tham số tự động</h2><p>Thuật toán học tập lí tưởng là thuật toán chỉ nhận vào một tập dữ liệu và tạo ra một hàm mà không cần tinh chỉnh thủ công các siêu tham số. Sự phổ biến của một số thuật toán học như hồi quy logit và các Máy vector hỗ trợ (SVM) một phần bắt nguồn từ khả năng hoạt động tốt của chúng chỉ với một hoặc hai siêu tham số cần tinh chỉnh. Mạng neuron đôi khi có thể hoạt động tốt với chỉ một lượng nhỏ siêu tham số cần tinh chỉnh, nhưng chúng còn làm tốt hơn nhiều khi được tinh chỉnh nhiều siêu tham số hơn. Tinh chỉnh siêu tham số thủ công có thể hữu hiệu khi người dùng có xuất phát điểm tốt, ví dụ như khi các siêu tham số đã được xác định bởi những người từng nghiên cứu trên ứng dụng và kiến trúc tương tự trước đó, hoặc khi người dùng có kinh nghiệm hàng tháng, hàng năm trời trong việc khám phá giá trị của các siêu tham số cho mạng neuron để áp dụng vào các tác vụ giống nhau. Tuy nhiên, trong nhiều ứng dụng, chúng ta thường không có xuất phát điểm tốt như vậy. Đối với những trường hợp này, thuật toán tự động có thể giúp ta tìm ra các siêu tham số hữu dụng.</p><p>Khi chúng ta suy nghĩ về cách người dùng thuật toán học tập tìm kiếm những giá trị tốt cho các siêu tham số, một bài toán tối ưu hoá xuất hiện: ta đang cố gắng tìm một giá trị của các siêu tham số có thể tối ưu một hàm mục tiêu nào đó, chẳng hạn như sai số xác thực, đôi khi bị ràng buộc (bởi giới hạn cho thời gian huấn luyện, bộ nhớ hoặc thời gian nhận dạng). Vì vậy, về nguyên tắc, ta có thể phát triển các thuật toán <em>tối ưu hoá siêu tham số</em>, bao quanh một thuật toán học tập và lựa chọn các siêu tham số của nó, từ đó giấu đi các siêu tham số của thuật toán khỏi người dùng. Tuy nhiên, thuật toán tối ưu siêu tham số lại có những siêu tham số của chính nó, chẳng hạn như miền giá trị của các siêu tham số đối với mỗi thuật toán học tập. Tuy nhiên, bộ siêu tham số thứ hai này thường có thể được lựa chọn một cách dễ dàng hơn, theo nghĩa là ta có thể đạt được hiệu suất chấp nhận được trên nhiều tác vụ khác nhau với cùng một bộ siêu tham số.</p><h2 id="1143-Tìm-kiếm-theo-lưới">11.4.3 Tìm kiếm theo lưới</h2><p>Khi cần chọn ít hơn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-84"><span class="MJXp-mn" id="MJXp-Span-85">3</span></span></span><script type="math/tex" id="MathJax-Element-27">3</script></span> siêu tham số, phương pháp <em>tìm kiếm theo lưới</em> (grid search) thường được sử dụng. Với mỗi siêu tham số, người dùng chọn một tập nhỏ hữu hạn các giá trị để thử nghiệm. Thuật toán tìm kiếm theo lưới sẽ huấn luyện một mô hình với mọi tổ hợp cấu hình của các siêu tham số trong tích Đề-các của tập các giá trị cho từng siêu tham số. Phép thử nào cho sai số trên tập xác thực tốt nhất sẽ được chọn. Phần bên trái của hình <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-86"><span class="MJXp-mn" id="MJXp-Span-87">11.2</span></span></span><script type="math/tex" id="MathJax-Element-28">11.2</script></span> mô tả một lưới các giá trị của các siêu tham số.</p><p>Một câu hỏi được đặt ra: danh sách các giá trị để thử nghiệm nên được lựa chọn như thế nào? Trong trường hợp các siêu tham số (đã được sắp xếp) có dạng số, phần tử nhỏ nhất và lớn nhất của mỗi danh sách được chọn một cách thận trọng, dựa trên kinh nghiệm trước đó ở các thực nghiệm tương tự, để đảm bảo giá trị tối ưu có thể nằm trong miền được chọn. Tìm kiếm theo lưới điển hình thường bao gồm việc chọn các giá trị xấp xỉ trên một thang logarit. Ví dụ, tốc độ học được chọn trong tập {<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-88"><span class="MJXp-mn" id="MJXp-Span-89">0.1</span><span class="MJXp-mo" id="MJXp-Span-90" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-91">0.01</span><span class="MJXp-mo" id="MJXp-Span-92" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-93"><span class="MJXp-mn" id="MJXp-Span-94" style="margin-right: 0.05em;">10</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-95" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-96">−</span><span class="MJXp-mn" id="MJXp-Span-97">3</span></span></span><span class="MJXp-mo" id="MJXp-Span-98" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-99"><span class="MJXp-mn" id="MJXp-Span-100" style="margin-right: 0.05em;">10</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-101" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-102">−</span><span class="MJXp-mn" id="MJXp-Span-103">4</span></span></span><span class="MJXp-mo" id="MJXp-Span-104" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-105"><span class="MJXp-mn" id="MJXp-Span-106" style="margin-right: 0.05em;">10</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-107" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-108">−</span><span class="MJXp-mn" id="MJXp-Span-109">5</span></span></span></span></span><script type="math/tex" id="MathJax-Element-29">0.1, 0.01, 10^{-3}, 10^{-4}, 10^{-5}</script></span>} hoặc số lượng của các đơn vị ẩn được chọn trong tập {<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-110"><span class="MJXp-mn" id="MJXp-Span-111">50</span><span class="MJXp-mo" id="MJXp-Span-112" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-113">100</span><span class="MJXp-mo" id="MJXp-Span-114" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-115">200</span><span class="MJXp-mo" id="MJXp-Span-116" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-117">500</span><span class="MJXp-mo" id="MJXp-Span-118" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-119">1000</span><span class="MJXp-mo" id="MJXp-Span-120" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-121">2000</span></span></span><script type="math/tex" id="MathJax-Element-30">50, 100, 200, 500, 1000, 2000</script></span>}.</p><p>Tìm kiếm theo lưới thường có hiệu quả tốt nhất khi được thực hiện lặp lại nhiều lần. Ví dụ, giả sử ta chạy tìm kiếm theo lưới cho một siêu tham số <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-122"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-123">α</span></span></span><script type="math/tex" id="MathJax-Element-31">\alpha</script></span> sử dụng các giá trị là {<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-124"><span class="MJXp-mo" id="MJXp-Span-125" style="margin-left: 0em; margin-right: 0.111em;">−</span><span class="MJXp-mn" id="MJXp-Span-126">1</span><span class="MJXp-mo" id="MJXp-Span-127" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-128">0</span><span class="MJXp-mo" id="MJXp-Span-129" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-130">1</span></span></span><script type="math/tex" id="MathJax-Element-32">-1, 0, 1</script></span>}. Nếu giá trị tốt nhất tìm được là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-131"><span class="MJXp-mn" id="MJXp-Span-132">1</span></span></span><script type="math/tex" id="MathJax-Element-33">1</script></span>, vậy thì chúng ta đã đánh giá thấp miền giá trị mà <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-133"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134">α</span></span></span><script type="math/tex" id="MathJax-Element-34">\alpha</script></span> tốt nhất có thể thuộc vào và nên chuyển lưới, chạy tìm kiếm giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-135"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-136">α</span></span></span><script type="math/tex" id="MathJax-Element-35">\alpha</script></span> khác trong tập {<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-137"><span class="MJXp-mn" id="MJXp-Span-138">1</span><span class="MJXp-mo" id="MJXp-Span-139" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-140">2</span><span class="MJXp-mo" id="MJXp-Span-141" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-142">3</span></span></span><script type="math/tex" id="MathJax-Element-36">1, 2, 3</script></span>}. Nếu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-143"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-144">α</span></span></span><script type="math/tex" id="MathJax-Element-37">\alpha</script></span> tốt nhất là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-mn" id="MJXp-Span-146">0</span></span></span><script type="math/tex" id="MathJax-Element-38">0</script></span>, ta có thể mong muốn thay đổi ước lượng cũ bằng cách phóng to và chạy tìm kiếm theo lưới trên tập {<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-147"><span class="MJXp-mo" id="MJXp-Span-148" style="margin-left: 0em; margin-right: 0.111em;">−</span><span class="MJXp-mn" id="MJXp-Span-149">0.1</span><span class="MJXp-mo" id="MJXp-Span-150" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-151">0</span><span class="MJXp-mo" id="MJXp-Span-152" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-153">0.1</span></span></span><script type="math/tex" id="MathJax-Element-39">-0.1, 0, 0.1</script></span>}.</p><p>Một vấn đề dễ thấy của tìm kiếm theo lưới là chi phí tính toán của nó tăng theo cấp số mũ của số lượng các siêu tham số. Nếu có <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-154"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155">m</span></span></span><script type="math/tex" id="MathJax-Element-40">m</script></span> siêu tham số, mỗi tham số có nhiều nhất <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-156"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157">n</span></span></span><script type="math/tex" id="MathJax-Element-41">n</script></span> giá trị, số lần thử cho huấn luyện và đánh giá có thể đạt tới <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-158"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-159">O</span><span class="MJXp-mo" id="MJXp-Span-160" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-161"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162" style="margin-right: 0.05em;">n</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-163" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164">m</span></span></span><span class="MJXp-mo" id="MJXp-Span-165" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-42">O(n^{m})</script></span>. Những lần thử có thể chạy song song và khai thác sự song song lỏng lẻo (gần như không cần có sự giao tiếp giữa các máy thực hiện tìm kiếm). Tuy nhiên, vì chi phí theo cấp số mũ của tìm kiếm theo lưới, ngay cả việc song song hoá cũng không mang lại kích thước hợp lý cho quá trình tìm kiếm.</p><p><img src="https://i.imgur.com/gXAOZsd.png" alt=""></p><blockquote>
<p>Hình 11.2: So sánh tìm kiếm theo lưới và tìm kiếm ngẫu nhiên. Với mục đích minh họa, chúng tôi biểu diễn hai siêu tham số, nhưng trên thực tế, chúng ta thường làm việc với số lượng siêu tham số lớn hơn. <em>(Hình trái)</em> Để thực thi tìm kiếm theo lưới, ta cung cấp một tập các giá trị cho mỗi siêu tham số. Thuật toán tìm kiếm thực hiện huấn luyện cho mọi thiết lập siêu tham số đồng thời trong tích có hướng của các tập giá trị này. <em>(Hình phải)</em> Để thực thi tìm kiếm ngẫu nhiên, chúng ta cung cấp một phân phối xác suất trên các cấu hình siêu tham số đồng thời. Hầu hết những siêu tham số này thường là độc lập với nhau. Các lựa chọn phổ biến cho phân phối của một siêu tham số bao gồm phân phối đều và logarit-đều (để biểu diễn phân phối logarit-đều, ta lấy <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-166"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-168">x</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169">p</span></span></span><script type="math/tex" id="MathJax-Element-43">exp</script></span> của một mẫu từ phân phối đều). Thuật toán tìm kiếm sẽ ngẫu nhiên tạo ra các cấu hình siêu tham số đồng thời và huấn luyện với từng giá trị của chúng. Tìm kiếm theo lưới và tìm kiếm ngẫu nhiên đều đánh giá sai số trên tập xác thực và trả về cấu hình tốt nhất. Hình trên mô tả trường hợp thường gặp, chỉ có một vài siêu tham số có ảnh hưởng lớn đến kết quả. Trong minh họa này, chỉ những siêu tham số trên trục ngang tạo ra ảnh hưởng lớn. Tìm kiếm theo lưới lãng phí chi phí tính toán một lượng theo cấp số mũ của số lượng siêu tham số không có ảnh hưởng, trong khi tìm kiếm ngẫu nhiên chỉ kiểm tra một giá trị duy nhất của mỗi siêu tham số có ảnh hưởng ở hầu hết các lần thử. Hình được vẽ lại với sự cho phép của Bergestra và Bengio, (2012).</p>
</blockquote><h2 id="1144-Tìm-kiếm-ngẫu-nhiên">11.4.4 Tìm kiếm ngẫu nhiên</h2><p>May mắn thay, có một thuật toán có thể lập trình dễ dàng, tiện dụng hơn và hội tụ về các giá trị tốt của các siêu tham số nhanh hơn để thay thế cho tìm kiếm theo lưới: tìm kiếm ngẫu nhiên [Bergstra and Bengio, 2012].</p><p>Tìm kiếm ngẫu nhiên hoạt động như sau. Đầu tiên, định nghĩa một phân phối biên cho mỗi siêu tham số. Ví dụ, phân phối Bernoulli hoặc mulltinoulli cho các siêu tham số dạng nhị phân hoặc rời rạc, hoặc phân phối đều trên thang logarit cho các giá trị thực dương của các siêu tham số. Ví dụ,</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-170"><span class="MJXp-mtable" id="MJXp-Span-171"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-172" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-173" style="text-align: center;"><span class="MJXp-mtext" id="MJXp-Span-174">log_learning_rate</span><span class="MJXp-mo" id="MJXp-Span-175" style="margin-left: 0.333em; margin-right: 0.333em;">∼</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176">u</span><span class="MJXp-mo" id="MJXp-Span-177" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mo" id="MJXp-Span-178" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mn" id="MJXp-Span-179">1</span><span class="MJXp-mo" id="MJXp-Span-180" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-181" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mn" id="MJXp-Span-182">5</span><span class="MJXp-mo" id="MJXp-Span-183" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span></span></span></span></span><script type="math/tex" id="MathJax-Element-44">\text{log_learning_rate} \sim u(-1,-5),\tag{11.2}</script></span><br>
<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-185"><span class="MJXp-mtable" id="MJXp-Span-186"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-187" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-188" style="text-align: center;"><span class="MJXp-mtext" id="MJXp-Span-189">learning_rate</span><span class="MJXp-mo" id="MJXp-Span-190" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-191"><span class="MJXp-mn" id="MJXp-Span-192" style="margin-right: 0.05em;">10</span><span class="MJXp-mtext MJXp-script" id="MJXp-Span-193" style="vertical-align: 0.5em;">log_learning_rate</span></span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><script type="math/tex" id="MathJax-Element-45">\text{learning_rate} = 10^\text{log_learning_rate}. \tag {11.3}</script></span></p><p>Trong đó, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-195"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196">u</span><span class="MJXp-mo" id="MJXp-Span-197" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-198">a</span><span class="MJXp-mo" id="MJXp-Span-199" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-200">b</span><span class="MJXp-mo" id="MJXp-Span-201" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-46">u(a,b)</script></span> là một mẫu của phân phối đều lấy trong khoảng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-202"><span class="MJXp-mo" id="MJXp-Span-203" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204">a</span><span class="MJXp-mo" id="MJXp-Span-205" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">b</span><span class="MJXp-mo" id="MJXp-Span-207" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-47">(a,b)</script></span>. Tương tự, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-208"><span class="MJXp-mtext" id="MJXp-Span-209">log_number_of_hidden_units</span></span></span><script type="math/tex" id="MathJax-Element-48">\text{log_number_of_hidden_units}</script></span> có thể là mẫu lấy từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-210"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">u</span><span class="MJXp-mo" id="MJXp-Span-212" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi" id="MJXp-Span-213">log</span><span class="MJXp-mo" id="MJXp-Span-214" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-215" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-216">50</span><span class="MJXp-mo" id="MJXp-Span-217" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi" id="MJXp-Span-219">log</span><span class="MJXp-mo" id="MJXp-Span-220" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-221" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-222">2000</span><span class="MJXp-mo" id="MJXp-Span-223" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-224" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-49">u(\log(50), \log(2000))</script></span>.</p><p>Không giống như phương pháp tìm kiếm theo lưới, ta <em>không nên rời rạc hoá</em> hoặc giới hạn các giá trị của các siêu tham số. Điều này giúp mô hình có thể khám phá một tập có nhiều giá trị hơn và tránh chi phí tính toán bổ sung. Trên thực tế, như đã minh họa trong hình 11.2, tìm kiếm ngẫu nhiên có thể hiệu quả hơn theo cấp số nhân so với tìm kiếm theo lưới, khi có một vài siêu tham số không ảnh hưởng nhiều đến việc đo lường hiệu suất. Điều này đã được nghiên cứu đầy đủ và chi tiết trong [Bergstra and Bengio, 2012], họ phát hiện ra rằng: tìm kiếm ngẫu nhiên giúp giảm sai số trên tập xác thực nhanh hơn nhiều so với tìm kiếm theo lưới, xét theo số lần thử được thực thi bởi mỗi thuật toán.</p><p>Giống với tìm kiếm theo lưới, ta thường cần chạy các phiên bản lặp lại của tìm kiếm ngẫu nhiên, để tìm kiếm tốt hơn dựa trên kết quả của lần thực thi đầu.</p><p>Lí do chính giải thích vì sao tìm kiếm ngẫu nhiên tìm được những lời giải tốt nhanh hơn tìm kiếm theo lưới là vì nó không lãng phí các lần thử. Không giống với tìm kiếm theo lưới, nó sẽ không thử khi hai giá trị của một siêu tham số (cho trước các giá trị của các siêu tham số còn lại) cho kết quả tương đương. Đối với tìm kiếm theo lưới, các siêu tham số khác sẽ có cùng giá trị với hai lần chạy này, trong khi với tìm kiếm ngẫu nhiên, chúng thường có các giá trị khác nhau. Vì vậy, nếu thay đổi giữa hai giá trị này không tạo ra nhiều khác biệt về sai số trên tập xác thực, tìm kiếm theo lưới sẽ lặp lại hai lần thử giống nhau một cách không cần thiết, trong khi đó, tìm kiếm ngẫu nhiên vẫn sẽ tạo ra hai lần thử độc lập với các siêu tham số còn lại.</p><h2 id="1145-Tối-ưu-hoá-siêu-tham-số-dựa-trên-mô-hình">11.4.5 Tối ưu hoá siêu tham số dựa trên mô hình</h2><p>Quá trình tìm kiếm một bộ siêu tham số tốt có thể được đưa về một bài toán tối ưu hoá. Các biến quyết định là các siêu tham số. Chi phí cần được tối ưu hoá chính là tập sai số xác thực từ quá trình huấn luyện khi sử dụng các siêu tham số này. Trong các trường hợp đơn giản, mà ta có thể tính toán gradient của một thang đo sai số khả vi nào đó trên tập xác thực theo các siêu tham số, ta chỉ cần tối ưu dựa theo gradient này [Bengio et al., 1999; Bengio, 2000; Maclaurin et al.,2015]. Thật không may, trong thực tế, gradient như vậy hầu như không sẵn có, bởi vì việc tính toán yêu cầu chi phí tính toán và bộ nhớ cao, hoặc vì các siêu tham số về bản chất có các tương tác không khả vi với tập sai số xác thực, như trong trường hợp các siêu tham số có giá trị rời rạc.</p><p>Để bù đắp cho sự thiếu hụt gradient này, ta có thể xây dựng một mô hình cho sai số trên tập xác thực, sau đó đề xuất các dự đoán cho các siêu tham số mới bằng cách thực hiện tối ưu hoá trong mô hình này. Phần lớn thuật toán tìm kiếm siêu tham số dựa trên mô hình đều sử dụng mô hình hồi quy Bayes để ước lượng giá trị mong muốn cho cả sai số trên tập xác thực cho mỗi siêu tham số lẫn độ không chắc chắn của kỳ vọng này. Vì vậy, quá trình tối ưu hoá phải đánh đổi giữa việc khám phá (đề xuất các siêu tham số có độ không chắc chắn cao, có thể dẫn đến sự cải thiện lớn nhưng cũng có thể hoạt động kém) và đào sâu (gợi ý các siêu tham số mà mô hình tự tin rằng sẽ thực thi tốt, cũng như bất kì siêu tham số nào nó đã gặp trước đây - thường là các siêu tham số rất giống với những siêu tham số nó đã gặp). Các cách tiếp cận hiện thời cho việc tối ưu hoá siêu tham số bao gồm Spearmint [Snoek et al., 2012],TPE [Bergstra et al., 2011] và SMAC [Hutter et al., 2011].</p><p>Hiện tại, là chúng tôi không thể đề xuất một cách rõ ràng thuật toán tối ưu hoá siêu tham số kiểu Bayes như là một công cụ được xây dựng để đạt những kết quả tốt hơn trong học sâu, hay là để thu được những kết quả này với chi phí thấp hơn. Tối ưu hoá siêu tham số kiểu Bayes có thể sánh ngang với con người, đôi khi tốt hơn, nhưng đôi khi lại thất bại thảm hại trên các bài toán khác. Dù sao đi nữa, cũng đáng để thử xem nó có hoạt động tốt trên một số bài toán đặc biệt hay không, nhưng nói chung nó vẫn chưa đáng tin cậy. Dù vậy, tối ưu hoá siêu tham số vẫn là một lĩnh vực nghiên cứu quan trọng, được dẫn dắt chủ yếu bởi nhu cầu của học sâu, nắm giữ nhiều lợi ích tiềm năng không chỉ trong toàn bộ lĩnh vực học máy mà còn trong phạm vi các ngành kỹ thuật nói chung.</p><p>Một nhược điểm thường thấy của hầu hết thuật toán tối ưu hoá siêu tham số phức tạp hơn so với tìm kiếm ngẫu nhiên là chúng yêu cầu một thực nghiệm huấn luyện phải được hoàn thành trước khi chúng có thể trích xuất bất cứ thông tin nào từ đó. Khiến cho chúng trở nên kém hiệu quả hơn nhiều so với tìm kiếm thủ công bởi một chuyên gia, xét trên khía cạnh số lượng thông tin có thể trích xuất được từ một thử nghiệm, bởi vì con người có thể sớm nhận ra nếu một vài tập siêu tham số có tính bất thường. Swersky và cộng sự. (2014) đã giới thiệu phiên bản đầu tiên của một thuật toán duy trì một tập hợp nhiều thử nghiệm. Tại các thời điểm khác nhau, thuật toán tối ưu hoá siêu tham số có thể lựa chọn bắt đầu một thử nghiệm mới, để “đóng băng” một thử nghiệm không có triển vọng đang chạy, hoặc để “rã đông” và phục hồi một thử nghiệm đã bị đóng băng trước đó nhưng có vẻ sẽ cung cấp thêm nhiều thông tin ở thời điểm hiện tại.</p><h1 id="115-Chiến-lược-gỡ-lỗi">11.5 Chiến lược gỡ lỗi</h1><p>Khi một hệ thống học máy hoạt động kém, thường thì khó có thể nói rằng hiệu năng kém là do bản chất của thuật toán hay do một lỗi nào đó trong quá trình thực thi thuật toán. Vì nhiều lý do, việc <em>gỡ lỗi</em> (debug) trong một hệ thống học máy là không đơn giản.</p><p>Trong hầu hết các trường hợp, chúng ta không biết một tiên nghiệm về hành vi dự định của thuật toán. Trên thực tế, cốt lõi của việc sử dụng học máy đó là nó sẽ khám phá ra hành vi hữu ích mà chúng ta không thể tự xác định được. Nếu chúng ta huấn luyện một mạng neuron trên một nhiệm vụ phân loại <em>mới</em> và đạt được sai số kiểm thử bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-225"><span class="MJXp-mn" id="MJXp-Span-226">5</span></span></span><script type="math/tex" id="MathJax-Element-50">5%</script></span>, chúng ta không có cách thức đơn giản nào để biết được rằng đây là hành vi kỳ vọng hay hành vi cận tối ưu.</p><p>Một khó khăn khác nữa là hầu hết các mô hình học máy đều có nhiều phần có tính chất thích ứng lẫn nhau. Nếu một phần bị lỗi, các phần khác có thể thích ứng và vẫn đạt được hiệu suất gần như chấp nhận được. Ví dụ, giả sử chúng ta đang huấn luyện một mạng neuron với một vài tầng được tham số hóa theo ma trận trọng số <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-227"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-228">W</span></span></span><script type="math/tex" id="MathJax-Element-51">\boldsymbol W</script></span> và các hệ số tự do <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-229"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-230">b</span></span></span><script type="math/tex" id="MathJax-Element-52">\boldsymbol b</script></span>. Giả sử thêm rằng chúng ta đã thực hiện thủ công phương pháp trượt gradient cho từng tham số riêng biệt, và đã tạo ra một lỗi khi cập nhật cho các hệ số tự do:</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-231"><span class="MJXp-mtable" id="MJXp-Span-232"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-233" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-234" style="text-align: center;"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-235">b</span><span class="MJXp-mo" id="MJXp-Span-236" style="margin-left: 0.333em; margin-right: 0.333em;">←</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-237">b</span><span class="MJXp-mo" id="MJXp-Span-238" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-239">α</span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-53"> \boldsymbol b \leftarrow \boldsymbol b - \alpha \tag {11.4}</script></span></p><p>trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-240"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-241">α</span></span></span><script type="math/tex" id="MathJax-Element-54">\alpha</script></span> là tốc độ học. Sai lầm ở đây là ta đã không sử dụng gradient trong phép cập nhật. Nó khiến cho hệ số tự do liên tục giảm giá trị trong suốt quá trình huấn luyện, điều này rõ ràng không phải là một cách thực thi đúng của bất kỳ thuật toán huấn luyện hợp lý nào. Nếu chỉ kiểm tra đầu ra của mô hình, ta khó có thể nhận ra ] lỗi này một cách rõ ràng. Bởi vì tuỳ thuộc vào phân bố của đầu vào, các trọng số có thể thay đổi để bù đắp cho các hệ số tự do.</p><p>Hầu hết các chiến lược gỡ lỗi cho mạng neuron được thiết kế để phát hiện một hoặc cả hai loại khó khăn này. Hoặc là thiết kế một trường hợp rất đơn giản sao cho hành vi đúng đắn là có thể dự đoán được, hoặc là thiết kế một phép thử để kiểm tra một phần của mạng neuron một cách cô lập.</p><p>Dưới đây là số phép thử quan trọng để gỡ lỗi mô hình:</p><p><em>Trực quan hóa hoạt động của mô hình</em>: Khi huấn luyện một mô hình phát hiện đối tượng trong hình ảnh, hãy xem một số hình ảnh hiển thị, cùng với các phát hiện mà mô hình đề xuất. Khi huấn luyện một <em>mô hình sinh hội thoại</em> (generative model of speech), hãy lắng nghe một số mẫu giọng nói mà mô hình tạo ra. Điều này có vẻ hiển nhiên, nhưng rất dễ bị bỏ quên trong thực tiễn khi chỉ nhìn vào các thang đo hiệu suất định lượng như độ chính xác hoặc logarit hàm hợp lý. Trực tiếp quan sát mô hình học máy thực hiện tác vụ của nó sẽ giúp ta xác định xem hiệu suất định lượng mà nó đạt được có hợp lý hay không. Lỗi đánh giá có thể là một trong số những lỗi “huỷ diệt” nhất vì chúng có thể đánh lừa bạn, khiến cho bạn tin rằng hệ thống đang hoạt động tốt trong khi sự thật không như vậy.</p><p><em>Trực quan hóa những lỗi tệ nhất</em>: Hầu hết các mô hình đều có thể xuất ra một thang đo độ tự tin cho tác vụ chúng thực hiện. Ví dụ, bộ phân loại dựa trên tầng softmax đầu ra chỉ định một xác suất cho mỗi lớp. Do đó, xác suất được gán cho lớp có khả năng cao nhất cho ta một ước lượng về độ tự tin của mô hình trong quyết định phân loại của nó. Thông thường, kết quả của quá trình huấn luyện hợp lý cực đại cho những giá trị này được đánh giá cao hơn là xác suất chính xác của việc dự đoán đúng, nhưng chúng có phần hữu ích theo nghĩa: những mẫu ít có khả năng được gán nhãn chính xác sẽ nhận xác suất nhỏ hơn trong mô hình. Bằng cách xem xét những mẫu huấn luyện khó có thể được mô hình hoá một cách chính xác nhất, người ta thường khám phá ra các vấn đề trong cách mà dữ liệu được tiền xử lý hoặc gán nhãn. Ví dụ: hệ thống phiên mã địa chỉ Street View ban đầu gặp một vấn đề rằng hệ thống phát hiện số địa chỉ sẽ cắt hình ảnh quá sát và bỏ qua một vài chữ số. Mạng phiên mã sau đó đã gán xác suất rất thấp cho câu trả lời đúng trên những hình ảnh này. Khi sắp xếp những hình ảnh bị phát hiện sai có độ tự tin lớn nhất, chúng tôi thấy rằng có một vấn đề mang tính hệ thống trong khâu cắt ảnh. Sửa đổi hệ thống để cắt hình ảnh rộng hơn đã mang lại hiệu suất tốt hơn nhiều cho hệ thống tổng thể, mặc dù mạng phiên mã cần xử lý sự biến động lớn hơn về vị trí và độ rộng của số địa chỉ.</p><p><em>Suy luận về phần mềm bằng cách sử dụng sai số huấn luyện và kiểm thử</em>: Việc xác định xem nền tảng bên trong của phần mềm có được thực thi đúng hay không thường là rất khó khăn. Ta có thể thu được một số manh mối từ sai số huấn luyện và kiểm thử. Nếu sai số huấn luyện thấp nhưng sai số kiểm thử cao, thì có khả năng là quá trình huấn luyện hoạt động chính xác và mô hình đã quá khớp vì các lý do cơ bản về mặt thuật toán. Một khả năng khác là sai số kiểm thử được đánh giá không chính xác do có vấn đề ở khâu lưu trữ mô hình sau khi hoàn tất huấn luyện và sau đó tải nó lên để kiểm tra tập xác thực, hoặc vì dữ liệu kiểm thử đã được chuẩn bị không giống với dữ liệu huấn luyện (theo nghĩa được lấy từ các phân phối sinh dữ liệu khác nhau). Nếu cả sai số huấn luyện và kiểm thử đều cao, rất khó để xác định xem nguyên nhân là do lỗi của phần mềm hay là mô hình bị vị khớp. Kịch bản này đòi hỏi ta cần thực hiện thêm những phép thử khác, sẽ được mô tả tiếp theo.</p><p><em>Khớp với một tập dữ liệu rất nhỏ nào đó</em>: Nếu sai số trên tập huấn luyện cao, hãy xác định xem điều này đến từ lỗi do vị khớp hay lỗi phần mềm. Thông thường, ngay cả các mô hình nhỏ cũng có thể được đảm bảo việc khớp với một tập dữ liệu nhỏ. Ví dụ, một tập dữ liệu phân loại với chỉ một mẫu có thể được khớp bằng cách thiết lập chính xác các hệ số tự do của tầng đầu ra. Thông thường, nếu bạn không thể huấn luyện được một bộ phân loại để gán nhãn chính xác cho một mẫu đơn giản, một bộ tự mã hóa tái tạo thành công một mẫu đơn lẻ với độ trung thực cao, hay một mô hình sinh mẫu tạo ra các mẫu nhất quán với một mẫu duy nhất, nghĩa là có một lỗi phần mềm nào đó ngăn chặn quá trình tối ưu hóa thành công trên tập huấn luyện. Phép thử này có thể được mở rộng thành một tập dữ liệu nhỏ với vài mẫu thay vì chỉ một.</p><p><em>So sánh các đạo hàm được lan truyền ngược với các đạo hàm số học</em>: Nếu bạn đang sử dụng một cài đặt yêu cầu thực thi các phép tính gradient của riêng bạn, hoặc nếu bạn đang thêm một phép toán mới vào thư viện tính vi phân và phải định nghĩa phương pháp <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-242"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-244">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-246">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-247">p</span></span></span><script type="math/tex" id="MathJax-Element-55">bprop</script></span> của nó, thì một nguồn gây lỗi thường gặp đó là sự không chính xác khi triển khai biểu thức tính gradient. Một cách để xác minh sự chính xác của những đạo hàm này là so sánh các đạo hàm được tính bởi phép vi phân tự động của bạn với các đạo hàm được tính bởi <em>sai phân hữu hạn</em> (finite difference). Bởi vì</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-261"><span class="MJXp-mtable" id="MJXp-Span-262"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-263" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-264" style="text-align: center;"><span class="MJXp-msup" id="MJXp-Span-265"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-267" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-268" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269">x</span><span class="MJXp-mo" id="MJXp-Span-270" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-271" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-munderover" id="MJXp-Span-272"><span><span class="MJXp-mo" id="MJXp-Span-273" style="margin-left: 0.333em; margin-right: 0.333em;">lim</span></span><span class="MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-274" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">ϵ</span><span class="MJXp-mo" id="MJXp-Span-276">→</span><span class="MJXp-mn" id="MJXp-Span-277">0</span></span></span></span><span class="MJXp-mstyle" id="MJXp-Span-278"><span class="MJXp-mfrac" id="MJXp-Span-279" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">f</span><span class="MJXp-mo" id="MJXp-Span-281" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-282">x</span><span class="MJXp-mo" id="MJXp-Span-283" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-284">ϵ</span><span class="MJXp-mo" id="MJXp-Span-285" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-286" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">f</span><span class="MJXp-mo" id="MJXp-Span-288" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">x</span><span class="MJXp-mo" id="MJXp-Span-290" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">ϵ</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-292" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-56"> f'(x) = \lim_{\epsilon \to 0} \dfrac{f(x + \epsilon ) - f(x)}{\epsilon}.
\tag{11.5}</script></span></p><p>nên ta có thể xấp xỉ đạo hàm bằng cách sử dụng một <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-293"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">ϵ</span></span></span><script type="math/tex" id="MathJax-Element-57">\epsilon</script></span> nhỏ, hữu hạn</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-295"><span class="MJXp-mtable" id="MJXp-Span-296"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-297" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-298" style="text-align: center;"><span class="MJXp-msup" id="MJXp-Span-299"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-301" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-302" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-303">x</span><span class="MJXp-mo" id="MJXp-Span-304" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-305" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mstyle" id="MJXp-Span-306"><span class="MJXp-mfrac" id="MJXp-Span-307" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-308">f</span><span class="MJXp-mo" id="MJXp-Span-309" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310">x</span><span class="MJXp-mo" id="MJXp-Span-311" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">ϵ</span><span class="MJXp-mo" id="MJXp-Span-313" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-314" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-315">f</span><span class="MJXp-mo" id="MJXp-Span-316" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317">x</span><span class="MJXp-mo" id="MJXp-Span-318" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">ϵ</span></span></span></span></span></span></span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-58"> f'(x) \approx \dfrac{f(x + \epsilon ) - f(x)}{\epsilon}
\tag{11.6}</script></span></p><p>Ta có thể cải thiện độ chính xác của phép lấy xấp xỉ bằng cách sử dụng <em>sai phân trung tâm</em> (centered difference):</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-320"><span class="MJXp-mtable" id="MJXp-Span-321"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-322" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-323" style="text-align: center;"><span class="MJXp-msup" id="MJXp-Span-324"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-326" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-327" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">x</span><span class="MJXp-mo" id="MJXp-Span-329" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-330" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mstyle" id="MJXp-Span-331"><span class="MJXp-mfrac" id="MJXp-Span-332" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">f</span><span class="MJXp-mo" id="MJXp-Span-334" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">x</span><span class="MJXp-mo" id="MJXp-Span-336" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mstyle" id="MJXp-Span-337"><span class="MJXp-mfrac" id="MJXp-Span-338" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-339">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-340">2</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-341">ϵ</span><span class="MJXp-mo" id="MJXp-Span-342" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-343" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-344">f</span><span class="MJXp-mo" id="MJXp-Span-345" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346">x</span><span class="MJXp-mo" id="MJXp-Span-347" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mstyle" id="MJXp-Span-348"><span class="MJXp-mfrac" id="MJXp-Span-349" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-350">1</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-351">2</span></span></span></span></span></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-352">ϵ</span><span class="MJXp-mo" id="MJXp-Span-353" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354">ϵ</span></span></span></span></span></span></span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-59"> f'(x) \approx \dfrac{f(x + \dfrac{1}{2}\epsilon ) - f(x - \dfrac{1}{2}\epsilon )}{\epsilon}
\tag{11.7}</script></span></p><p>Kích thước nhiễu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-355"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-356">ϵ</span></span></span><script type="math/tex" id="MathJax-Element-60">\epsilon</script></span> phải đủ lớn để đảm bảo rằng nhiễu loạn không bị làm tròn xuống quá nhiều bởi các tính toán với số có phần thập phân hữu hạn.</p><p>Thông thường, chúng ta sẽ muốn kiểm tra gradient hoặc Jacob của một của hàm mà đầu ra có dạng vector <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-357"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">g</span><span class="MJXp-mo" id="MJXp-Span-359" style="margin-left: 0.111em; margin-right: 0.167em;">:</span><span class="MJXp-msubsup" id="MJXp-Span-360"><span class="MJXp-mrow" id="MJXp-Span-361" style="margin-right: 0.05em;"><span class="MJXp-mi undefined" id="MJXp-Span-362">R</span></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-363" style="vertical-align: 0.5em;">m</span></span><span class="MJXp-mo" id="MJXp-Span-364" style="margin-left: 0.333em; margin-right: 0.333em;">→</span><span class="MJXp-msubsup" id="MJXp-Span-365"><span class="MJXp-mrow" id="MJXp-Span-366" style="margin-right: 0.05em;"><span class="MJXp-mi undefined" id="MJXp-Span-367">R</span></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-368" style="vertical-align: 0.5em;">n</span></span></span></span><script type="math/tex" id="MathJax-Element-61">g: \mathbb{R}^m \to \mathbb{R}^n</script></span>. Thật không may, sai phân hữu hạn chỉ cho phép lấy một đạo hàm duy nhất tại một thời điểm. Chúng ta có thể thực hiện sai phân hữu hạn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-369"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-370">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-371">n</span></span></span><script type="math/tex" id="MathJax-Element-62">mn</script></span> lần để lấy tất cả các đạo hàm riêng của hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-372"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373">g</span></span></span><script type="math/tex" id="MathJax-Element-63">g</script></span>, hoặc áp dụng phép thử cho một hàm mới mà nó sử dụng các phép chiếu ngẫu nhiên lên cả đầu vào và đầu ra của hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-374"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375">g</span></span></span><script type="math/tex" id="MathJax-Element-64">g</script></span>. Ví dụ, chúng ta có thể áp dụng phép thử cho thực thi để tính các đạo hàm của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-376"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-377">f</span><span class="MJXp-mo" id="MJXp-Span-378" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379">x</span><span class="MJXp-mo" id="MJXp-Span-380" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-65">f(x)</script></span>, trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-390"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391">f</span><span class="MJXp-mo" id="MJXp-Span-392" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393">x</span><span class="MJXp-mo" id="MJXp-Span-394" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-395" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-396"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-397" style="margin-right: 0.05em;">u</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-398" style="vertical-align: 0.5em;">⊺</span></span><span class="MJXp-mspace" id="MJXp-Span-399" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">g</span><span class="MJXp-mo" id="MJXp-Span-401" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-402">v</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-403">x</span><span class="MJXp-mo" id="MJXp-Span-404" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-66">f(x) = u^\intercal \,g(\boldsymbol{v}x)</script></span>, với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-405"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-406">u</span></span></span><script type="math/tex" id="MathJax-Element-67">u</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-407"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">v</span></span></span><script type="math/tex" id="MathJax-Element-68">v</script></span> là các vector được chọn ngẫu nhiên. Việc tính toán đạo hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-409"><span class="MJXp-msup" id="MJXp-Span-410"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-411" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-412" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-413" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-414">x</span><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-69">f’(x)</script></span> một cách chính xác đòi hỏi quá trình lan truyền ngược thông qua <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-416"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-417">g</span></span></span><script type="math/tex" id="MathJax-Element-70">g</script></span> một cách chính xác, nhưng cũng hiệu quả với sai phân hữu hạn vì <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-418"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-419">f</span></span></span><script type="math/tex" id="MathJax-Element-71">f</script></span> chỉ có một đầu vào và một đầu ra. Thông thường, lặp lại phép thử này cho nhiều hơn một giá trị của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-420"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-421">u</span></span></span><script type="math/tex" id="MathJax-Element-72">\boldsymbol{u}</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-422"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-423">v</span></span></span><script type="math/tex" id="MathJax-Element-73">\boldsymbol{v}</script></span> là một ý tưởng hay vì nó giúp giảm nguy cơ các lỗi bị bỏ qua trong phép thử, trong đó các lỗi này trực giao với phép chiếu ngẫu nhiên.</p><p>Nếu có thể tính toán số trên các số phức, có một cách rất hiệu quả để ước lượng gradient bằng cách sử dụng các số phức làm đầu vào của hàm [Squire and Trapp, 1998]. Phương pháp này dựa trên quan sát rằng</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-424"><span class="MJXp-mtable" id="MJXp-Span-425"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-426" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-427" style="text-align: right;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-428">f</span><span class="MJXp-mo" id="MJXp-Span-429" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-430">x</span><span class="MJXp-mo" id="MJXp-Span-431" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-432">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-433">ϵ</span><span class="MJXp-mo" id="MJXp-Span-434" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtd" id="MJXp-Span-435" style="padding-left: 0em; text-align: left;"><span class="MJXp-mi" id="MJXp-Span-436"></span><span class="MJXp-mo" id="MJXp-Span-437" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-438">f</span><span class="MJXp-mo" id="MJXp-Span-439" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-440">x</span><span class="MJXp-mo" id="MJXp-Span-441" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-442" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-443">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-444">ϵ</span><span class="MJXp-msup" id="MJXp-Span-445"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-446" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-447" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-448" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-449">x</span><span class="MJXp-mo" id="MJXp-Span-450" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-451" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-452">O</span><span class="MJXp-mo" id="MJXp-Span-453" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-454"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-455" style="margin-right: 0.05em;">ϵ</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-456" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-457" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-458" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span><span class="MJXp-mlabeledtr" id="MJXp-Span-459" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-460" style="padding-top: 0.3em; text-align: right;"><span class="MJXp-mtext" id="MJXp-Span-461">real</span><span class="MJXp-mo" id="MJXp-Span-462" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-463">f</span><span class="MJXp-mo" id="MJXp-Span-464" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-465">x</span><span class="MJXp-mo" id="MJXp-Span-466" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-467">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-468">ϵ</span><span class="MJXp-mo" id="MJXp-Span-469" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-470" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mtd" id="MJXp-Span-471" style="padding-left: 0em; padding-top: 0.3em; text-align: left;"><span class="MJXp-mi" id="MJXp-Span-472"></span><span class="MJXp-mo" id="MJXp-Span-473" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-474">f</span><span class="MJXp-mo" id="MJXp-Span-475" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-476">x</span><span class="MJXp-mo" id="MJXp-Span-477" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-478" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-479">O</span><span class="MJXp-mo" id="MJXp-Span-480" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-481"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-482" style="margin-right: 0.05em;">ϵ</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-483" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-484" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-485" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mspace" id="MJXp-Span-486" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-487">imag</span><span class="MJXp-mo" id="MJXp-Span-488" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mfrac" id="MJXp-Span-489" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-490">f</span><span class="MJXp-mo" id="MJXp-Span-491" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492">x</span><span class="MJXp-mo" id="MJXp-Span-493" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-494">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-495">ϵ</span><span class="MJXp-mo" id="MJXp-Span-496" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-497">ϵ</span><span class="MJXp-mo" id="MJXp-Span-498" style="margin-left: 0em; margin-right: 0em;">)</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-499" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-500" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msup" id="MJXp-Span-501"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-502" style="margin-right: 0.05em;">f</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-503" style="vertical-align: 0.5em;">′</span></span><span class="MJXp-mo" id="MJXp-Span-504" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-505">x</span><span class="MJXp-mo" id="MJXp-Span-506" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-507" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-508">O</span><span class="MJXp-mo" id="MJXp-Span-509" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-510"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511" style="margin-right: 0.05em;">ϵ</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-512" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-513" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-514" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span></span></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-74">\begin{align}
f(x + i \epsilon) &= f(x) + i \epsilon f’(x) + O(\epsilon^2),   \tag{11.8}
\\
\text{real}(f(x+ i \epsilon)) &= f(x) + O(\epsilon^2), \,\text{imag}(\frac{f(x+ i \epsilon )}{\epsilon)}) = f’(x) + O(\epsilon^2) ,   \tag{11.9}
\end{align}</script></span></p><p>trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-515"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-516">i</span><span class="MJXp-mo" id="MJXp-Span-517" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msqrt" id="MJXp-Span-518"><span class="MJXp-surd"><span style="font-size: 134%; margin-top: 0.104em;">√</span></span><span class="MJXp-root"><span class="MJXp-rule" style="border-top: 0.08em solid;"></span><span class="MJXp-box"><span class="MJXp-mo" id="MJXp-Span-519" style="margin-left: 0em; margin-right: 0.111em;">−</span><span class="MJXp-mn" id="MJXp-Span-520">1</span></span></span></span></span></span><script type="math/tex" id="MathJax-Element-75">i = \sqrt{− 1}</script></span>. Không giống như trong trường hợp giá trị thực ở trên, sẽ không có sự triệt tiêu nào khi lấy vi phân giữa các giá trị của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-521"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-522">f</span></span></span><script type="math/tex" id="MathJax-Element-76">f</script></span> tại các điểm khác nhau. Điều này cho phép ta sử dụng những giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-523"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-524">ϵ</span></span></span><script type="math/tex" id="MathJax-Element-77">\epsilon</script></span> siêu nhỏ chẳng hạn như <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-525"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-526">ϵ</span><span class="MJXp-mo" id="MJXp-Span-527" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-528"><span class="MJXp-mn" id="MJXp-Span-529" style="margin-right: 0.05em;">10</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-530" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-531">−</span><span class="MJXp-mn" id="MJXp-Span-532">150</span></span></span></span></span><script type="math/tex" id="MathJax-Element-78">\epsilon = 10^{−150}</script></span>, khiến cho sai số <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-533"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-534">O</span><span class="MJXp-mo" id="MJXp-Span-535" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-536"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-537" style="margin-right: 0.05em;">ϵ</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-538" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-539" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-79">O (\epsilon ^2)</script></span> trở nên không đáng kể so với tất cả các mục đích thực tế.</p><p><em>Giám sát biểu đồ tần suất của các tín hiệu kích hoạt và gradient</em>: Sẽ là rất hữu ích khi chúng ta trực quan hóa số liệu thống kê của các gradient và tín hiệu kích hoạt trên mạng neuron, được thu thập qua một số lượng lớn các vòng lặp huấn luyện (có thể là một epoch - tức một lượt duyệt toàn bộ các dữ liệu). Giá trị tiền kích hoạt của các đơn vị ẩn có thể cho biết liệu các đơn vị có bị bão hòa hay không, hoặc tần suất hoạt động của các đơn vị này. Ví dụ, đối với các bộ tuyến tính hiệu chỉnh, chúng có thường bị tắt đi hay không? Có đơn vị nào luôn luôn bị tắt không? Đối với các đơn vị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-540"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-541">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-542">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-543">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-544">h</span></span></span><script type="math/tex" id="MathJax-Element-80">tanh</script></span>, trung bình trị tuyệt đối của các giá trị tiền kích hoạt cho ta biết mức độ bão hòa của những đơn vị này. Trong một mạng đa tầng, nơi mà các gradient được lan truyền bùng nổ hoặc tiêu biến quá nhanh, quá trình tối ưu hóa có thể bị cản trở. Cuối cùng, việc so sánh độ lớn của gradient theo từng tham số với chính độ lớn của các tham số này cũng là rất hữu ích. Theo đề xuất của Bottou (2015), ta muốn độ lớn của các cập nhật tham số trên một lô nhỏ nên có giá trị khoảng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-545"><span class="MJXp-mn" id="MJXp-Span-546">1</span></span></span><script type="math/tex" id="MathJax-Element-81">1%</script></span> độ lớn của tham số, thay vì <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-547"><span class="MJXp-mn" id="MJXp-Span-548">50</span></span></span><script type="math/tex" id="MathJax-Element-82">50%</script></span> hoặc <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-549"><span class="MJXp-mn" id="MJXp-Span-550">0</span><span class="MJXp-mo" id="MJXp-Span-551" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-552">001</span></span></span><script type="math/tex" id="MathJax-Element-83">0,001%</script></span> (khiến quá trình cập nhật tham số trở nên rất chậm chạp). Đôi khi, có một nhóm tham số đang được cập nhật với tốc độ tốt trong khi các nhóm khác bị mắc kẹt. Khi dữ liệu là thưa (như trong ngôn ngữ tự nhiên), một vài tham số có thể hiếm khi được cập nhật, và điều này nên được lưu ý khi giám sát quá trình tiến triển của chúng.</p><p>Cuối cùng, nhiều thuật toán học sâu cho ta một số bảo đảm về kết quả đạt được ở mỗi bước. Ví dụ, trong phần III của cuốn sách này, một số thuật toán suy luận gần đúng hoạt động bằng cách sử dụng các phương pháp đại số cho bài toán tối ưu. Thông thường, những thuật toán này có thể được gỡ lỗi bằng cách kiểm tra từng bảo đảm. Một số đảm bảo mà một số thuật toán tối ưu đưa ra bao gồm: hàm mục tiêu sẽ không tăng sau một bước của thuật toán, gradient tương ứng với một tập con nào đó của các biến sẽ bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-553"><span class="MJXp-mn" id="MJXp-Span-554">0</span></span></span><script type="math/tex" id="MathJax-Element-84">0</script></span> sau mỗi bước của thuật toán, hay gradient tương ứng với tất cả các biến sẽ bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-555"><span class="MJXp-mn" id="MJXp-Span-556">0</span></span></span><script type="math/tex" id="MathJax-Element-85">0</script></span> khi mạng hội tụ. Thông thường, do sai số làm tròn, các điều kiện này sẽ không được đảm bảo chính xác trong một máy tính kỹ thuật số, vì vậy các phép thử thử gỡ lỗi nên bao gồm thêm một số tham số dung sai.</p><h1 id="116-Ví-dụ-Nhận-dạng-số-có-nhiều-chữ-số">11.6 Ví dụ: Nhận dạng số có nhiều chữ số</h1><p>Để cung cấp một mô tả hoàn chỉnh từ đầu đến cuối về cách áp dụng phương pháp luận về thiết kế trong thực tế, chúng tôi sẽ trình bày một mô tả ngắn gọn về hệ thống phiên mã địa chỉ Street View, từ quan điểm thiết kế các thành phần học sâu. Rõ ràng, nhiều thành phần khác trong hệ thống hoàn chỉnh, chẳng hạn như các xe ô tô được thiết lập dành riêng cho Street View, hạ tầng cơ sở dữ liệu, v.v…, cũng có vai trò vô cùng quan trọng.</p><p>Từ quan điểm của tác vụ học máy, quá trình thực hiện bắt đầu với việc thu thập dữ liệu. Những chiếc xe thu thập dữ liệu thô, sau đó chúng được gán nhãn bởi con người. Tác vụ phiên mã bắt đầu khi đã có một lượng lớn dữ liệu được chọn lọc, bao gồm cả việc sử dụng các kỹ thuật học máy khác để <em>phát hiện</em> số nhà trước khi phiên mã chúng.</p><p>Dự án phiên mã bắt đầu với việc lựa chọn các thang đo hiệu suất và giá trị mong muốn của các thang đo này. Một nguyên tắc chung quan trọng là điều chỉnh lựa chọn thang đo cho mục tiêu kinh doanh của dự án. Một bản đồ chỉ hữu ích khi nó có độ chính xác cao, vì vậy độ chính xác cao là yêu cầu rất quan trọng trong dự án này. Cụ thể, mục tiêu của chúng tôi là đạt được mức độ chính xác ở mức như con người, tức là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-557"><span class="MJXp-mn" id="MJXp-Span-558">98</span><span class="MJXp-mi" id="MJXp-Span-559">%</span></span></span><script type="math/tex" id="MathJax-Element-86">98\%</script></span>. Mức độ chính xác này có thể không phải lúc nào cũng khả thi. Để đạt được nó, hệ thống phiên mã Street View phải hi sinh độ phủ. Do đó, độ phủ trở thành thước đo hiệu suất chính, được tối ưu hóa trong suốt dự án, với độ chính xác giữ cố định ở mức <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-560"><span class="MJXp-mn" id="MJXp-Span-561">98</span><span class="MJXp-mi" id="MJXp-Span-562">%</span></span></span><script type="math/tex" id="MathJax-Element-87">98\%</script></span>. Khi mạng tích chập được cải thiện, việc giảm ngưỡng tin cậy - đại lượng quyết định mạng sẽ thực hiện phiên mã hay từ chối một địa chỉ - để đạt được độ bao phủ vượt quá mục tiêu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-563"><span class="MJXp-mn" id="MJXp-Span-564">95</span></span></span><script type="math/tex" id="MathJax-Element-88">95%</script></span> là hoàn toàn khả thi.</p><p>Sau khi chọn các mục tiêu định lượng, bước tiếp theo trong phương pháp luận mà chúng tôi đề xuất là nhanh chóng thiết lập một mô hình cơ sở hợp lý. Đối với các tác vụ về thị giác, lựa chọn phổ biển là một mạng tích chập với các đơn vị tuyến tính hiệu chỉnh. Dự án phiên mã bắt đầu với một mô hình như vậy. Vào thời điểm đó, một mạng tích chập cho đầu ra là một chuỗi các dự đoán vẫn chưa phổ biến. Để bắt đầu với mô hình cơ sở đơn giản nhất có thể, cài đặt đầu tiên cho tầng đầu ra của mô hình bao gồm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-565"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-566">n</span></span></span><script type="math/tex" id="MathJax-Element-89">n</script></span> đơn vị softmax khác nhau để dự đoán một chuỗi gồm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-567"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-568">n</span></span></span><script type="math/tex" id="MathJax-Element-90">n</script></span> ký tự. Các đơn vị softmax này được huấn luyện chính xác tương tự như với tác vụ phân lớp, được huấn luyện độc lập với nhau.</p><p>Phương pháp mà chúng tôi khuyến nghị là lặp đi lặp lại việc tinh chỉnh mô hình cơ sở và kiểm tra xem mỗi thay đổi có cải thiện được gì hay không. Thay đổi đầu tiên trong hệ thống phiên mã Street View được thúc đẩy bởi lý thuyết về thang đo độ phủ và cấu trúc của dữ liệu. Đặc biệt, mạng từ chối phân loại một đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-569"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-570">x</span></span></span><script type="math/tex" id="MathJax-Element-91">\boldsymbol{x}</script></span> khi thấy xác suất của chuỗi đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-571"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-572">p</span><span class="MJXp-mo" id="MJXp-Span-573" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-574">y</span><span class="MJXp-mrow" id="MJXp-Span-575"><span class="MJXp-mo" id="MJXp-Span-576" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-577">x</span><span class="MJXp-mo" id="MJXp-Span-578" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-579" style="margin-left: 0.333em; margin-right: 0.333em;">&lt;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-580">t</span></span></span><script type="math/tex" id="MathJax-Element-92">p(\boldsymbol{y} | \boldsymbol{x}) < t</script></span>, với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-581"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-582">t</span></span></span><script type="math/tex" id="MathJax-Element-93">t</script></span> là một ngưỡng được định trước. Ban đầu, định nghĩa của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-583"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-584">p</span><span class="MJXp-mo" id="MJXp-Span-585" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-586">y</span><span class="MJXp-mrow" id="MJXp-Span-587"><span class="MJXp-mo" id="MJXp-Span-588" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-589">x</span><span class="MJXp-mo" id="MJXp-Span-590" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><script type="math/tex" id="MathJax-Element-94">p(\boldsymbol{y} | \boldsymbol{x})</script></span> đơn giản chỉ là dựa trên phép nhân toàn bộ các đầu ra softmax với nhau. Điều này thúc đẩy sự phát triển của một tầng đầu ra chuyên biệt và hàm chi phí thực sự tính toán một logarit hàm hợp lý có chủ đích. Cách tiếp cận này cho phép mô hình thực hiện cơ chế từ chối mẫu đầu vào nhằm thu được hiệu suất tốt hơn.</p><p>Tại thời điểm này, độ phủ vẫn ở dưới mức <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-591"><span class="MJXp-mn" id="MJXp-Span-592">90</span><span class="MJXp-mi" id="MJXp-Span-593">%</span></span></span><script type="math/tex" id="MathJax-Element-95">90\%</script></span>, mặc dù hướng tiếp cận này không có vấn đề rõ ràng nào về mặt lý thuyết. Do đó, phương pháp luận của chúng tôi đề xuất đo lường tập huấn luyện và tập kiểm thử để xác định xem vấn đề là quá khớp hay vị khớp. Trong trường hợp này, sai số tập trên tập huấn luyện và trên tập kiểm thử đã gần giống nhau. Thật vậy, lý do chính mà dự án này tiến hành rất suôn sẻ là sự sẵn có của một tập dữ liệu với hàng chục triệu mẫu đã được gán nhãn. Sai số trên tập huấn luyện và kiểm thử rất gần nhau, gợi ý rằng vấn đề ở đây là vị khớp hoặc dữ liệu huấn luyện. Một trong những chiến lược gỡ lỗi đã được đề xuất là trực quan hóa các lỗi tệ nhất của mô hình. Trong trường hợp này, có nghĩa là trực quan hóa các phiên mã không chính xác trong tập huấn luyện mà mô hình đã đưa ra với độ tin cậy cao nhất. Kết quả cho thấy những mẫu lỗi này có ảnh đầu vào bị cắt gọt quá sát, làm mất một vài số chữ số của địa chị. Ví dụ, một ảnh của địa chỉ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-594"><span class="MJXp-mo" id="MJXp-Span-595" style="margin-left: 0em; margin-right: 0em;">“</span><span class="MJXp-mn" id="MJXp-Span-596">1849</span><span class="MJXp-mo" id="MJXp-Span-597" style="margin-left: 0em; margin-right: 0em;">”</span></span></span><script type="math/tex" id="MathJax-Element-96">“1849”</script></span> có thể bị cắt quá sát, chỉ còn lại phần ảnh chứa nội dung <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-598"><span class="MJXp-mo" id="MJXp-Span-599" style="margin-left: 0em; margin-right: 0em;">“</span><span class="MJXp-mn" id="MJXp-Span-600">849</span><span class="MJXp-mo" id="MJXp-Span-601" style="margin-left: 0em; margin-right: 0em;">”</span></span></span><script type="math/tex" id="MathJax-Element-97">“849”</script></span>. Vấn đề này có thể được giải quyết bằng cách dành một vài tuần để nâng cao độ chính xác của hệ thống phát hiện vùng chữ số trong địa chỉ nhằm định vị các vùng cắt. Tuy nhiên, nhóm nghiên cứu đã đưa ra một quyết định thực tế hơn nhiều, chỉ đơn giản là mở rộng chiều rộng của vùng cắt rộng hơn so với dự đoán ban đầu của mô hình một cách có hệ thống. Thay đổi đơn giản này giúp tăng thêm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-602"><span class="MJXp-mn" id="MJXp-Span-603">10</span><span class="MJXp-mi" id="MJXp-Span-604">%</span></span></span><script type="math/tex" id="MathJax-Element-98">10\%</script></span> độ phủ của hệ thống.</p><p>Một vài phần trăm cuối cùng của hiệu suất đến từ việc điều chỉnh các siêu tham số. Điều chỉnh này chủ yếu bao gồm việc làm cho mô hình lớn hơn nhưng vẫn duy trì một số giới hạn về chi phí tính toán. Bởi vì sai số huấn luyện và sai số kiểm thử vẫn gần như bằng nhau, nên rõ ràng rằng hiệu suất thiếu hụt là do vị khớp và một số vấn đề còn lại với chính tập dữ liệu.</p><p>Nhìn chung, dự án phiên mã là một thành công lớn, cho phép hàng trăm triệu địa chỉ được phiên mã nhanh hơn và với chi phí thấp hơn nhiều so với cách làm thủ công.</p><p>Chúng tôi hy vọng rằng các nguyên lý thiết kế được mô tả trong chương này sẽ là tiền đề cho thành công của nhiều hệ thống tương tự khác.</p></div>