<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Chương 1 
Giới thiệu - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/emojify.js/1.1.0/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif;padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;padding:0 15px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc-label{opacity:.3;background-color:#ccc;border:none;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child > ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}.ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}.markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}.ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}.markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}.ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled"><h1 id="Chương-1-Giới-thiệu"><a class="anchor hidden-xs" href="#Chương-1-Giới-thiệu" title="Chương-1-Giới-thiệu"><span class="octicon octicon-link"></span></a><center>Chương 1 <br>
Giới thiệu</center></h1><blockquote>
<p><em><strong>ND</strong></em>: <em>Những phần có ghi <strong>ND</strong> là chú giải thêm của người dịch</em></p>
</blockquote><p>Từ xa xưa, các nhà phát minh đã mơ ước tạo ra những cỗ máy có khả năng suy nghĩ. Khao khát này đã có từ thời Hy Lạp cổ đại. Những hình tượng thần thoại như Pygmalion, Daedalus, và Hephaestus được coi là những nhà phát minh huyền thoại, và Galatea, Talos, Pandora có thể xem như là những dạng sống nhân tạo [Ovid and Martin,2004; Sparkes, 1996; Tandy, 1997].</p><p>Ngay từ khi những chiếc máy tính đầu tiên có khả năng lập trình được tạo ra, người ta đã tự hỏi xem liệu chúng có thể trở nên thông minh được hay không, cho đến khi một cỗ máy thông minh như kỳ vọng ra đời sau đó một thế kỷ [Lovelace, 1842]. Ngày nay, <em>trí tuệ nhân tạo</em> (artificial intelligence - AI) đã trở thành một lĩnh vực với vô vàn những ứng dụng thực tiễn và trở thành chủ đề thu hút rất nhiều các đề tài nghiên cứu trên toàn thế giới. Chúng ta sử dụng phần mềm thông minh để tự động hóa các công việc chân tay, nhận dạng hình ảnh, âm thanh, chẩn đoán y học và hỗ trợ nghiên cứu khoa học cơ bản.</p><p>Trong những ngày đầu của trí tuệ nhân tạo, ngành này đã nhanh chóng giải quyết những vấn đề tuy phức tạp đối với con người, nhưng lại tương đối đơn giản đối với máy tính - những vấn đề có thể diễn tả được bằng một danh sách những quy tắc toán học hình thức. Thách thức thực sự đối với trí tuệ nhân tạo là giải quyết những tác vụ dễ thực hiện với con người nhưng lại khó diễn tả một cách tường minh (bằng ngôn ngữ toán học hình thức) - những vấn đề mà con người chúng ta xử lý qua trực giác một cách rất tự nhiên, như nhận dạng lời nói hay khuôn mặt chẳng hạn.</p><p>Cuốn sách này đưa ra một giải pháp tới những vấn đề trực giác đó. Giải pháp này cho phép máy tính tự học từ những kinh nghiệm thu được và hiểu hơn về thế giới thông qua một <em>hệ phân cấp khái niệm</em> (hierarchy of concepts) mà trong đó mỗi <em>khái niệm</em> (concept) lại được định nghĩa từ mối quan hệ của nó với những khái niệm đơn giản hơn. Bằng cách cho máy tính tự động thu thập tri thức từ kinh nghiệm, cách tiếp cận này giúp giảm bớt gánh nặng cho người vận hành trong việc mô tả tường minh tất cả tri thức cần thiết cho máy tính. <em>Hệ phân cấp khái niệm</em> cho phép máy tính tự học những khái niệm phức tạp từ những khái niệm đơn giản hơn. Nếu chúng ta vẽ một đồ thị để miêu tả hệ phân cấp khái niệm này thì đồ thị đó sẽ có rất nhiều tầng, và nó rất <em>sâu</em> (deep). Vì vậy, ta gọi cách tiếp cận trí tuệ nhân tạo này là <em>học sâu</em> (deep learning).</p><p>Rất nhiều trí tuệ nhân tạo thành công thế hệ đầu tiên ra đời trong môi trường thí nghiệm, và môi trường đó không yêu cầu máy tính phải có nhiều hiểu biết về thế giới bên ngoài. Ví dụ, hệ thống chơi cờ vua mang tên <em>Deep Blue</em> của công ty IBM đã đánh bại nhà vô địch thế giới Garry Kasparov vào năm 1997 [Hsu, 2002]. Cờ vua dĩ nhiên là một môi trường đơn giản, chỉ bao gồm 64 ô vuông và 32 quân cờ, trong đó mỗi quân cờ chỉ có thể di chuyển theo những quy tắc xác định. Việc sáng tạo ra một chiến thuật chơi cờ hiệu quả là một thành tựu lớn, nhưng thách thức của hệ thống không nằm ở việc dạy cho máy tính hiểu về các quân cờ và cách di chuyển của chúng. Luật chơi cờ vua được biểu diễn đầy đủ bằng một số quy tắc và có thể lập trình một cách dễ dàng.</p><p>Trớ trêu thay, những tác vụ trừu tượng và rập khuôn khiến cho con người phải đau đầu lại là những tác vụ dễ nhất với máy tính. Máy tính từ lâu đã có khả năng đánh bại người chơi cờ vua giỏi nhất, nhưng mới chỉ theo kịp trình độ của người bình thường trong việc nhận dạng vật thể và lời nói trong thời gian gần đây. Cuộc sống hàng ngày của một con người đòi hỏi một lượng rất lớn tri thức về thế giới. Phần lớn những kiến thức này mang tính chủ quan và dựa nhiều vào trực giác của mỗi người nên rất khó để có thể diễn đạt chúng một cách rõ ràng. Máy tính cần nắm bắt được cùng lượng tri thức như vậy về thế giới để có thể trở nên thông minh hơn. Một trong những thách thức chính trong ngành trí tuệ nhân tạo là cách để đưa những tri thức không tường minh đó vào trong một chiếc máy tính.</p><p>Một vài dự án trí tuệ nhân tạo cố gắng <em>gắn cứng</em> (hard-code) tri thức về thế giới thông qua <em>ngôn ngữ hình thức</em> (formal language). Máy tính có thể tư duy tự động về các <em>chỉ lệnh</em> (statement) trong các ngôn ngữ hình thức này thông qua các quy tắc suy luận logic. Cách tiếp cận trí tuệ nhân tạo này được gọi là tiếp cận thông qua <em>cơ sở tri thức</em> (knowledge base). Tuy nhiên, không một dự án nào trong số đó đạt được thành công lớn. Một trong số những dự án nổi tiếng nhất là dự án Cyc [Lenat &amp; Guha, 1989]. Cyc là một công cụ suy luận và là một cơ sở dữ liệu gồm các chỉ lệnh được viết bằng ngôn ngữ CycL. Những chỉ lệnh này được đội ngũ nhân viên giám sát của Cyc đưa vào. Đó là một quá trình đầy khó khăn. Người ta gặp vấn đề khi tạo ra những quy tắc tỉ mỉ rất phức tạp để cố gắng miêu tả chính xác nhất về thế giới. Ví dụ, Cyc không hiểu được câu chuyện về một người tên Fred cạo râu vào buổi sáng [Linde, 1992]. Công cụ suy luận của Cyc đã phát hiện ra một sự mâu thuẫn trong câu chuyện: nó biết rằng con người không có những bộ phận điện tử trên cơ thể của họ, nhưng bởi vì lúc đó Fred đang cầm một chiếc máy cạo râu, cho nên nó cho rằng thực thể mang tên “FredWhileShaving” có chứa những bộ phận điện tử trong đó. Cho nên Cyc đặt ra câu hỏi liệu Fred có còn là con người khi anh ta đang cạo râu hay không.</p><p>Những khó khăn mà các mô hình xây dựng dựa trên <em>tri thức gắn cứng</em> gặp phải cho thấy rằng một hệ thống trí tuệ nhân tạo thực thụ cần phải có khả năng tự thu thập tri thức của riêng chúng, bằng việc trích xuất những <em>mô thức</em> (pattern) từ dữ liệu thô. Khả năng này gọi là <em>học máy</em> (machine learning). Sự xuất hiện của <em>học máy</em> cho phép máy tính giải quyết các vấn đề liên quan đến tri thức về thế giới thực và đưa ra quyết định có vẻ chủ quan. Một thuật toán <em>học máy</em> đơn giản như <em>hồi quy logit</em> (logistic regression) có thể chẩn đoán và khuyến nghị có nên thực hiện phẫu thuật để hỗ trợ bà bầu khi sinh hay không [Mor-Yosef et al., 1990]. Một thuật toán khác, gọi là <em>giản luận Bayes</em> (naive Bayes), có thể phân loại  email hợp lệ và email rác.</p><p>Hiệu suất của những thuật toán học máy đơn giản này phụ thuộc nhiều vào <em>cách biểu diễn</em> (representation) của dữ liệu đầu vào. Ví dụ, khi hồi quy logit chẩn đoán phẫu thuật khi sinh, hệ thống AI không kiểm tra bệnh nhân một cách trực tiếp. Thay vào đó, các bác sĩ nạp một vài thông tin liên quan vào hệ thống, ví dụ như có tồn tại sẹo tử cung hay không. Mỗi mảnh thông tin liên quan đến bệnh nhân được gọi là một <em>đặc trưng</em> (feature). Hồi quy logit sẽ học mối liên hệ tương quan giữa mỗi đặc trưng với nhiều kết quả đầu ra. Tuy nhiên, thuật toán này không thể quyết định việc các đặc trưng được định nghĩa thế nào. Nếu hồi quy logit được cung cấp một bản chụp MRI của bệnh nhân, thay vì báo cáo chi tiết của bác sĩ, nó sẽ không thể đưa ra dự đoán chính xác. Những điểm ảnh đơn lẻ trong bản chụp MRI không có nhiều tương quan với các biến chứng có thể xảy ra trong khi thực hiện phẫu thuật.</p><p>Sự phụ thuộc vào cách biểu diễn là hiện tượng phổ biến xuyên suốt trong khoa học máy tính và cuộc sống hàng ngày. Trong khoa học máy tính, những thao tác như tìm kiếm trong một tập hợp dữ liệu có thể diễn ra nhanh hơn theo cấp số nhân nếu tập hợp đó được tổ chức có cấu trúc và được lập <em>chỉ mục</em> (index) một cách thông minh. Người ta có thể dễ dàng thực hiện các phép toán số học trên hệ chữ số Ả Rập nhưng lại mất nhiều thời gian khi thực hiện trên hệ La Mã. Không có gì ngạc nhiên khi việc lựa chọn cách biểu diễn có ảnh hưởng lớn đến hiệu suất của các thuật toán <em>học máy</em>. Hình 1.1 minh họa một ví dụ trực quan.</p><p><img src="https://i.imgur.com/g696lrD.png" alt=""></p><blockquote>
<p>Hình 1.1: Ví dụ về sự khác biệt trong cách biểu diễn: giả sử chúng ta muốn phân loại hai dạng dữ liệu bằng cách vẽ một đường thẳng phân tách chúng trong một đồ thị điểm. Trong đồ thị bên trái, chúng ta biểu diễn dữ liệu theo hệ tọa độ Decart, và đường thẳng như vậy không tồn tại. Trong đồ thị bên phải ta biểu thị dữ liệu dưới hệ tọa độ cực và ta chỉ cần vẽ một đường thẳng dọc là có thể phân đôi được hai tập điểm. (Hình vẽ có sự giúp đỡ của David Warde-Farley).</p>
</blockquote><p>Nhiều tác vụ trong trí tuệ nhân tạo có thể được giải quyết bằng việc thiết kế những bộ đặc trưng phù hợp cho nó, sau đó cung cấp bộ đặc trưng đó cho một thuật toán học máy đơn giản. Ví dụ, một đặc trưng hữu ích cho công việc nhận người qua giọng nói là ước lượng kích cỡ thanh quản của người đó. Đặc trưng này cho phép ta nhận biết được người nói là đàn ông, đàn bà hay một đứa trẻ.</p><p>Tuy nhiên, với nhiều tác vụ, rất khó để biết được nên trích xuất những đặc trưng nào. Ví dụ, chúng ta muốn viết một chương trình nhận dạng xe hơi trong những tấm ảnh. Chúng ta biết xe hơi có bánh, nên ta cho sự xuất hiện của bánh xe là một đặc trưng. Tuy nhiên, rất khó để miêu tả chính xác hình dạng của một chiếc bánh xe thông qua giá trị của các điểm ảnh. Một chiếc bánh xe có hình dạng vật lý tuy đơn giản, nhưng ảnh chụp của nó có thể trở nên phức tạp hơn nhiều do bóng của ánh nắng phủ lên bánh xe, và ánh nắng lóa chói ở những bộ phận kim loại của bánh xe, hay tấm chắn bùn của xe hoặc một vật thể nào đó che khuất vài bộ phận của bánh xe, và nhiều thứ khác nữa.</p><p>Một giải pháp cho vấn đề này là sử dụng học máy để không chỉ phát hiện ra một ánh xạ từ một <em>biểu diễn</em> (representation) tới đầu ra phù hợp mà còn phát hiện ra chính biểu diễn đó. Cách tiếp cận này  được gọi là <em>học biểu diễn</em> (representation learning). Những <em>biểu diễn học được</em> (learned representions) thường hiệu quả hơn những biểu diễn được thiết kế thủ công. Chúng cũng cho phép các hệ thống trí tuệ nhân tạo thích nghi nhanh với những tác vụ mới với đòi hỏi rất ít sự can thiệp từ con người. Một thuật toán học biểu diễn có thể tìm ra bộ đặc trưng phù hợp cho những tác vụ đơn giản trong vài phút, hay cho những tác vụ phức tạp trong thời gian vài giờ cho đến vài tháng. Còn những đặc trưng thủ công cho một tác vụ phức tạp yêu cầu nhiều thời gian và nỗ lực của con người; cả một cộng đồng đông đảo các nhà nghiên cứu có thể mất hàng thập kỷ chỉ để làm được việc tương tự đó (thiết kế bộ đặc trưng).</p><p>Một ví dụ điển hình của thuật toán học biểu diễn là <em>bộ tự mã hóa</em> (auto-encoder). Một <em>bộ tự mã hóa</em> là sự kết hợp của một <em>hàm mã hóa</em> (encoder), có chức năng chuyển đổi dữ liệu đầu vào thành một biểu diễn khác, và một <em>hàm giải mã</em> (decoder), có chức năng chuyển cách biểu diễn mới ngược trở về dạng ban đầu. Bộ tự mã hóa được huấn luyện để giữ lại nhiều thông tin nhất có thể khi dữ liệu đầu vào đi qua bộ mã hóa và bộ giải mã, nhưng chúng cũng được huấn luyện để các biểu diễn mới này có nhiều tính chất thú vị. Các bộ tự mã hóa khác nhau có mục tiêu học ra những <em>thuộc tính</em> (property) khác nhau.</p><p>Trong thiết kế các đặc trưng hay thuật toán học tập đặc trưng, mục tiêu của chúng ta là tách rời các <em>biến tố</em> (factors of variation) đóng vai trò giải thích dữ liệu quan sát được. Chữ <em>tố</em> ở đây, viết gọn của <em>nhân tố</em> (factor) là chỉ đến những nguồn ảnh hưởng riêng lẻ. Những nhân tố như vậy thường là các đại lượng ta không quan sát được. Thay vào đó, chúng tồn tại dưới dạng những vật thể hay những lực không thể đo đạc trong thế giới vật chất, nhưng có tác động đến những đại lượng ta quan sát thấy. Chúng còn có thể tồn tại dưới dạng các <em>thành tố</em> (construct) khái niệm trong tâm trí con người, giúp đưa ra cách giải thích đơn giản hơn hay suy diễn những nguyên nhân đằng sau dữ liệu thu thập được. Ta có thể coi chúng như những <em>khái niệm</em> (concept) hay những dạng <em>trừu tượng</em> (abstraction) giúp giải thích sự đa dạng trong dữ liệu. Khi phân tích một đoạn ghi âm lời nói, những biến tố bao gồm tuổi, giới tính, chất giọng và những câu từ của người nói. Khi phân tích một bức ảnh chụp xe hơi, những biến tố bao gồm vị trí, màu sắc, góc quan sát chiếc xe và độ sáng của ánh nắng mặt trời.</p><p>Khó khăn chính đối với đa số những ứng dụng trí tuệ nhân tạo thực tế đó là nhiều biến tố gây ảnh hưởng tới mọi đơn vị dữ liệu thu thập được. Những điểm ảnh trong bức ảnh chụp chiếc xe hơi màu đỏ có thể có màu gần với màu đen nếu ảnh được chụp vào ban đêm. Hình dạng cái bóng của chiếc xe tùy thuộc vào góc quan sát. Hầu hết các ứng dụng đòi hỏi ta phải tách rời các biến tố ra và loại bỏ những nhân tố không cần thiết.</p><p>Dĩ nhiên, việc trích xuất các đặc trưng mức cao, trừu tượng từ dữ liệu thô có thể rất khó. Đa số những biến tố, chẳng hạn như chất giọng người nói, chỉ có thể được xác định khi AI có trình độ hiểu biết tinh tế, gần ngang mức con người. Khi việc học biểu diễn khó tương đương với việc giải quyết bài toán ban đầu, thì có vẻ như học biểu diễn cũng không giúp chúng ta giải bài toán được bao nhiêu.</p><p><em>Học sâu</em> (deep learning) giải quyết vấn đề trọng tâm này của phương pháp học biểu diễn bằng cách đưa vào cách biểu diễn mới dựa theo những cách biểu diễn khác đơn giản hơn. Học sâu cho phép máy tính xây dựng những khái niệm phức tạp trên cơ sở những khái niệm đơn giản hơn. Hình 1.2 cho ta thấy một hệ thống học sâu có thể biểu diễn khái niệm về một bức ảnh chụp con người bằng việc kết hợp các khái niệm đơn giản hơn, như các <em>góc</em> (corner) và <em>đường bao</em> (contour), các khái niệm này lại được xác định từ các <em>cạnh biên</em> (edge).</p><p>Ví dụ điển hình cho một mô hình học sâu là <em>mạng lan truyền thuận đa tầng</em> (feed forward deep network), còn gọi là <em>mạng perceptron đa tầng</em> (multilayer perceptron - MLP). Một mạng perceptron đa tầng thực chất là một hàm toán học có chức năng <em>ánh xạ</em> (mapping) dữ liệu đầu vào tới các giá trị đầu ra. Hàm này được hợp thành từ những hàm đơn giản hơn. Chúng ta có thể coi mỗi lần áp dụng một hàm toán học khác nhau là một lần đưa ra cách biểu diễn mới cho dữ liệu đầu vào.</p><p>Ý tưởng của việc học cách biểu diễn đúng cho dữ liệu đã cho ta một góc nhìn về học sâu. Một góc nhìn khác về học sâu là <em>độ sâu</em> (depth) của nó cho phép máy tính học ra một chương trình nhiều công đoạn. Mỗi tầng của cách biểu diễn có thể xem như là trạng thái của bộ nhớ máy tính sau khi thực hiện một loạt các <em>câu lệnh</em> (instructions) song song. Các mạng với độ sâu lớn hơn có thể thực hiện nhiều <em>lệnh tuần tự</em> (sequence of instructions) hơn. Những dãy lệnh tuần tự có công năng rất lớn, bởi những câu lệnh sau có thể sử dụng lại kết quả của những câu lệnh trước. Theo như cách nhìn này của học sâu, không phải tất cả thông tin trong các <em>hàm kích hoạt</em> (activation) của cùng một tầng đều mã hóa những biến tố giải thích dữ liệu đầu vào. Cách biểu diễn này còn lưu trữ thông tin trạng thái, giúp chương trình máy tính có thể giải thích dữ liệu đầu vào. Thông tin trạng thái này có thể tương tự như bộ đếm hay con trỏ trong một chương trình máy tính truyền thống. Nó không liên quan tới nội dung đầu vào, nhưng giúp <em>mô hình</em> (model) tổ chức quá trình xử lí (của chính nó) hiệu quả hơn.</p><p><img src="https://i.imgur.com/P1Txr7v.png" alt=""></p><blockquote>
<p>Hình 1.2: Minh họa một mô hình học sâu. Thật khó khăn để máy tính có thể hiểu được ý nghĩa của các dữ liệu đầu vào dạng thô, chẳng hạn như bức ảnh này, nó được biểu diễn bằng một tập hợp các giá trị điểm ảnh. Hàm ánh xạ từ tập các điểm ảnh đến <em>định dạng của vật thể</em> (object identity) cực kỳ phức tạp. Việc học và đánh giá hàm ánh xạ này có vẻ như là nhiệm vụ bất khả thi nếu muốn xử lý trực tiếp. <em>Học sâu</em> giải quyết khó khăn này bằng cách chia nhỏ hàm ánh xạ phức tạp cần tìm thành một chuỗi các hàm ánh xạ đơn giản lồng vào nhau, mỗi một hàm ánh xạ được mô tả bởi một <em>tầng</em> (layer) khác nhau của mô hình. Đầu vào được mô tả ở <em>tầng hữu hình</em> (visible layer), ta đặt tên như thế là bởi các biến số ở tầng này có thể quan sát được (khi cho hiển thị thành điểm ảnh). Tiếp theo là một dãy các <em>tầng ẩn</em> (hidden layer) phân giải <em>đặc trưng</em> (feature) theo hướng tăng dần mức độ trừu tượng tính từ <em>tầng hữu hình</em>. Chúng được gọi là <em>tầng ẩn</em> bởi vì giá trị ở những tầng này không có sẵn trong dữ liệu; thay vào đó, mô hình phải tự xác định những khái niệm nào là hữu ích trong việc lý giải mối quan hệ giữa các dữ liệu quan sát được (và tự tinh chỉnh giá trị cho mỗi tầng). Những hình ảnh (ở mỗi tầng) bạn thấy ở đây là hiển thị của các đặc trưng được biểu diễn ở mỗi tầng. Từ các điểm ảnh cho trước, tầng thứ nhất có thể dễ dàng nhận ra các cạnh biên, bằng cách so sánh độ sáng giữa các điểm ảnh lân cận với nhau. Từ các cạnh biên đã được mô tả bởi tầng ẩn thứ nhất, tầng ẩn thứ 2 có thể dễ dàng tìm ra các góc và đường bao mở rộng, những đặc trưng này có thể được nhận diện bằng một tập các cạnh biên cơ bản. Từ các góc và đường bao được mô tả bởi tầng ẩn thứ hai, tầng ẩn thứ ba có thể phát hiện ra toàn bộ các phần cụ thể của vật thể, bằng cách tìm các tập đường bao và góc cụ thể. Cuối cùng, các mô tả dưới dạng các bộ phận của vật thể có thể được dùng để nhận diện vật thể trong ảnh. Những hình ảnh (trong ví dụ này) này được tái hiện với sự cho phép của Zeiler và Fergus (2014).</p>
</blockquote><p>Có 2 cách chính để đánh giá độ sâu của một mô hình. Cách thứ nhất dựa trên chuỗi các lệnh tuần tự cần thực hiện khi thực thi toàn bộ kiến trúc. Ta có thể xem nó giống như độ dài của đường dài nhất trong <em>lưu đồ</em> (flow chart) mô tả cách tính đầu ra của mỗi mô hình ứng với với đầu vào tương ứng. Cũng như hai chương trình máy tính tương đương sẽ có độ dài khác nhau phụ thuộc vào ngôn ngữ được sử dụng để viết chúng, cùng một hàm có thể có lưu đồ thuật toán với độ dài khác nhau phụ thuộc vào những hàm chúng ta được phép sử dụng trong các bước đơn lẻ của lưu đồ. Hình 1.3 minh họa việc lựa chọn ngôn ngữ có thể cho ra thang đo (độ sâu) khác nhau cho cùng một kiến trúc như thế nào.</p><p><img src="https://imgur.com/MWZw3qH.png" alt="" title="Hình 1.3"></p><blockquote>
<p>Hình 1.3: Minh họa một biểu đồ tính toán với đầu vào-đầu ra, và mỗi nút thực hiện một phép tính. Độ sâu là chiều dài của đường đi dài nhất từ đầu vào tới đầu ra, nhưng lại phụ thuộc vào định nghĩa một bước tính là gì. Tính toán trong những biểu đồ trên minh họa một hình hồi quy tuyến tính logit, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi mathvariant=&quot;bold-italic&quot;>w</mi><mi>T</mi></msup><mi mathvariant=&quot;bold-italic&quot;>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.936em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.397em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.35em, 1003.29em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main;">(</span><span class="msubsup" id="MathJax-Span-5"><span style="display: inline-block; position: relative; width: 1.404em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.81em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-6" style="font-family: MathJax_Math-bold-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -4.362em; left: 0.811em;"><span class="mi" id="MathJax-Span-7" style="font-size: 70.7%; font-family: MathJax_Math-italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.11em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-8" style="font-family: MathJax_Math-bold-italic;">x</span><span class="mo" id="MathJax-Span-9" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.378em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold-italic">w</mi><mi>T</mi></msup><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1">\sigma(\boldsymbol w^T \boldsymbol x)</script></span>, ở đây <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 0.703em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.595em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1000.6em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-family: MathJax_Math-italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">\sigma</script></span> là hàm <em>logit hình chữ S</em> (logistic sigmoid). Nếu ta sử dụng phép cộng, nhân và <em>hàm logit chữ S</em> là các thao tác cơ bản trong ngôn ngữ lập trình, thì mô hình này có độ sâu là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mn" id="MathJax-Span-15" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-3">3</script></span>. Nếu xem toàn bộ hàm hồi quy logit như là một thao tác cơ bản thì mô hình này chỉ có độ sâu là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mn" id="MathJax-Span-18" style="font-family: MathJax_Main;">1</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4">1</script></span>.</p>
</blockquote><p>Một cách khác, thường sử dụng những <em>mô hình đồ thị xác xuất sâu</em> (deep graphical model), định  nghĩa chiều sâu của một mô hình không phải là chiều sâu của biểu đồ tính toán, mà là của đồ thị mô tả các khái niệm có quan hệ với nhau như thế nào. Trong trường hợp đó, độ sâu biểu đồ tính toán để tìm ra một biểu diễn của mỗi khái niệm có thể sâu hơn nhiều so với đồ thị của chính các khái niệm đó. Đó là vì cách hiểu của hệ thống về các khái niệm có thể được làm mịn hơn khi chúng ta có thêm thông tin về các khái niệm phức tạp hơn. Ví dụ, một hệ thống trí tuệ nhân tạo quan sát một hình ảnh của một khuôn mặt với một mắt trong bóng tối ban đầu sẽ chỉ nhìn thấy một mắt. Sau đó phát hiện rằng có một khuôn mặt, hệ thống có thể suy luận thêm là có thể tồn tại con mắt thứ hai. Trong trường hợp này, đồ thị các khái niệm chỉ bao gồm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mn" id="MathJax-Span-21" style="font-family: MathJax_Main;">2</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn></math></span></span><script type="math/tex" id="MathJax-Element-5">2</script></span> tầng là mắt và khuôn mặt nhưng đồ thị các tính toán bao gồm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn><mi>n</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-22" style="width: 1.242em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.08em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1001.08em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-23"><span class="mn" id="MathJax-Span-24" style="font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-25" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-6">2n</script></span> tầng nếu chúng ta thực hiện <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-26" style="width: 0.703em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.595em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1000.6em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28" style="font-family: MathJax_Math-italic;">n</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-7">n</script></span> lần <em>tinh chỉnh</em> (refine) ước lượng của chúng ta về mỗi khái niệm khi biết các khái niệm khác.</p><p>Bởi vì rất khó xác định một trong hai cách nhìn - độ sâu của đồ thị tính toán hay độ sâu của mô hình đồ thị xác xuất - cách nhìn nào phù hợp nhất, và bởi vì các cá nhân khác nhau chọn tập các thao tác cơ bản khác nhau để xây dựng đồ thị tính toán cho riêng mình nên không có một giá trị độ sâu duy nhất cho một kiến trúc, tương tự với việc không có một giá trị duy nhất nào cho chiều sâu của một chương trình máy tính. Cũng như không có quy ước về độ sâu của một mô hình rằng bao nhiêu thì được xem là “sâu”. Tuy nhiên, hoàn toàn có thể coi học sâu là lĩnh vực nghiên cứu những mô hình liên quan đến sự kết hợp tuần tự một lượng lớn các hàm hoặc các khái niệm, lớn hơn các các mô hình học máy truyền thống.</p><p>Tóm lại, học sâu, chủ đề của cuốn sách, là một phương pháp tiếp cận trí tuệ nhân tạo. Cụ thể, nó là một loại học máy, một kĩ thuật cho phép cải tiến hệ thống máy tính bằng tri thức và dữ liệu. Chúng tôi dám chắc rằng học máy là một phương pháp tiếp cận khả thi duy nhất để xây dựng hệ thống trí tuệ nhân tạo có thể vận hành trong thế giới thực phức tạp. Học sâu là một dạng cụ thể của học máy, đạt được sức mạnh và sự mềm dẻo tuyệt vời thông qua việc biểu diễn thế giới như một hệ phân cấp các khái niệm, trong đó mỗi khái niệm được định nghĩa dựa trên mối liên hệ với những khái niệm đơn giản hơn, và nhiều cách biểu diễn trừu tượng có thể tính được từ những biểu diễn ít trừu tượng hơn. Hình 1.4 thể hiện mối liên hệ giữa những ngành trí tuệ nhân tạo khác nhau. Hình 1.5 mô tả sơ đồ ở mức cao về việc mỗi ngành hoạt động như thế nào.</p><p><img src="https://imgur.com/lluaLpU.png" alt=""></p><blockquote>
<p>Hình 1.4: Một biểu đồ Venn cho thấy học sâu là một dạng của học biểu diễn, và học biểu diễn lại là một dạng của học máy, một ngành được sử dụng trong nhiều (nhưng không phải tất cả) cách tiếp cận trí tuệ nhân tạo. Mỗi vùng của biểu đồ Venn bao gồm một ví dụ của một công nghệ trí tuệ nhân tạo.</p>
</blockquote><p><img src="https://imgur.com/nZqkLsr.png" alt=""></p><blockquote>
<p>Hình 1.5: Biểu đồ cho biết các phần khác nhau của một hệ thống trí tuệ nhân tạo liên quan với nhau như thế nào bên trong mỗi ngành trí tuệ nhân tạo khác nhau. Hình vuông tô đậm là các thành phần ta có thể học được từ dữ liệu.</p>
</blockquote><h1 id="11-Ai-nên-đọc-cuốn-sách-này"><a class="anchor hidden-xs" href="#11-Ai-nên-đọc-cuốn-sách-này" title="11-Ai-nên-đọc-cuốn-sách-này"><span class="octicon octicon-link"></span></a>1.1 Ai nên đọc cuốn sách này?</h1><p>Cuốn sách có thể hữu ích cho rất nhiều đối tượng độc giả khác nhau, nhưng chúng tôi hướng tới hai đối tượng độc giả chính. Thứ nhất là sinh viên các trường đại học (hoặc sau đại học) nghiên cứu về học máy, bao gồm cả những người mới bắt đầu tìm hiểu về học sâu và trí tuệ nhân tạo. Thứ hai là những kĩ sư phần mềm, những người chưa có kiến thức nền tảng về học máy, thống kê nhưng mong muốn nhanh chóng tiếp thu kiến thức về hai lĩnh vực này và bắt đầu sử dụng học sâu trong sản phẩm của mình. Học sâu đã chứng minh được tính hữu ích của nó trong nhiều ngành phần mềm, bao gồm thị giác máy tính, xử lí âm thanh và giọng nói, xử lí ngôn ngữ tự nhiên, robot, sinh - tin học và hóa học, trò chơi điện tử, máy tìm kiếm, quảng cáo trực tuyến và tài chính.</p><p>Cuốn sách này được chia thành ba phần để giúp đỡ cho nhiều đối tượng đọc giả khác nhau một cách tốt nhất. Phần I giới thiệu công cụ toán học và những khái niệm học máy cơ bản. Phần II mô tả những thuật toán học sâu phổ biến, là những công nghệ hữu dụng không thể thiếu. Phần III mô tả những ý tưởng mang tính suy đoán mà nhiều người tin rằng chúng có tầm quan trọng cho việc nghiên cứu học sâu trong tương lai.</p><p>Bạn có thể bỏ qua bất cứ phần nào trong sách mà bạn không quan tâm hoặc không phù hợp với kiến thức nền tảng của bạn. Ví dụ, độc giả quen thuộc với đại số tuyến tính, xác suất, và khái niệm học máy cơ bản có thể bỏ qua phần I, trong khi đó, những người chỉ muốn áp dụng học máy vào sản phẩm sẽ không cần quan tâm tới phần III. Để giúp bạn lựa chọn chương cần đọc, chúng tôi cung cấp một lưu đồ tổng quan thể hiện bố cục nội dung trong sách.</p><p><img src="https://imgur.com/vmeOEQi.png" alt=""></p><blockquote>
<p>Hình 1.6: Tổng quan bố cục nội dung cuốn sách. Mũi tên từ chương này tới chương khác có ý nghĩa cần phải đọc chương xuất phát của mũi tên mới có thể hiểu được chương mũi tên chỉ đến.</p>
</blockquote><p>Chúng tôi tạm giả sử rằng tất cả các bạn đọc đều có hiểu biết cơ bản về về khoa học máy tính như: lập trình, hiểu biết cơ bản về các vấn đề hiệu năng tính toán, lý thuyết độ phức tạp của thuật toán, các phương pháp tính cơ bản và một số thuật ngữ lý thuyết đồ thị.</p><h1 id="12-Lịch-sử-các-xu-hướng-trong-ngành-học-sâu"><a class="anchor hidden-xs" href="#12-Lịch-sử-các-xu-hướng-trong-ngành-học-sâu" title="12-Lịch-sử-các-xu-hướng-trong-ngành-học-sâu"><span class="octicon octicon-link"></span></a>1.2 Lịch sử các xu hướng trong ngành học sâu</h1><p>Cách dễ nhất để hiểu học sâu là tìm hiểu một vài bối cảnh lịch sử của nó. Thay vì liệt kê lịch sử của học sâu một cách chi tiết, chúng tôi đưa ra một vài xu hướng chính:</p><ul>
<li>Học sâu có lịch sử lâu dài và phong phú, với nhiều tên gọi khác nhau, phản ánh những quan điểm triết học khác nhau, và đã trải qua những thời kì thịnh, suy.</li>
<li>Học sâu trở nên hữu ích hơn khi mà lượng dữ liệu huấn luyện ngày càng tăng.</li>
<li>Kích thước của những mô hình học sâu ngày càng tăng do cơ sở hạ tầng máy tính (bao gồm phần cứng và phần mềm) cho học sâu ngày càng được cải thiện.</li>
<li>Học sâu đã và đang giải quyết những vấn đề phức tạp với độ chính xác ngày càng cao.</li>
</ul><h2 id="121-Những-cái-tên-và-vận-mệnh-đang-đổi-thay-của-các-mạng-neuron-nhân-tạo"><a class="anchor hidden-xs" href="#121-Những-cái-tên-và-vận-mệnh-đang-đổi-thay-của-các-mạng-neuron-nhân-tạo" title="121-Những-cái-tên-và-vận-mệnh-đang-đổi-thay-của-các-mạng-neuron-nhân-tạo"><span class="octicon octicon-link"></span></a>1.2.1 Những cái tên và vận mệnh đang đổi thay của các mạng neuron nhân tạo</h2><p>Chúng tôi đoán rằng rất nhiều độc giả biết tới học sâu như là một công nghệ mới thú vị, và quý vị sẽ rất ngạc nhiên khi thấy cuốn sách đề cập tới “lịch sử” của một ngành mới ra đời. Thực tế, học sâu đã xuất hiện từ những năm 1940. Học sâu có vẻ như là một đề tài mới, bởi vì nó ít được biết đến cho đến vài năm gần đây, khi nó trở nên nổi tiếng, và cũng vì có rất nhiều tên gọi khác nhau, chỉ gần đây thôi người ta mới gọi nó là học sâu. Lĩnh vực này đã được đổi tên nhiều lần, phản ánh ảnh hưởng của nhiều nhà nghiên cứu khác nhau và nhiều góc nhìn khác nhau.</p><p>Việc mô tả đầy đủ, chi tiết về lịch sử của học sâu nằm ngoài phạm vi của cuốn sách này. Tuy nhiên, lượt qua vài dấu mốc lịch sử có thể hữu ích trong việc tìm hiểu học sâu. Nói chung, đã có ba làn sóng phát triển học sâu: học sâu được  biết tới như là <em>điều khiển học</em> (cybernetics) những năm 1940-1960, sau đó học sâu được hiểu dưới góc nhìn của <em>thuyết kết nối</em> (connectionism) những năm 1980-1990 và gần đây nổi lên dưới cái lên <em>học sâu</em> (deep learning) từ năm 2006.  Hình 1.7 minh họa điểm này mang tính định lượng hơn một chút.</p><p><img src="https://i.imgur.com/mgMXKPu.png" alt=""></p><blockquote>
<p>Hình 1.7: Hai làn sóng nghiên cứu mạng neuron nhân tạo, đo bằng tần suất xuất hiện từ “điều khiển học” và “thuyết kết nối” hay “mạng neuron”, dựa trên Google Books (làn sóng thứ ba mới chỉ xuất hiện gần đây). Làn sóng đầu tiên bắt đầu bằng điều khiển học trong những năm 1940-1960, với sự phát triển của lý thuyết học <em>tập sinh học</em> (biological learning) [McCulloch and Pitts, 1943; Hebb, 1949] và các mô hình đầu tiên được thực thi như perceptron [Rosenblatt, 1958], cho phép huấn luyện một neuron riêng lẻ. Làn sóng thứ hai bắt đầu với cách tiếp cận theo trường phái kết nối giai đoạn 1980-1995, đánh dấu sự ra đời của thuật toán <em>lan truyền ngược</em> (back-propagation) [Rumelhart et al., 1986a] cho phép huấn luyện một mạng neuron có một hoặc hai tầng neuron ẩn. Làn sóng thứ ba và cũng là làn sóng hiện tại, học sâu, bắt đầu khoảng 2006 [Hinton et al.,2006; Bengio et al., 2007; Ranzato et al., 2007a] và mới chỉ xuất hiện trong sách vở, tính tới thời điểm năm 2016. Hai làn sóng đầu cũng đã xuất hiện trong sách vở rất lâu so với thời điểm các hoạt động nghiên cứu tương ứng bắt đầu.</p>
</blockquote><p>Những thuật toán học sớm nhất mà chúng ta biết được ngày nay mô phỏng các mô hình tính toán trong cơ chế học tập sinh học, những mô hình mô tả cách thức hoặc phỏng đoán về cách thức mà bộ não sử dụng để học tập. Kết quả là một trong những cái tên mà học sâu đã từng có theo nghĩa này là <em>mạng neuron nhân tạo</em> (artificial neural network). Góc nhìn tương ứng với cái tên đó cho rằng những hệ thống học sâu được lấy cảm hứng thiết kế từ bộ não sinh học (não người hoặc não của những loài động vật khác). Mặc dù có những loại mạng neuron trong học máy được sử dụng để tìm hiểu về chức năng bộ não [Hinton and Shallice, 1991] nhưng mạng neuron nói chung không được thiết kế theo mô hình chức năng sinh học thực thụ. Quan điểm neuron thần kinh trở thành một động lực cho học sâu bởi hai ý tưởng chính. Một ý tưởng là bộ não bản thân nó chính là minh chứng cho việc tồn tại một hệ thống có hành vi thông minh, và một cách dễ nhận thấy để xây dựng một hệ thống thông minh là đi <em>dịch ngược</em> (reverse engineer) các nguyên lý tính toán bên trong não bộ tạo nên sự thông minh đó và sao chép chức năng của nó. Một ý tưởng khác, việc hiểu bộ não và các nguyên tắc làm nền tảng cho trí thông minh của con người là điều vô cùng thú vị, do vậy các mô hình học máy ngoài khả năng giải quyết những vấn đề kỹ thuật còn có hữu ích trong việc giúp làm sáng tỏ nhiều câu hỏi khoa học cơ bản (về não bộ).</p><p>Thuật ngữ “học sâu” hiện đại vượt xa quan điểm của khoa học thần kinh trong các mô hình học máy hiện nay. Nó thiên về một nguyên tắc tổng quát hơn của việc học đó là học nhiều cấp độ nhỏ trong sự cấu thành của một sự vật hiện tượng. Nguyên tắc này có thể được áp dụng trong các lĩnh vực học máy khác nhau, không nhất thiết chỉ cho các lĩnh vực lấy cảm hứng từ mạng neuron thần kinh.</p><p>Tiền thân của học sâu hiện đại là những <em>mô hình tuyến tính</em>  (linear model) đơn giản lấy cảm hứng từ khoa học thần kinh. Những mô hình đó được thiết kế để lấy một tập hợp giá trị đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-29" style="width: 5.013em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.313em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1004.31em, 2.751em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-30"><span class="msubsup" id="MathJax-Span-31"><span style="display: inline-block; position: relative; width: 1.026em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.54em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-32" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.595em;"><span class="mn" id="MathJax-Span-33" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-34" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-35" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-36" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-37" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-38" style="font-family: MathJax_Main; padding-left: 0.164em;">,</span><span class="msubsup" id="MathJax-Span-39" style="padding-left: 0.164em;"><span style="display: inline-block; position: relative; width: 1.08em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.54em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-40" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.595em;"><span class="mi" id="MathJax-Span-41" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-8">x_1, . . ., x_n</script></span> và đi kèm với một giá trị đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-42" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1000.49em, 2.751em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-43"><span class="mi" id="MathJax-Span-44" style="font-family: MathJax_Math-italic;">y<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></span></span><script type="math/tex" id="MathJax-Element-9">y</script></span>. Chúng sẽ “học” ra một tập hợp <em>trọng số</em> (weight) <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-45" style="width: 5.337em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.582em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1004.58em, 2.751em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-46"><span class="msubsup" id="MathJax-Span-47"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.7em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-48" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.703em;"><span class="mn" id="MathJax-Span-49" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-50" style="font-family: MathJax_Main;">,</span><span class="mo" id="MathJax-Span-51" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-52" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-53" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-54" style="font-family: MathJax_Main; padding-left: 0.164em;">,</span><span class="msubsup" id="MathJax-Span-55" style="padding-left: 0.164em;"><span style="display: inline-block; position: relative; width: 1.242em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.7em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-56" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.703em;"><span class="mi" id="MathJax-Span-57" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-10">w_1, ..., w_n</script></span> tính toán đầu ra của chúng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi mathvariant=&quot;bold-italic&quot;>x</mi><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>x</mi><mi>n</mi></msub><msub><mi>w</mi><mi>n</mi></msub></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-58" style="width: 13.85em; display: inline-block;"><span style="display: inline-block; position: relative; width: 11.91em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1011.91em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-59"><span class="mi" id="MathJax-Span-60" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-61" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-62" style="font-family: MathJax_Math-bold-italic;">x</span><span class="mo" id="MathJax-Span-63" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-64" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-65" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-66" style="font-family: MathJax_Main; padding-left: 0.272em;">=</span><span class="msubsup" id="MathJax-Span-67" style="padding-left: 0.272em;"><span style="display: inline-block; position: relative; width: 1.026em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.54em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-68" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.595em;"><span class="mn" id="MathJax-Span-69" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-70"><span style="display: inline-block; position: relative; width: 1.134em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.7em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-71" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.703em;"><span class="mn" id="MathJax-Span-72" style="font-size: 70.7%; font-family: MathJax_Main;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-73" style="font-family: MathJax_Main;">+</span><span class="mo" id="MathJax-Span-74" style="font-family: MathJax_Main;">.</span><span class="mo" id="MathJax-Span-75" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-76" style="font-family: MathJax_Main; padding-left: 0.164em;">.</span><span class="mo" id="MathJax-Span-77" style="font-family: MathJax_Main; padding-left: 0.164em;">+</span><span class="msubsup" id="MathJax-Span-78"><span style="display: inline-block; position: relative; width: 1.08em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.54em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-79" style="font-family: MathJax_Math-italic;">x</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.595em;"><span class="mi" id="MathJax-Span-80" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-81"><span style="display: inline-block; position: relative; width: 1.242em; height: 0px;"><span style="position: absolute; clip: rect(3.397em, 1000.7em, 4.151em, -999.997em); top: -3.984em; left: 0em;"><span class="mi" id="MathJax-Span-82" style="font-family: MathJax_Math-italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.823em; left: 0.703em;"><span class="mi" id="MathJax-Span-83" style="font-size: 70.7%; font-family: MathJax_Math-italic;">n</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><msub><mi>w</mi><mn>1</mn></msub><mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>+</mo><msub><mi>x</mi><mi>n</mi></msub><msub><mi>w</mi><mi>n</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-11">f(\boldsymbol x,\boldsymbol w) = x_1w_1 + ... + x_nw_n</script></span>. Làn sóng nghiên cứu mạng neuron đầu tiên này có tên là điều khiển học, như minh họa trong hình 1.7.</p><p>Tế bào thần kinh McCulloch-Pitts [McCulloch and Pitts, 1943] là một mô hình chức năng não bộ xuất hiện từ sớm. Mô hình tuyến tính này đã có thể nhận biết hai <em>danh mục</em> (category) dữ liệu khác nhau bằng cách kiểm tra xem <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi mathvariant=&quot;bold-italic&quot;>x</mi><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-84" style="width: 3.774em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.235em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1003.13em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-85"><span class="mi" id="MathJax-Span-86" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-87" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-88" style="font-family: MathJax_Math-bold-italic;">x</span><span class="mo" id="MathJax-Span-89" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-90" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-91" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-12">f(\boldsymbol x,\boldsymbol w)</script></span> là âm hay dương. Tất nhiên, để mô hình có thể xác định đúng danh mục của dữ liệu, ta cần thiết lập trọng số <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;bold-italic&quot;>w</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-92" style="width: 0.973em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.811em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.781em, 1000.76em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-93"><span class="mi" id="MathJax-Span-94" style="font-family: MathJax_Math-bold-italic;">w</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">w</mi></math></span></span><script type="math/tex" id="MathJax-Element-13">\boldsymbol w</script></span> chính xác. Những trọng số này có thể được con người thiết lập thủ công. Trong những năm 1950, thuật toán perceptron [Rosenblatt, 1958, 1962] đã trở thành mô hình đầu tiên có thể tự học những trọng số để xác định các danh mục từ dữ liệu đầu vào ứng với mỗi mục. Thuật toán <em>phần tử tuyến tính thích nghi</em> (adaptive linear element - ADALINE), ra đời cùng thời với thuật toán perceptron, trả về chính giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-95" style="width: 2.212em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.888em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1001.78em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-96"><span class="mi" id="MathJax-Span-97" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-98" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-99" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-100" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-14">f(x)</script></span> để dự đoán một số thực [Widrowand Hoﬀ, 1960] và có thể tự học để dự đoán những con số đó từ dữ liệu.</p><p>Những thuật toán học tập đơn giản này có ảnh hưởng rất lớn tới bối cảnh chung của học máy hiện đại. Thuật toán sử dụng để thay đổi trọng số trong thuật toán ADALINE là một trường hợp đặc biệt của một thuật toán gọi là <em>trượt gradient ngẫu nhiên</em> (stochastic gradient descent). Các phiên bản hơi khác của thuật toán trượt gradient ngẫu nhiên vẫn là các thuật toán được sử dụng rộng rãi nhất trong các mô hình học sâu ngày nay.</p><p>Những mô hình dựa trên <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi mathvariant=&quot;bold-italic&quot;>x</mi><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-101" style="width: 3.774em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.235em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1003.13em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-102"><span class="mi" id="MathJax-Span-103" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-104" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-105" style="font-family: MathJax_Math-bold-italic;">x</span><span class="mo" id="MathJax-Span-106" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-107" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-108" style="font-family: MathJax_Main;">)</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-15">f(\boldsymbol x,\boldsymbol w)</script></span> như đã được sử dụng bởi thuật toán perceptron và ADALINE được gọi chung là các mô hình tuyến tính. Những mô hình này vẫn được sử dụng rộng rãi trong các mô hình học máy, qua nhiều trường hợp chúng được huấn luyện theo những cách khác nhau so với mô hình gốc.</p><p>Các mô hình tuyến tính có nhiều hạn chế. Hạn chế nổi tiếng nhất đó là chúng không thể học được hàm XOR, trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>1</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-109" style="width: 7.438em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.414em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1006.36em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-110"><span class="mi" id="MathJax-Span-111" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-112" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-113" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-114" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-115" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-116" style="font-family: MathJax_Main; padding-left: 0.164em;">1</span><span class="mo" id="MathJax-Span-117" style="font-family: MathJax_Main;">]</span><span class="mo" id="MathJax-Span-118" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-119" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-120" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-121" style="font-family: MathJax_Main; padding-left: 0.272em;">=</span><span class="mn" id="MathJax-Span-122" style="font-family: MathJax_Main; padding-left: 0.272em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-16">f([0, 1], \boldsymbol w) = 1</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mo stretchy=&quot;false&quot;>[</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=&quot;false&quot;>]</mo><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>1</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-123" style="width: 7.438em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.414em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1006.36em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-124"><span class="mi" id="MathJax-Span-125" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-126" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-127" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-128" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-129" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-130" style="font-family: MathJax_Main; padding-left: 0.164em;">0</span><span class="mo" id="MathJax-Span-131" style="font-family: MathJax_Main;">]</span><span class="mo" id="MathJax-Span-132" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-133" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-134" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-135" style="font-family: MathJax_Main; padding-left: 0.272em;">=</span><span class="mn" id="MathJax-Span-136" style="font-family: MathJax_Main; padding-left: 0.272em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="false">]</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-17">f([1, 0], \boldsymbol w) = 1</script></span> nhưng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mo stretchy=&quot;false&quot;>[</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-137" style="width: 7.438em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.414em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1006.36em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-138"><span class="mi" id="MathJax-Span-139" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-140" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-141" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-142" style="font-family: MathJax_Main;">1</span><span class="mo" id="MathJax-Span-143" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-144" style="font-family: MathJax_Main; padding-left: 0.164em;">1</span><span class="mo" id="MathJax-Span-145" style="font-family: MathJax_Main;">]</span><span class="mo" id="MathJax-Span-146" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-147" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-148" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-149" style="font-family: MathJax_Main; padding-left: 0.272em;">=</span><span class="mn" id="MathJax-Span-150" style="font-family: MathJax_Main; padding-left: 0.272em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-18">f([1, 1],\boldsymbol w) = 0</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy=&quot;false&quot;>]</mo><mo>,</mo><mi mathvariant=&quot;bold-italic&quot;>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-151" style="width: 7.438em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.414em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1006.36em, 2.804em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-152"><span class="mi" id="MathJax-Span-153" style="font-family: MathJax_Math-italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.057em;"></span></span><span class="mo" id="MathJax-Span-154" style="font-family: MathJax_Main;">(</span><span class="mo" id="MathJax-Span-155" style="font-family: MathJax_Main;">[</span><span class="mn" id="MathJax-Span-156" style="font-family: MathJax_Main;">0</span><span class="mo" id="MathJax-Span-157" style="font-family: MathJax_Main;">,</span><span class="mn" id="MathJax-Span-158" style="font-family: MathJax_Main; padding-left: 0.164em;">0</span><span class="mo" id="MathJax-Span-159" style="font-family: MathJax_Main;">]</span><span class="mo" id="MathJax-Span-160" style="font-family: MathJax_Main;">,</span><span class="mi" id="MathJax-Span-161" style="font-family: MathJax_Math-bold-italic; padding-left: 0.164em;">w</span><span class="mo" id="MathJax-Span-162" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-163" style="font-family: MathJax_Main; padding-left: 0.272em;">=</span><span class="mn" id="MathJax-Span-164" style="font-family: MathJax_Main; padding-left: 0.272em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.316em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="false">]</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-19">f([0, 0], \boldsymbol w) = 0</script></span>. Những nhà phê bình sau khi thấy được điểm yếu này đã phản ứng mãnh liệt chống lại các thuật toán học tập lấy cảm hứng từ sinh học [Minsky and Papert,1969]. Đó là lần thoái trào đầu tiên trong quá trình phát triển của mạng neuron.</p><p>Ngày nay, <em>khoa học thần kinh</em> (neuroscience) vẫn là nguồn cảm hứng cho các nhà nghiên cứu về <em>học sâu</em>, nhưng nó không còn là ngọn hải đăng dẫn đường nữa.</p><p>Lí do chính cho việc sụt giảm vai trò của khoa học thần kinh là vì con người chưa có đủ thông tin về não bộ để có thể xem đó là chỉ dẫn. Muốn hiểu một cách sâu sắc các thuật toán mà não người sử dụng, ta cần quan sát hoạt động của (ít nhất) hàng ngàn tế bào thần kinh liên kết với nhau đồng thời - một nhiệm vụ bất khả thi. Do đó, ngay cả việc hiểu những thành phần đơn giản và đã được nghiên cứu kỹ lưỡng nhất trong não bộ cũng trở nên ngoài tầm với của khoa học hiện tại [Olshausen and Field, 2005].</p><p><em>Khoa học thần kinh</em> thắp lên hi vọng rằng một thuật toán <em>học sâu</em> đơn lẻ có thể giải quyết nhiều nhiệm vụ khác nhau. Các nhà <em>thần kinh học</em> phát hiện ra rằng chồn có thể “nhìn” bằng vùng xử lý thính giác trong não bộ nếu não chúng được “đấu nối lại” để tín hiệu thị giác đi tới khu vực đó [Von Melchner et al., 2000]. Điều này cho thấy phần lớn não của các loài động vật có vú, có thể sử dụng một thuật toán đơn lẻ để giải quyết hầu hết những tác vụ nó cần thực hiện. Trước khi giả thuyết này được đưa ra, việc nghiên cứu học máy bị phân hoá khá rõ rệt thành nhiều cộng đồng nghiên cứu: <em>xử lý ngôn ngữ tự nhiên</em>, <em>thị giác máy tính</em>, <em>nhận dạng tiếng nói</em>,… Ngày nay, tuy những cộng đồng này vẫn còn tách biệt nhau, nhưng các nhóm nghiên cứu <em>học sâu</em> đã có thể làm việc với nhiều hoặc thậm chí là tất cả các lĩnh vực đó cùng lúc.</p><p>Chúng ta có thể liệt kê ra một vài gợi ý từ khoa học thần kinh.  Ý tưởng của việc nhiều đơn vị tính toán sẽ trở nên “thông minh” sau khi tương tác với nhau được lấy cảm hứng từ bộ não. Dựa trên cấu trúc về hệ thống thị giác của động vật có vú, Fukushima đưa ra neocognitron [Fukushima 1980], một cấu trúc xử lí ảnh rất mạnh và sau này trở thành nền móng cho <em>mạng tích chập</em> (convolutional network) hiện đại [LeCun et al., 1998b]; chúng ta sẽ nói đến điều này trong phần 19.10. Hiện nay, hầu hết mạng neuron nhân tạo đều dựa trên mô hình neuron mang tên <em>bộ chỉnh lưu tuyến tính</em> (rectified linear unit – ReLU). Bản cognitron đầu tiên [Fukushima, 1975] là một bản phức tạp hơn và gần như hoàn toàn lấy cảm hứng từ các chức năng não bộ. Phiên bản hiện đại đơn giản hơn được phát triển dựa trên kết hợp nhiều ý tưởng từ các góc nhìn khác nhau: Nair và Hilton (2010) và Glorot cùng cộng sự (2011a) đều cho rằng khoa học thần kinh là một nguồn ảnh hưởng, Jarrett cùng cộng sự (2009)  đưa ra các ảnh hưởng khác từ khía cạnh kĩ thuật. Dẫu cho khoa học thần kinh là một nguồn cảm hứng quan trọng, nhưng nó không phải thứ ta cần mô phỏng chính xác. Ta đã biết rằng các neuron thực sự của con người thực hiện việc tính toán rất khác so với các bộ chỉnh lưu tuyến tính hiện đại, nhưng các nỗ lực bắt chước các neuron nhiều hơn, cho đến hiện tại, vẫn chưa mang đến sự cải thiện hiệu năng. Mặc dù khoa học thần kinh đã thành công trong định hướng một vài cấu trúc mạng neuron, chúng ta vẫn chưa đủ hiểu biết về <em>học tập sinh học</em> (biological learning) để áp dụng vào những <em>thuật toán học tập</em> (learning algorithms) khi huấn luyện các kiến trúc đó.</p><p>Giới truyền thông thường nhấn mạnh sự tương đồng giữa não bộ với học sâu. Thực tế là các nhà nghiên cứu học sâu lấy cảm hứng từ bộ não nhiều hơn so với những lĩnh vực học máy khác như <em>mô hình hàm lõi</em> (kernel machine) hay thống kê Bayes, nhưng ta không nên coi học sâu như một phiên bản mô phỏng của não bộ. Các mô hình học sâu hiện đại hiện đang áp dụng ý tưởng từ nhiều lĩnh vực, đặc biệt là những nguyên tắc cơ bản của toán ứng dụng như đại số tuyến tính, xác suất, lý thuyết thông tin và toán tối ưu. Một số nhà nghiên cứu học sâu cho rằng khoa học thần kinh là nguồn cảm hứng quan trọng trong sự phát triển của học sâu, nhưng một số khác hoàn toàn không quan tâm đến khoa học thần kinh.</p><p>Cần lưu ý rằng, con người vẫn đang cố gắng hiểu cách não bộ làm việc ở cấp độ thuật toán. Đó là một lĩnh vực nghiên cứu riêng biệt với học sâu, thường được biết đến với tên gọi <em>“khoa học thần kinh tính toán"</em> (computational neuroscience). Chuyện các nhà nghiên cứu nhảy từ lĩnh vực này sang lĩnh vực kia và ngược lại giữa hai lĩnh vực này là chuyện rất dễ gặp. Ngành học sâu chủ yếu quan tâm tới làm thế nào để xây dựng được một hệ thống máy tính có khả năng giải thành công các tác vụ liên quan tới sự thông minh, trong khi ngành khoa học thần kinh tính toán lại chủ yếu quan tâm tới xây dựng chính xác hơn các mô hình mô tả sự hoạt động của bộ não trong thực tế.</p><p>Trong những năm thập niên 80, lần thứ hai làn sóng nghiên cứu mạng neuron nổi lên mạnh mẽ nhờ những trào lưu trong khoa học máy tính và công nghệ thông tin như <em>thuyết kết nối</em> (connectionism), hay <em>công nghệ xử lý phân tán song song</em> (parallel distributed processing)[Rumelhart et al., 1986c; McClelland et al., 1995]. Thuyết kết nối nổi lên trong bối cảnh khoa học nhận thức. Khoa học nhận thức là một cách tiếp cận liên ngành nghiên cứu tâm trí, kết hợp nhiều mức độ phân tích khác nhau. Đầu những năm 1980, hầu hết các nhà nghiên cứu khoa học nhận thức nghiên cứu các mô hình <em>suy luận ký hiệu</em> (symbolic reasoning). Dù rất phổ biến ở thời điểm bấy giờ, các mô hình ký hiệu cũng bế tắc trong việc giải thích cách hoạt động của não bộ sử dụng các neuron. Do đó, nhóm người theo thuyết kết nối đã bắt đầu nghiên cứu một mô hình nhận thức thực sự bắt nguồn từ các hoạt động thần kinh [Touretzky and Minton, 1985], qua đó khai quật nhiều ý tưởng của nhà tâm lý học Donald Hebb đưa ra trong những năm 1940 [Hebb, 1949].</p><p>Tư tưởng cốt lõi của <em>thuyết kết nối</em> là khi có một lượng lớn các đơn vị tính toán đơn giản được kết nối với nhau thành mạng lưới, chúng có thể trở nên thông minh. Khẳng định này đúng với cả các tế bào trong hệ thần kinh sinh học lẫn các đơn vị ẩn trong các mô hình tính toán.</p><p>Nhiều khái niệm chính xuất phát trong kỷ nguyên của thuyết kết nối vẫn đóng vai trò trọng tâm trong học sâu ngày nay. Một trong số đó là <em>biểu diễn phân tán</em> (distributed representation) [Hinton et al., 1986]. Theo đó, mỗi điểm đầu vào của một hệ thống được biểu diễn bởi nhiều đặc trưng khác nhau, cũng như mỗi đặc trưng có thể tham gia biểu diễn nhiều điểm đầu vào. Một ví dụ đơn giản: giả sử ta có một hệ thống thị giác có thể nhận biết ô tô, xe tải và chim. Và những vật thể đó có thể mang màu đỏ, xanh lục hoặc xanh dương. Một cách để biểu diễn các đầu vào này là sử dụng một neuron hoặc một đơn vị ẩn riêng biệt, mỗi neuron hoặc đơn vị ẩn này sẽ kích hoạt một trong chín khả năng có thể xảy ra: ô tô đỏ, xe tải đỏ, chim đỏ, ô tô xanh, v.v. Như vậy, ta cần <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-165" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-166"><span class="mn" id="MathJax-Span-167" style="font-family: MathJax_Main;">9</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn></math></span></span><script type="math/tex" id="MathJax-Element-20">9</script></span> neuron khác nhau để biểu diễn đầu vào, và mỗi neuron phải học các sự kết hợp giữa màu sắc và vật thể một cách độc lập. Cách tốt hơn là sử dụng một biểu diễn phân tán, với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-168" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-169"><span class="mn" id="MathJax-Span-170" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-21">3</script></span> neuron mô tả màu sắc và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-171" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-172"><span class="mn" id="MathJax-Span-173" style="font-family: MathJax_Main;">3</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></span></span><script type="math/tex" id="MathJax-Element-22">3</script></span> neuron mô tả đối tượng. Nghĩa là ta chỉ cần tổng cộng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-174" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-175"><span class="mn" id="MathJax-Span-176" style="font-family: MathJax_Main;">6</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn></math></span></span><script type="math/tex" id="MathJax-Element-23">6</script></span> neuron thay vì <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-177" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-178"><span class="mn" id="MathJax-Span-179" style="font-family: MathJax_Main;">9</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn></math></span></span><script type="math/tex" id="MathJax-Element-24">9</script></span> và neuron mô tả màu đó có thể học được màu đỏ từ ảnh ô tô, xe tải và ảnh chim, chứ không chỉ học từ ảnh của đúng một loại đối tượng. Khái niệm biểu diễn phân tán chính là trọng tâm của cuốn sách này, và sẽ được mô tả chi tiết hơn trong chương 15.</p><p>Một thành tựu nổi bật khác của phong trào thuyết kết nối là việc áp dụng thành công thuật toán lan truyền nghịch để huấn luyện các mạng neuron đa tầng cho phép tái biểu diễn bên trong mạng và sự phổ biến thuật toán lan truyền nghịch [Rumelhartet al., 1986a; LeCun, 1987]. Thuật toán này từ đó đã trải qua nhiều thăng trầm và tại thời điểm chúng tôi đang viết cuốn sách này (năm 2016), nó đang là phương pháp chủ đạo để huấn luyện các mô hình học sâu.</p><p>Đến những năm 1990, giới nghiên cứu đã có nhiều tiến bộ quan trọng trong việc mô hình hoá chuỗi bằng các mạng neuron. Hochreiter (1991) và Bengio cùng cộng sự (1994) đã chỉ ra những khó khăn về mặt toán học trong mô hình các chuỗi dài, như mô tả trong phần 10.7. Hochreiter và Schmidhuber (1997) đưa ra <em>mạng bộ nhớ ngắn hạn hướng dài hạn</em> (Long short-term memory network – LSTM network) để giải quyết một vài khó khăn đó. Cho đến nay, bộ nhớ ngắn hạn hướng dài hạn đã được áp dụng rộng rãi vào nhiều tác vụ mô hình hoá chuỗi, bao gồm nhiều  tác vụ xử lý ngôn ngữ tự nhiên tại Google.</p><p>Làn sóng nghiên cứu các mạng neuron kéo dài đến tận giữa 1990. Các dự án kêu gọi đầu tư mạo hiểm vào mạng neuron và các công nghệ trí tuệ nhân tạo khác bắt đầu có những phát biểu tham vọng không tưởng để tìm kiếm đầu tư. Khi các nghiên cứu trí tuệ nhân tạo không thể khỏa lấp những kì vọng vô lý đó, các nhà đầu tư bắt đầu cảm thấy thất vọng. Giữa thời điểm đó, các ngành học máy khác lại bắt đầu có những tiến bộ mới. <em>Mô hình hàm lõi</em> (kernel machines) [Boser et al.,1992; Cortes and Vapnik, 1995; Schölkopf et al., 1999] và các <em>mô hình đồ thị xác suất</em> (probabilistic graphical models) [Jor-dan, 1998] đều đạt được kết quả tốt trên nhiều tác vụ quan trọng. Những yếu tố này làm cho quá trình phổ cập của mạng neuron đi vào thoái trào cho tới tận năm 2007.</p><p>Trong thời gian đó, mạng neuron tiếp tục đạt được hiệu suất ấn tượng trong nhiều tác vụ [LeCun et al., 1998b; Bengio et al., 2001]. Đóng góp rất lớn cho sự tồn tại và phát triển mạnh mẽ của mạng neuron là dự án mang tên <em>Tính toán Thần kinh và Nhận thức Thích nghi</em> (Neural Computation and Adaptive Perception - NCAP), được Viện Nghiên cứu Cao cấp Canada (CIFAR) khởi xướng. Chương trình này đã hợp nhất các nhóm nghiên cứu học máy của Geoffrey Hinton tại Đại học Toronto, Yoshua Bengio tại Đại học Montreal, và Yann LeCun tại Đại học New York. CIFAR NCAP là một dự án nghiên cứu đa ngành, quy tụ nhiều nhà thần kinh học cũng như các chuyên gia hàng đầu trong lĩnh vực <em>thị giác máy tính</em> (computer vision).</p><p>Lúc bấy giờ, huấn luyện các mạng học sâu vẫn được cho là quá khó. Hiện nay chúng ta đều đã biết rằng các thuật toán ra đời từ những năm 1980 hoạt động khá tốt, nhưng phải đến tận những năm 2006 điều đó mới được chứng minh. Lý do đơn giản là vì các thuật toán này ngốn nhiều tài nguyên tính toán so với nền tảng phần cứng bấy giờ.</p><p>Làn sóng nghiên cứu mạng neuron lần thứ ba nổi lên với bước đột phá vào năm 2006. Geoffrey Hinton chứng minh rằn một loại mạng neuron có tên <em>mạng phân phối đa tầng</em> (deep belief network), một kiến trúc mạng neuron có thể được huấn luyện hiệu quả nhờ chiến lược mang tên <em>tiền huấn luyện tham lam theo tầng</em> (greedy layer-wise pre-training) [Hinton et al., 2006], chúng tôi sẽ mô tả chi tiết hơn về nó trong phần 15.1. Sau đó, các nhóm nghiên cứu khác của CIFAR nhanh chóng chứng minh rằng có thể huấn luyện nhiều loại mạng đa tầng khác nhau với cùng một chiến lược [Bengio et al., 2007; Ranzato et al., 2007a], giúp cải thiện khả năng khái quát cho các mẫu kiểm thử.  Làn sóng này đã góp phần phổ cập việc sử dụng thuật ngữ “học sâu”, để nhấn mạnh rằng các nhà nghiên cứu giờ đây có thể huấn luyện các mạng neuron sâu hơn rất nhiều so với trước đây, và tập trung sự chú ý vào tầm quan trọng về mặt lý thuyết của chiều sâu trong mạng [Bengio and LeCun, 2007; Delalleau and Bengio,2011; Pascanu et al., 2014a; Montufar et al., 2014]. Ở thời điểm đó, mạng neuron đa tầng tỏ ra vượt trội so với các hệ thống trí tuệ nhân tạo dựa trên những công nghệ học máy khác cũng như các hệ chức năng được thiết kế thủ công. Làn sóng nghiên cứu các mạng neuron lần thứ ba vẫn đang tiếp diễn cho đến thời điểm chúng tôi viết nên cuốn sách này, dù trọng tâm của nghiên cứu học sâu đã thay đổi đáng kể trong suốt thời gian ấy. Làn sóng thứ ba ban đầu tập trung vào các kỹ thuật học không giám sát mới và tập trung vào nghiên cứu làm thế nào các mạng sâu có thể tổng quát hóa tốt trên các tập dữ liệu nhỏ, nhưng ngày nay giới nghiên cứu đang dành nhiều sự quan tâm hơn cho các thuật toán học có giám sát cổ điển và tiềm năng của mô hình học sâu khi tận dụng các bộ dữ liệu lớn gán nhãn sẵn.</p><h2 id="122-Kích-thước-dữ-liệu-ngày-càng-tăng"><a class="anchor hidden-xs" href="#122-Kích-thước-dữ-liệu-ngày-càng-tăng" title="122-Kích-thước-dữ-liệu-ngày-càng-tăng"><span class="octicon octicon-link"></span></a>1.2.2 Kích thước dữ liệu ngày càng tăng</h2><p>Người ta có thể thắc mắc lí do tại sao học sâu chỉ mới được thừa nhận là một công nghệ đóng vai trò quan trọng trong thời gian gần đây, mặc dù các thử nghiệm đầu tiên với mạng neuron nhân tạo đã được thực hiện vào những năm 1950. Học sâu đã được áp dụng thành công vào các ứng dụng thương mại từ thập niên 90, tuy nhiên lúc bấy giờ nó thường được coi là một nghệ thuật hơn là một công nghệ, một thứ gì đó xa vời mà chỉ có những chuyên gia hàng đầu mới có thể sử dụng, cho tới gần đây. Trên thực tế thì đúng là cần có vài kỹ năng để đạt được kết quả khả quan khi sử dụng các thuật toán học sâu. May mắn thay, số kỹ năng cần thiết ấy đã giảm đáng kể khi lượng dữ liệu huấn luyện ngày càng tăng. Hiện nay, nhiều thuật toán đã có thể đạt được hiệu suất của con người trong các tác vụ phức tạp nhưng lại gần giống với các thuật toán đã phải vật lộn với những vấn đề trẻ con ở thập niên 80, mặc dù các mô hình mà chúng ta huấn luyện bằng cùng thuật toán đó đã trải qua nhiều cải tiến, giúp đơn giản hoá việc huấn luyện các kiến trúc đa tầng.</p><p>Sự phát triển quan trọng nhất gần đây đó là ta đã có thể cung cấp cho các thuật toán này những nguồn lực mà chúng cần để thành công. Hình 1.8 cho thấy kích thước của các bộ dữ liệu tiêu chuẩn tăng nhanh đến thế nào qua thời gian. Xu hướng này được thúc đẩy bởi quá trình số hoá của xã hội ngày càng lớn. Con người ngày càng có nhiều hoạt động diễn ra trên máy tính, và càng nhiều thứ chúng ta làm được ghi lại. Khi máy tính của chúng ta càng ngày càng được kết nối với nhau, việc thu thập nhiều bản ghi dữ liệu và tập hợp chúng thành một tập dữ liệu phù hợp cho các ứng dụng học máy càng trở nên đơn giản.</p><p><img src="https://i.imgur.com/C7twjBy.png" alt=""></p><blockquote>
<p>Hình 1.8: Sự tăng trưởng của dữ liệu qua thời gian. Trong đầu những năm 1900, các nhà thống kê đã nghiên cứu các tập dữ liệu sử dụng hàng trăm hay hàng nghìn phép đo được thực hiện thủ công [Garson, 1900; Gosset, 1908; Anderson, 1935; Fisher, 1936]. Trong thập niên 50 đến đầu thập niên 80, những người tiên phong trong lĩnh vực học máy lấy cảm hứng từ sinh học thường làm việc với các bộ dữ liệu tổng hợp nhỏ, chẳng hạn như các ảnh bitmap độ phân giải thấp của các ký tự, được thiết kế để chịu chi phí tính toán thấp và chứng minh rằng các mạng neuron có thể học các loại hàm cụ thể [Widrowand Hoﬀ, 1960; Rumelhart et al., 1986b]. Trong những năm 1980 và 1990, học máy mang ảnh hưởng của toán thống kê nhiều hơn và nhờ đó có thể tận dụng hiệu quả những tập dữ liệu lớn chứa hàng chục ngàn điểm dữ liệu, chẳng hạn như bộ dữ liệu chữ số viết tay MNIST (hình 1.9) [LeCun et al., 1998b]. Trong thập niên đầu của thế kỷ 21, nhiều bộ dữ liệu phức tạp hơn với cùng kích thước tiếp tục được tạo ra, chẳng hạn CIFAR-10 [Krizhevsky and Hinton, 2009]. Đến cuối thập niên đó và trong suốt nửa đầu những năm 2010, kích thước các bộ dữ liệu đã lớn hơn đáng kể, chứa hàng trăm ngàn đến hàng chục triệu điểm dữ liệu, nhờ đó hoàn toàn thay đổi những gì học sâu có thể làm được. Trong đó bao gồm tập dữ liệu số nhà công khai bởi Google Street View [Netzer et al., 2011], nhiều phiên bản của bộ dữ liệu ImageNet [Deng et al., 2009, 2010a; Russakovsky et al., 2014a] và dữ liệu Sports-1M [Karpathy et al., 2014]. Ở phía trên cùng của biểu đồ, có thể thấy rằng các tập dữ liệu các câu văn đã được dịch, chẳng hạn như tập dữ liệu của IBM được xây dựng từ Canadian Hansard [Brown et al., 1990] và tập dữ liệu WMT 2014 từ tiếng Anh sang tiếng Pháp [Schwenk,2014] thường vượt xa kích thước của các bộ dữ liệu khác.</p>
</blockquote><blockquote>
<p><em><strong>ND</strong></em>: ảnh bitmap gọi theo từ kỹ thuật là ảnh mành hóa (Rater Image) - dùng lưới các điểm ảnh, tức pixel, để biểu thị hình ảnh. Mỗi điểm ảnh được gán một vị trí và gán giá trị màu cụ thể.</p>
</blockquote><blockquote>
<p><em><strong>ND</strong></em>: Hansard: biên bản chính thức về các cuộc họp nghị viện ở các nước thuộc khối Thịnh vượng chung (các quốc gia thuộc vương quốc Anh).</p>
</blockquote><p><img src="https://i.imgur.com/GpC5XqI.png" alt=""></p><blockquote>
<p>Hình 1.9: Một số ví dụ đầu vào của bộ dữ liệu MNIST. Trong đó, “NIST” là viết tắt cho Viện Tiêu chuẩn và Công nghệ Quốc gia Hoa Kỳ (National Institute of Standards and Technology), cơ quan đầu tiên thu thập bộ dữ liệu này, và “M” là viết tắt của từ “đã được điều chỉnh” (modified) bởi dữ liệu đã được tiền xử lý để phù hợp hơn với các thuật toán học máy. Bộ dữ liệu MNIST bao gồm các bản <em>quét</em> (scan) của các chữ số viết tay được gán nhãn từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>0</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-180" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-181"><span class="mn" id="MathJax-Span-182" style="font-family: MathJax_Main;">0</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-25">0</script></span> đến <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>9</mn></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-183" style="width: 0.595em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.488em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.565em, 1000.43em, 2.535em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-184"><span class="mn" id="MathJax-Span-185" style="font-family: MathJax_Main;">9</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn></math></span></span><script type="math/tex" id="MathJax-Element-26">9</script></span> trong mỗi ảnh. Cho đến nay bài toán phân loại đơn giản này là một trong những phép kiểm tra được sử dụng rộng rãi nhất trong giới nghiên cứu học sâu, dẫu cho các kỹ thuật hiện đại ngày nay đã giải quyết nó một cách đơn giản. Geoffrey Hinton từng ví von MNIST là “ruồi giấm trong học máy”, có nghĩa là nó cho phép các nhà nghiên cứu học máy dễ dàng thử nghiệm thuật toán của họ giống như cách mà các nhà sinh học thực hiện thí nghiệm trên ruồi giấm.</p>
</blockquote><p>Kỷ nguyên của “dữ liệu lớn” là đòn bẩy đưa học máy lên tầm cao mới, bởi gánh nặng chính của bài toán ước lượng thống kê - khái quát hoá dữ liệu mới sau khi chỉ quan sát một lượng dữ liệu nhỏ - đã nhẹ nhàng hơn rất nhiều. Tính đến năm 2016, có một nguyên tắc chung là các thuật toán học sâu có giám sát có thể đạt được hiệu suất chấp nhận được với khoảng 5,000 điểm dữ liệu cho mỗi nhãn, và sẽ chạm tới hoặc thậm chí vượt qua hiệu suất của con người khi được huấn luyện với bộ dữ liệu chứa ít nhất 10 triệu điểm dữ liệu đã gán nhãn. Tuy nhiên, làm sao để làm việc thành công với những bộ dữ liệu nhỏ hơn nhiều là một lĩnh vực nghiên cứu quan trong, tập trung chủ yếu vào câu hỏi làm sao chúng ta có thể tận dụng được một lượng lớn dữ liệu không có nhãn, sử dụng học không giám sát hoặc bán giám sát.</p><h2 id="123-Kích-thước-mô-hình-ngày-càng-tăng"><a class="anchor hidden-xs" href="#123-Kích-thước-mô-hình-ngày-càng-tăng" title="123-Kích-thước-mô-hình-ngày-càng-tăng"><span class="octicon octicon-link"></span></a>1.2.3 Kích thước mô hình ngày càng tăng</h2><p>Một lý do quan trọng khác khiến mạng neuron cực kỳ thành công như ngày nay sau một khoảng thời gian nổi lên từ những năm 1980, thời gian mà mạng neuron chỉ có được những thành tựu khiêm tốn, là nhờ vào việc chúng ta đã đạt được những thành công về phát triển phần cứng chạy tốt các mô hình lớn hơn rất nhiều. Một trong những bài học chính rút ra từ thuyết kết nối là động vật trở nên thông minh khi nhiều tế bào thần kinh của chúng hoạt động cùng nhau. Một tế bào thần kinh riêng lẻ hoặc một tập nhỏ các tế bào thần kinh không hữu ích lắm trong thực tế.</p><p>Các tế bào thần kinh sinh học được kết nối với mật độ không quá dày đặc. Như đã thấy trong hình 1.10, các mô hình học máy của chúng ta đã đạt được số lượng các kết nối trên một tế bào thần kinh ngang với não bộ của một số loài động vật có vú từ nhiều thập kỷ trước.</p><p>Như trong hình 1.11, ngày nay, nếu xét về số lượng các tế bào thần kinh, các mô hình mạng neuron trong quá khứ có số lượng tế bào ít đến đáng kinh ngạc và điều này chỉ thay đổi trong khoảng thời gian gần đây, khi kích thước mạng ngày càng tăng. Kể từ khi đưa ra khái niệm về các đơn vị ẩn, mạng neuron nhân tạo đã tăng gấp đôi kích thước sau mỗi 2,4 năm. Sự tăng trưởng này nhờ vào hai yếu tố: các hệ thống máy tính đang trở nên mạnh mẽ hơn với bộ nhớ lớn hơn và  chúng ta đang có các bộ dữ liệu lớn hơn. Các mạng có kích thước lớn hơn có thể đạt được độ chính xác cao hơn đối với các tác vụ phức tạp. Xu hướng này có thể sẽ còn tiếp tục trong nhiều thập kỷ tới. Các mạng neuron nhân tạo sẽ chưa thể đạt được số lượng tế bào thần kinh như não bộ của người ít  nhất là cho đến năm 2050, trừ khi các công nghệ mới xuất hiện cho phép sự tăng trưởng này diễn ra nhanh hơn. Các tế bào thần kinh sinh học có thể biểu diễn các chức năng phức tạp hơn các tế bào thần kinh nhân tạo hiện tại, vì vậy nhiều mạng neuron sinh học thậm chí có thể lớn hơn mạng trong hình 1.11.</p><p>Khi nhìn lại, ta sẽ thấy không có gì phải ngạc nhiên khi mạng neuron với ít tế bào thần kinh hơn so với một con đỉa sẽ không thể giải quyết được các vấn đề trí tuệ nhân tạo tinh vi. Ngay cả những mạng mới xuất hiện gần đây, nếu nhìn từ quan điểm của hệ thống tính toán thì chúng đã có số lượng tế bào thần kinh khá lớn, nhưng vẫn còn nhỏ hơn hệ thần kinh của các loài động vật có xương sống tương đối nguyên thủy như ếch.</p><p>Kích thước mô hình ngày càng tăng theo thời gian nhờ vào việc CPU trở nên nhanh hơn, sự ra đời của các GPU có thể sử dụng đa mục đích (sẽ được mô tả chi tiết trong phần 12.1.2), kết nối mạng nhanh hơn và cơ sở hạ tầng phần mềm tốt hơn cho các hệ thống máy tính phân tán là một trong những xu hướng quan trọng nhất trong lịch sử của học sâu. Xu hướng này được dự kiến sẽ tiếp tục phát triển tốt trong tương lai.</p><p><img src="https://i.imgur.com/jz4vmnO.png" alt=""></p><blockquote>
<p>Hình 1.10: Số lượng các kết nối trên mỗi tế bào thần kinh theo thời gian. Ban đầu, số lượng kết nối giữa các tế bào thần kinh trong các mạng neuron nhân tạo bị giới hạn bởi khả năng làm việc của phần cứng. Ngày nay, số lượng kết nối giữa các tế bào thần kinh hầu như được xem xét như một phần của việc thiết kế mô hình. Một số mạng neuron nhân tạo có số lượng kết nối trên mỗi neuron gần giống như một con mèo, phổ biến hơn là các mạng neuron có số lượng kết nối trên mỗi neuron tương đương như các động vật có vú nhỏ hơn như chuột. Ngay cả bộ não của con người cũng không có số lượng kết nối quá lớn cho mỗi tế bào thần kinh. Kích thước mạng neuron sinh học lấy từ Wikipedia [2015].</p>
<ol>
<li>Phần tử tuyến tính thích nghi [Widrow and Hoﬀ, 1960].</li>
<li>Neocognitron [Fukushima, 1980].</li>
<li>Mạng tích chập tăng tốc bởi GPU [Chellapilla et al., 2006].</li>
<li>Máy Boltzmann đa tầng [Salakhutdinov and Hinton, 2009a].</li>
<li>Mạng tích chập không giám sát [Jarrett et al., 2009].</li>
<li>Mạng perceptron đa tầng  tăng tốc bởi GPU [Ciresan et al., 2010].</li>
<li>Bộ tự mã hóa phân tán [Le et al., 2012].</li>
<li>Mạng tích chập đa GPU [Krizhevsky et al., 2012].</li>
<li>Mạng tích chập không giám sát COTS HPC [Coates et al., 2013]</li>
<li>GoogLeNet [Szegedy et al., 2014a].</li>
</ol>
</blockquote><h2 id="124-Độ-phức-tạp-độ-chính-xác-và-mức-độ-ảnh-hưởng-thực-tế-ngày-càng-tăng"><a class="anchor hidden-xs" href="#124-Độ-phức-tạp-độ-chính-xác-và-mức-độ-ảnh-hưởng-thực-tế-ngày-càng-tăng" title="124-Độ-phức-tạp-độ-chính-xác-và-mức-độ-ảnh-hưởng-thực-tế-ngày-càng-tăng"><span class="octicon octicon-link"></span></a>1.2.4 Độ phức tạp, độ chính xác và mức độ ảnh hưởng thực tế ngày càng tăng</h2><p>Từ những năm thập niên 80, học sâu đã được cải thiện liên tục trong khả năng dự đoán và nhận dạng có độ chính xác cao. Hơn nữa, học sâu ngày càng được áp dụng rộng rãi hơn trong nhiều ứng dụng một cách rất thành công.</p><p><img src="https://i.imgur.com/r5HQw1h.png" alt=""></p><blockquote>
<p>Hình 1.11: Kích thước mạng neuron theo thời gian. Kể từ khi đưa ra khái niệm về các đơn vị ẩn, mạng neuron nhân tạo đã tăng gấp đôi kích thước, khoảng 2,4 năm một lần. Kích thước mạng neuron sinh học lấy từ Wikipedia [2015].</p>
</blockquote><p>Các mô hình đa tầng đầu tiên được sử dụng để nhận biết các vật thể riêng lẻ trong hình ảnh đã được cắt để chỉ còn chứa những vật thể cần được nhận diện và chúng có kích thước rất nhỏ [Rumelhart et al. 1986]. Kể từ đó đã có sự tăng dần về kích thước ảnh mà các mạng neuron có thể xử lý được. Các mạng nhận diện vật thể hiện đại xử lý các bức ảnh có độ phân giải cao và không yêu cầu ảnh bị cắt gần đối tượng cần nhận dạng [Krizhevsky et al. 2012]. Tương tự như vậy, các mạng đầu tiên có thể nhận diện chỉ hai loại đối tượng (hoặc trong một số trường hợp, sự vắng mặt hoặc sự hiện diện của một loại đối tượng duy nhất), trong khi các mạng hiện đại ngày nay có khả năng nhận diện ít nhất 1,000 loại đối tượng khác nhau. <em>ImageNet Large Scale Visual Recognition Challenge</em> (ILSVRC) là cuộc thi lớn nhất về nhận dạng vật thể  được tổ chức hàng năm. Một khoảnh khắc ấn tượng báo hiệu cho sự trỗỉ dậy của học sâu là khi mạng mạng tích chập lần đầu tiên giành chiến thắng với một khoảng cách khá xa với mô hình xếp thứ hai, giảm tỷ lệ lỗi top-5 tốt nhất thời điểm đó từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>26.1</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-186" style="width: 3.02em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.589em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1002.53em, 2.589em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-187"><span class="mn" id="MathJax-Span-188" style="font-family: MathJax_Main;">26.1</span><span class="mi" id="MathJax-Span-189" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>26.1</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">26.1\%</script></span> xuống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>15.3</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-190" style="width: 3.02em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.589em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1002.53em, 2.589em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-191"><span class="mn" id="MathJax-Span-192" style="font-family: MathJax_Main;">15.3</span><span class="mi" id="MathJax-Span-193" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>15.3</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">15.3\%</script></span> [Krizhevskyet al. 2012], nghĩa là mạng tích chập sẽ đưa ra một danh sách dự đoán nhãn của bức ảnh theo thứ tự độ tin cậy giảm dần nếu nhãn chính xác của bức ảnh nằm trong năm dự đoán đầu tiên thì được tính là một lần dự đoán chính xác và ngược lại được tính là một lần dự đoán lỗi, mô hình đã đạt kết quả độ lỗi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>15.3</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-194" style="width: 3.02em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.589em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1002.53em, 2.589em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-195"><span class="mn" id="MathJax-Span-196" style="font-family: MathJax_Main;">15.3</span><span class="mi" id="MathJax-Span-197" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>15.3</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-29">15.3\%</script></span> trên tập kiểm tra theo cách tính trên. Từ đó, các cuộc thi tiếp theo liên tục ghi nhận sự chiến thắng của mạng neuron tích chập đã đưa tỷ lệ lỗi top-5 xuống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3.6</mn><mi mathvariant=&quot;normal&quot;>&amp;#x0025;</mi></math>" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-198" style="width: 2.481em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.104em; height: 0px; font-size: 116%;"><span style="position: absolute; clip: rect(1.457em, 1002.05em, 2.589em, -999.997em); top: -2.368em; left: 0em;"><span class="mrow" id="MathJax-Span-199"><span class="mn" id="MathJax-Span-200" style="font-family: MathJax_Main;">3.6</span><span class="mi" id="MathJax-Span-201" style="font-family: MathJax_Main;">%</span></span><span style="display: inline-block; width: 0px; height: 2.373em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3.6</mn><mi mathvariant="normal">%</mi></math></span></span><script type="math/tex" id="MathJax-Element-30">3.6\%</script></span>, tính tới thời đểm cuốn sách này đang được viết, như mô tả trong hình 1.12.</p><p>Học sâu cũng có tác động đáng kể đến bài toán nhận dạng giọng nói. Sau khi cải thiện trong suốt những năm 1990, tỷ lệ lỗi cho bài toán nhận dạng giọng nói gần như không có sự cải thiện đảng kể nào vào khoảng những năm 2000. Sự ra đời của học sâu [Dahl et al. 2010, Denget al. 2010, Seide et al. 2011, Hinton et al. 2012] giúp bài toán nhận dạng giọng nói giảm tỷ lệ lỗi một cách tuyệt vời, với một số tỷ lệ lỗi giảm một nửa. Chúng ta sẽ khám phá điều này chi tiết hơn trong phần 12.3.</p><p>Những mô hình mạng đa tầng cũng đã đạt được những thành công ngoạn mục trong bài toán phát hiện người đi bộ và <em>phân vùng ảnh</em> (image segmentation) [Sermanet et al. 2013, Farabet et al. 2013, Couprie et al. 2013], đạt được độ chính xác hơn hẳn con người trong bài toán phân loại biển báo giao thông [Ciresanet al. 2012].</p><p>Mạng đa tầng ngày càng có khả năng giải quyết được nhiều tác vụ phức tạp hơn khi quy mô và độ chính xác của chúng tăng lên. Goodfellow và cộng sự (2014) đã chỉ ra rằng các mạng neuron có khả năng đưa ra một đoạn văn bản miêu tả nội dung một bức ảnh thay vì chỉ dừng lại ở việc xác định những đối tượng nào xuất hiện trong bức ảnh đó. Trước đây, nhiều người tin rằng cách học như trên đòi hỏi một chuỗi dữ liệu phải có nhãn riêng lẻ cho từng phần tử của chúng [Gülçehre và Bengio, 2013]. Các <em>mạng neuron hồi quy</em> (recurrent neural network), như mô hình bộ nhớ ngắn hạn hướng dài hạn vừa được nói đến ở phần trên, hiện đang được sử dụng để mô hình hóa các mối quan hệ giữa các chuỗi với nhau thay vì một đầu vào cố định. Thuật toán <em>học chuỗi-tới-chuỗi</em> (sequence-to-sequence learning) đang là một phương pháp học tạo ra cuộc cách mạng cho ứng dụng rất nổi tiếng hiện nay: <em>dịch máy</em> (machine translation) [Sutskever et al. 2014, Bahdanau et al. 2015].</p><p><img src="https://i.imgur.com/ibZqpy4.png" alt=""></p><blockquote>
<p>Hình 1.12: Tỷ lệ lỗi giảm dần theo thời gian. Kể từ khi các mạng đa tầng đạt đến quy mô cần thiết để cạnh tranh trong cuộc thi ILSVRC, chúng đã liên tục giành được giải cao nhất mỗi năm với tỷ lệ lỗi ngày càng giảm dần. Dữ liệu lấy từ [Russakovsky et al. 2014, He et al. 2015].</p>
</blockquote><p>Xu hướng tăng dần độ phức tạp này đã đưa đến sự ra đời của <em>máy Turing neuron</em> (neural Turing machine) [Graves et al. 2014], một mô hình có thể học cách đọc và ghi nội dung tùy ý vào các tế bào bộ nhớ. Những mạng neuron như thế có thể tự học để tạo ra các chương trình đơn giản từ những ví dụ mẫu mô tả các hành vi mong muốn. Ví dụ, một mô hình có thể học các sắp xếp các chữ số từ những chuỗi đã được sắp xếp đã từ trước. Công nghệ tự lập trình này đang trong giai đoạn sơ khai, nhưng trong tương lai, về nguyên tắc chúng có thể được áp dụng cho hầu hết mọi công việc.</p><p>Một thành tựu vượt trội của học sâu là sự mở rộng của nó đến lĩnh vực <em>học củng cố</em> (reinforcement learning). Đối với học củng cố, một tác nhân tự chủ học cách thực hiện một nhiệm vụ bằng cách <em>thử và sai</em> (trial and error) mà không có sự hướng dẫn nào từ con người. DeepMind đã công bố một hệ thống học củng cố dựa trên học sâu có khả năng học được cách chơi các trò trong các game của <em>Atari</em>, ngoài ra hệ thống này còn đạt được hiệu suất tương đương con người ở nhiều tác vụ khác nhau [Mnih et al. 2015]. Trong lĩnh vực nghiên cứu về robot (robotics), học sâu đồng thời đã cải thiện đáng kể hiệu suất của học củng cố [Finn et al. 2015].</p><p>Nhiều ứng dụng của học sâu mang lại lợi nhuận rất cao. Hiện nay, học sâu được sử dụng bởi nhiều hãng công nghệ hàng đầu trên thế giới gồm Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe, Netﬂix, NVIDIA và NEC.</p><p>Những tiến bộ trong mảng học sâu đã và đang phụ thuộc rất lớn vào những tiến bộ trong việc phát triển kiến trúc hạ tầng phần mềm. Các thư viện phần mềm như <em>Theano</em> [Bergstra et al. 2010, Bastienet al. 2012], <em>PyLearn2</em> [Goodfellow et al. 2013], <em>Torch</em> [Collobert et al. 2011], <em>DistBelief</em> [Dean et al. 2012], <em>Caffe</em> [Jia 2013], <em>MXNet</em> [Chen et al. 2015] và <em>TensorFlow</em> [Abadi et al. 2015] đã hỗ trợ cho các dự án nghiên cứu quan trọng hoặc các sản phẩm thương mại.</p><p>Học sâu cũng đã đóng góp cho nhiều thành tựu của các ngành khoa học khác. Mạng neuron tích chập dùng trong bài toán nhận dạng vật thể đã cung cấp một mô hình xử lý hình ảnh mà các nhà thần kinh học có thể nghiên cứu và áp dụng [DiCarlo 2013]. Học sâu cũng cung cấp các công cụ hữu ích để xử lý một lượng lớn dữ liệu và đưa ra các dự đoán hữu ích trong các lĩnh vực khoa học. Nó đã được áp dụng rộng rãi và thành công trong việc dự đoán các phân tử tương tác với nhau như thế nào để giúp các công ty dược phẩm điều chế các loại thuốc mới [Dahl et al. 2014], giúp tìm kiếm các hạt hạ nguyên tử [Baldi et al. 2014], xây dựng bản đồ 3 chiều về bộ não con người từ ảnh chụp của kính hiển vi [Knowles-Barley et al. 2014]. Chúng tôi hy vọng rằng trong tương lai, học sâu sẽ xuất hiện trong ngày càng nhiều lĩnh vực khoa học hơn nữa.</p><p>Tóm lại, học sâu là một cách tiếp cận của học máy, sử dụng rất nhiều kiến thức của chúng ta về não bộ con người, về thống kê và toán ứng dụng để phát triển trong suốt những thập kỷ qua. Trong những năm gần đây, học sâu đã cho thấy sự tăng trưởng vượt bậc về mức độ phổ biến và sự hữu ích của nó. Các kết quả mà nó có được phần lớn là nhờ vào những yếu tố: phần cứng máy tính ngày càng mạnh mẽ hơn, các bộ dữ liệu lớn hơn và các kỹ thuật cao cấp để huấn luyện các mạng neuron nhiều tầng hơn. Những năm tiếp theo sẽ mở ra nhiều thách thức và cơ hội để chúng ta cải thiện các mô hình học sâu và đưa nó vượt qua những giới hạn mới.</p><hr><p>Người dịch: Nam Tran, Karl Terry, Hung Le, Dzu.gidiel, DuyetPT, LA, lhlong, Xuan Tu, Trần Duy Thanh</p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-1-Giới-thiệu" title="Chương 1 
Giới thiệu">Chương 1 
Giới thiệu</a></li>
<li class=""><a href="#11-Ai-nên-đọc-cuốn-sách-này" title="1.1 Ai nên đọc cuốn sách này?">1.1 Ai nên đọc cuốn sách này?</a></li>
<li class=""><a href="#12-Lịch-sử-các-xu-hướng-trong-ngành-học-sâu" title="1.2 Lịch sử các xu hướng trong ngành học sâu">1.2 Lịch sử các xu hướng trong ngành học sâu</a><ul class="nav">
<li class=""><a href="#121-Những-cái-tên-và-vận-mệnh-đang-đổi-thay-của-các-mạng-neuron-nhân-tạo" title="1.2.1 Những cái tên và vận mệnh đang đổi thay của các mạng neuron nhân tạo">1.2.1 Những cái tên và vận mệnh đang đổi thay của các mạng neuron nhân tạo</a></li>
<li class=""><a href="#122-Kích-thước-dữ-liệu-ngày-càng-tăng" title="1.2.2 Kích thước dữ liệu ngày càng tăng">1.2.2 Kích thước dữ liệu ngày càng tăng</a></li>
<li class=""><a href="#123-Kích-thước-mô-hình-ngày-càng-tăng" title="1.2.3 Kích thước mô hình ngày càng tăng">1.2.3 Kích thước mô hình ngày càng tăng</a></li>
<li class=""><a href="#124-Độ-phức-tạp-độ-chính-xác-và-mức-độ-ảnh-hưởng-thực-tế-ngày-càng-tăng" title="1.2.4 Độ phức tạp, độ chính xác và mức độ ảnh hưởng thực tế ngày càng tăng">1.2.4 Độ phức tạp, độ chính xác và mức độ ảnh hưởng thực tế ngày càng tăng</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;"  >
        <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-1-Giới-thiệu" title="Chương 1 
Giới thiệu">Chương 1 
Giới thiệu</a></li>
<li class=""><a href="#11-Ai-nên-đọc-cuốn-sách-này" title="1.1 Ai nên đọc cuốn sách này?">1.1 Ai nên đọc cuốn sách này?</a></li>
<li class=""><a href="#12-Lịch-sử-các-xu-hướng-trong-ngành-học-sâu" title="1.2 Lịch sử các xu hướng trong ngành học sâu">1.2 Lịch sử các xu hướng trong ngành học sâu</a><ul class="nav">
<li class=""><a href="#121-Những-cái-tên-và-vận-mệnh-đang-đổi-thay-của-các-mạng-neuron-nhân-tạo" title="1.2.1 Những cái tên và vận mệnh đang đổi thay của các mạng neuron nhân tạo">1.2.1 Những cái tên và vận mệnh đang đổi thay của các mạng neuron nhân tạo</a></li>
<li class=""><a href="#122-Kích-thước-dữ-liệu-ngày-càng-tăng" title="1.2.2 Kích thước dữ liệu ngày càng tăng">1.2.2 Kích thước dữ liệu ngày càng tăng</a></li>
<li class=""><a href="#123-Kích-thước-mô-hình-ngày-càng-tăng" title="1.2.3 Kích thước mô hình ngày càng tăng">1.2.3 Kích thước mô hình ngày càng tăng</a></li>
<li class=""><a href="#124-Độ-phức-tạp-độ-chính-xác-và-mức-độ-ảnh-hưởng-thực-tế-ngày-càng-tăng" title="1.2.4 Độ phức tạp, độ chính xác và mức độ ảnh hưởng thực tế ngày càng tăng">1.2.4 Độ phức tạp, độ chính xác và mức độ ảnh hưởng thực tế ngày càng tăng</a></li>
</ul>
</li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
