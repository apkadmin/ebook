<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Chương 15: Học biểu diễn (Representation Learning) - HackMD
    </title>
    <link rel="icon" type="image/png" href="https://hackmd.io/favicon.png">
    <link rel="apple-touch-icon" href="https://hackmd.io/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/3.5.0/octicons.min.css" integrity="sha256-QiWfLIsCT02Sdwkogf6YMiQlj4NE84MKkzEMkZnMGdg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/emojify.js/1.1.0/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,500,500i|Source+Code+Pro:300,400,500|Source+Sans+Pro:300,300i,400,400i,600,600i|Source+Serif+Pro&subset=latin-ext);.hljs{display:block;background:#fff;padding:.5em;color:#333;overflow-x:auto}.hljs-comment,.hljs-meta{color:#969896}.hljs-emphasis,.hljs-quote,.hljs-string,.hljs-strong,.hljs-template-variable,.hljs-variable{color:#df5000}.hljs-keyword,.hljs-selector-tag,.hljs-type{color:#a71d5d}.hljs-attribute,.hljs-bullet,.hljs-literal,.hljs-number,.hljs-symbol{color:#0086b3}.hljs-built_in,.hljs-builtin-name{color:#005cc5}.hljs-name,.hljs-section{color:#63a35c}.hljs-tag{color:#333}.hljs-attr,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-selector-pseudo,.hljs-title{color:#795da3}.hljs-addition{color:#55a532;background-color:#eaffea}.hljs-deletion{color:#bd2c00;background-color:#ffecec}.hljs-link{text-decoration:underline}.markdown-body{font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{font-size:16px;padding:0 1em;color:#777;border-left:.25em solid #ddd}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1 .octicon-link,.markdown-body h2 .octicon-link,.markdown-body h3 .octicon-link,.markdown-body h4 .octicon-link,.markdown-body h5 .octicon-link,.markdown-body h6 .octicon-link{color:#000;vertical-align:middle;visibility:hidden}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .octicon-link,.markdown-body h2:hover .anchor .octicon-link,.markdown-body h3:hover .anchor .octicon-link,.markdown-body h4:hover .anchor .octicon-link,.markdown-body h5:hover .anchor .octicon-link,.markdown-body h6:hover .anchor .octicon-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:0;padding-top:.2em;padding-bottom:.2em;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\00a0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid #ccc;border-bottom-color:#bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0 none}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,sans-serif;padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body .emoji{vertical-align:top}.markdown-body pre{border:inherit!important}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.markdown-body code[data-gist-id]{background:none;padding:0}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg{max-width:100%;height:100%}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body .mark,.markdown-body mark{background-color:#fff1a7}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:760px;margin:25px auto -25px;padding:0 15px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:998}.ui-toc-label{opacity:.3;background-color:#ccc;border:none;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#fff;transition:opacity .2s}.ui-toc-label:focus{opacity:.3;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child > ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,Hiragino Kaku Gothic Pro,ヒラギノ角ゴ Pro W3,Osaka,Meiryo,メイリオ,MS Gothic,ＭＳ\ ゴシック,sans-serif}.ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,ＭＳ\ Ｐゴシック,sans-serif}.markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang TC,Microsoft JhengHei,微軟正黑,sans-serif}.ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,微軟正黑UI,sans-serif}.markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Helvetica Neue,Helvetica,Roboto,Arial,PingFang SC,Microsoft YaHei,微软雅黑,sans-serif}.ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,微软雅黑UI,sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:#999}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:cover}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}.ui-infobar>small>span{line-height:22px}.ui-infobar>small .dropdown{display:inline-block}.ui-infobar>small .dropdown a:focus,.ui-infobar>small .dropdown a:hover{text-decoration:none}.unselectable{-webkit-user-select:none;-o-user-select:none;user-select:none}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid comment-enabled"><h1 id="Chương-15-Học-biểu-diễn-Representation-Learning"><a class="anchor hidden-xs" href="#Chương-15-Học-biểu-diễn-Representation-Learning" title="Chương-15-Học-biểu-diễn-Representation-Learning"><span class="octicon octicon-link"></span></a>Chương 15: Học biểu diễn (Representation Learning)</h1><p>— Minh Vũ bắt đầu dịch từ đây (524-528)–</p><p>Trong chương này, đầu tiên, chúng ta bàn về ý nghĩa của việc học các <em>biểu diễn</em> (representation) và cùng xem khái niệm biểu diễn có thể hữu ích trong việc thiết kế các <em>kiến trúc đa tầng</em> (deep architecture) như thế nào. Ta cùng khám phá cách mà những <em>thuật toán học tập</em> (learning algorithms) chia sẻ sức mạnh thống kê qua các tác vụ khác nhau, bao gồm việc sử dụng thông tin từ các <em>tác vụ không giám sát</em> (unsupervised task) để thực hiện các <em>tác vụ có giám sát</em> (supervised tasks). Các <em>biểu diễn dùng chung</em> (shared representation) rất hữu ích cho việc xử lý đa <em>phương thức</em> (modality) hoặc đa <em>miền</em> (domain), hoặc để chuyển giao tri thức đã học được cho các tác vụ vốn chỉ được cung cấp ít mẫu hoặc không có mẫu nào nhưng lại có một tác vụ biểu diễn tồn tại. Cuối cùng, chúng ta nhìn lại và chỉ rõ những lý do đóng góp vào thành công của phương pháp học biểu diễn, bắt đầu với những điểm mạnh xét về lý thuyết của các <em>biểu diễn phân bố</em> (distributed representation) và các <em>biểu diễn đa tầng</em> (deep representation), kết thúc với một ý tưởng tổng quát hơn của các giả định cơ bản về <em>quá trình sinh dữ liệu</em> (data-generating process), cụ thể hơn, đó là về nguồn gốc cơ bản của dữ liệu quan sát được.</p><p>Nhiều tác vụ xử lý thông tin là rất dễ dàng hoặc rất khó tùy thuộc vào cách mà thông tin được biểu diễn. Đây là một nguyên tắc chung áp dụng được cho cuộc sống hàng ngày, cho khoa học máy tính nói chung, và cho học máy nói riêng. Ví dụ, một người có thể chia 210 cho 6 một cách đơn giản bằng cách đặt tính trên giấy rồi tính (ND: là cách ta được học ở tiểu học). Tác vụ này trở nên khó khăn hơn nhiều nếu sử dụng cách ghi số kiểu La Mã. Hầu hết mọi người ngày nay khi được yêu cầu chia CCX (210) cho VI (6) sẽ bắt đầu bằng cách chuyển đổi các con số La Mã này sang kiểu chữ số Ả Rập, để có thể thực hiện phương pháp đặt tính, phương pháp vốn dựa vào hệ thống biếu diễn giá trị của số theo vị trí. Cụ thể, ta có thể định lượng thời gian tiệm cận của việc chạy các tác vụ khác nhau bằng cách sử dụng các biểu diễn phù hợp hoặc không phù hợp. Ví dụ, chèn một số vào vị trí chính xác trong một dãy các số đã được sắp xếp là tác vụ có độ phức tạp <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">O</span><span class="MJXp-mo" id="MJXp-Span-3" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">n</span><span class="MJXp-mo" id="MJXp-Span-5" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processed" id="MathJax-Element-1-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-1"><span style="display: inline-block; position: relative; width: 0em; height: 0px; font-size: 116%;"><span style="position: absolute;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">)</span></span></span></span></span></nobr></span><script type="math/tex" id="MathJax-Element-1">O(n)</script></span> nếu danh sách được biểu diễn dưới dạng <em>danh sách liên kết</em> (linked list), và độ phức tạp chỉ còn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-6"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-7">O</span><span class="MJXp-mo" id="MJXp-Span-8" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi" id="MJXp-Span-9">log</span><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11">n</span><span class="MJXp-mo" id="MJXp-Span-12" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processed" id="MathJax-Element-2-Frame" tabindex="0" style=""><nobr><span class="math" id="MathJax-Span-7"><span style="display: inline-block; position: relative; width: 0em; height: 0px; font-size: 116%;"><span style="position: absolute;"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-family: MathJax_Math-italic;">O</span><span class="mo" id="MathJax-Span-10" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-11" style="font-family: MathJax_Main;">log</span><span class="mo" id="MathJax-Span-12"></span><span class="mi" id="MathJax-Span-13" style="font-family: MathJax_Math-italic; padding-left: 0.164em;">n</span><span class="mo" id="MathJax-Span-14" style="font-family: MathJax_Main;">)</span></span></span></span></span></nobr></span><script type="math/tex" id="MathJax-Element-2">O(\log n)</script></span> nếu danh sách được biểu diễn dưới dạng <em>cây đỏ đen</em> (red-black tree).</p><p>Xét trong bối cảnh của học máy, điều gì làm cho một biểu diễn tốt hơn biểu diễn khác? Nói chung, một biểu diễn tốt là biểu diễn làm cho các tác vụ tiếp theo dễ dàng hơn. Việc lựa chọn biểu diễn thường phụ thuộc vào việc lựa chọn tác vụ học tiếp theo.</p><p>Các <em>mạng lan truyền thuận</em> (feedforward network) đã được bằng phương pháp <strong>học có giám sát</strong> (supervised learning) có thể xem là một kiểu học biểu diễn. Đặc biệt, lớp cuối cùng của mạng thường là một bộ phân lớp tuyến tính, chẳng hạn như bộ phân lớp hồi quy softmax. Phần còn lại của mạng sẽ học cách cung cấp một biểu diễn cho lớp này. Việc đào tạo với <em>tiêu chí có giám sát</em> (supervised criterion) một cách tự nhiên dẫn đến sự biểu diễn ở mọi lớp ẩn (nhưng gần lớp ẩn trên cùng hơn) lấy các thuộc tính sao cho tác vụ phân loại dễ dàng hơn. Ví dụ, những lớp không tách biệt tuyến tính trong các đặc trưng đầu vào (input feature) có thể bị phân tách tuyến tính trong lớp ẩn cuối cùng. Về nguyên tắc, lớp cuối cùng có thể là một loại mô hình khác, chẳng hạn như một lớp <em>bộ phân lớp các điểm gần nhất</em> (nearest neighbor classifier)(Salakhutdinov và Hinton, 2007a). Các đặc trưng trong lớp áp chót cần tìm hiểu các thuộc tính khác nhau tùy thuộc vào loại của lớp cuối cùng.</p><p><em>Huấn luyện có giám sát</em> (supervised training) của mạng lan truyền thuận (feedforward networks) không liên quan đến việc áp đặt bất kỳ điều kiện nào lên các đặc trưng trung gian đã học được. Các loại thuật toán học biểu diễn khác thường được thiết kế rõ ràng để định hình một biểu diễn theo một cách cụ thể nào đó. Ví dụ, giả sử chúng ta muốn học một biểu diễn làm cho việc ước lượng mật độ dễ dàng hơn. Những phân phối có nhiều biến độc lập hơn thì mô hình hóa dễ dàng hơn, vì vậy chúng ta có thể thiết kế một hàm mục tiêu (objective function) để khuyến khích các phần tử của biểu diễn vector <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-13"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-14">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-3-Frame" tabindex="0" style=""></span><script type="math/tex" id="MathJax-Element-3">\boldsymbol{h}</script></span> trở nên độc lập. Giống như mạng có giám sát, thuật toán học sâu không giám sát&nbsp;có một đối tượng huấn luyện chính, nhưng thêm vào đó, nó cũng học một biểu diễn như một hiệu ứng biên. Bất kể biểu diễn thu được có như thế nào đi nữa, nó đều có thể dùng cho tác vụ khác. Ngoài ra, nhiều tác vụ kép (một vài là có giám sát, một vài là không có giám sát) có thể được học cùng với một số biểu diễn nội bộ dùng chung.</p><p>Hầu hết các vấn đề trong học biểu diễn đó là phải đánh đối giữa việc bảo toàn càng nhiều thông tin đầu vào càng tốt và đạt được các thuộc tính tốt (chẳng hạn như sự độc lập).</p><p>Việc học biểu diễn đặc biệt thú vị vì nó cung cấp một cách để thực hiện phương pháp học không giám sát và bán giám sát. Chúng ta thường có một lượng rất lớn dữ liệu huấn luyện chưa gắn nhãn và tương đối ít dữ liệu huấn luyện đã gắn nhãn. Huấn luyện với kỹ thuật học giám sát trên tập con đã gắn nhãn thường dẫn đến tình trạng quá khớp (overfitting) nghiêm trọng. Học bán giám sát cho phép giải quyết vấn đề trên bằng cách học hỏi từ dữ liệu chưa gắn nhãn. Đặc biệt, ta có thể học các biểu diễn tốt từ dữ liệu chưa gắn nhãn, và sau đó sử dụng các biểu diễn để giải quyết tác vụ học có giám sát.</p><p>Con người và động vật có thể học hỏi từ một lượng rất ít dữ liệu đã gắn nhãn. Chúng ta chưa biết làm thế nào điều này có thể xảy ra. Nhiều yếu tố có thể giải thích cho việc cải thiện hiệu năng của con người - Ví dụ, bộ não có thể sử dụng một lượng rất lớn phân loại kết hợp (ensembles of classiﬁers) hoặc kĩ thuật suy luận Bayes (Bayesian inference techniques). Một giả thuyết phổ biến là bộ não có thể tận dụng việc học tập không giám sát hoặc bán giám sát. Có rất nhiều cách để tận dụng dữ liệu chưa gắn nhãn. Trong chương này, chúng ta tập trung vào giả thuyết rằng dữ liệu chưa gắn nhãn có thể sử dụng để học một biểu diễn tốt.</p><h1 id="151-Tiền-huấn-luyện-tham-lam-theo-lớp-không-giám-sát-Greedy-Layer-Wise-Unsupervised-Pretraining"><a class="anchor hidden-xs" href="#151-Tiền-huấn-luyện-tham-lam-theo-lớp-không-giám-sát-Greedy-Layer-Wise-Unsupervised-Pretraining" title="151-Tiền-huấn-luyện-tham-lam-theo-lớp-không-giám-sát-Greedy-Layer-Wise-Unsupervised-Pretraining"><span class="octicon octicon-link"></span></a>15.1 Tiền huấn luyện tham lam theo lớp không giám sát (Greedy Layer-Wise Unsupervised Pretraining)</h1><p>Học không giám sát đóng vai trò lịch sử quan trọng trong sự hồi sinh của các mạng neuron sâu (deep neural network), cho phép các nhà nghiên cứu lần đầu tiên tạo ra một mạng lưới giám sát sâu mà không yêu cầu các mô tả kiến trúc như cụ thể như tích chập (convolution - convolutional neural network) hoặc hồi quy (recurrence - recurrent neural network). Chúng ta gọi thủ tục này là <em>tiền huấn luyện không giám sát</em> (unsupervised pretraining) , hoặc cụ thể hơn là <em>tiền huấn luyện tham lam theo lớp không giám sát</em> (greedy layer-wise unsupervised pretraining). Thủ tục này là một ví dụ kinh điển về cách một biểu diễn được học cho tác vụ này (học không giám sát, cố gắng nắm hình dạng của phân phối đầu vào) đôi khi lại có thể hữu ích cho một tác vụ khác (học có giám sát với cùng một miền đầu vào).</p><p>Tiền huấn luyện tham lam theo lớp không giám sát phụ thuộc vào một thuật toán học biểu diễn một lớp như là: RBM, một bộ mã hoá tự động một lớp, một mô hình mã hoá thưa hoặc một mô hình khác mà học những biểu diễn ẩn. Mỗi lớp được tiền huấn luyện sử dụng phương pháp học không giám sát, lấy kết quả của lớp trước và tạo ra cái như là kết quả của một biểu diễn của dữ liệu, phân phối của biểu diễn này (hoặc mối quan hệ của nó đến các biến khác, chẳng hạn phân loại dùng để dự đoán) có khả năng sẽ đơn giản hơn. Xem thuật toán 15.1 để biết mô tả chính thức.</p><p>Các bước huấn luyện tham lam theo lớp dựa trên các tiêu chuẩn không giám sát từ lâu đã được sử dụng để tránh né những khó khăn của việc huấn luyện nhiều lớp đồng thời trong mạng neuron sâu (deep neural net) cho một tác vụ có giám sát. Cách tiếp cận này xuất hiện khá lâu, ít nhất cũng từ neocognitron (Fukushima, 1975). Sự phục hưng học sâu trong năm 2006 bắt đầu với việc khám phá ra rằng quá trình học tham lam này có thể được sử dụng để khởi tạo một tiến trình học trên tất cả các lớp đồng thời, và rằng cách tiếp cận này có thể sử dụng để huấn luyện thành công các mô hình kết nối đầy đủ. (Hinton và cộng sự, 2006; Hintonand Salakhutdinov, 2006; Hinton, 2006; Bengio và cộng sự, 2007; Ranzato và cộng sự, 2007a). Trước khám phá này, chỉ có mạng tích chập sâu (convolutional deep networks) hoặc những mạng có độ sâu theo cơ chế hồi quy (recurrence) mới được coi là khả thi cho việc huấn luyện. Ngày nay, chúng ta biết rằng tiền huấn luyện tham lam theo lớp là không cần thiết cho việc huấn luyện các cấu trúc kết nối sâu, nhưng cách tiếp cận tiền huấn luyện không giám sát là phương thức đầu tiên thành công.</p><p>Tiền huấn luyện tham lam theo lớp được gọi là <strong>tham lam</strong> vì nó là một <strong>thuật toán tham lam</strong> (greedy algorithm). Có nghĩa là nó tối ưu từng phần của thuật toán một cách độc lập, mỗi phần tại một thời điểm, thay vì cùng nhau tối ưu tất cả các phần. Nó được gọi là <strong>theo lớp</strong> (layer-wise) bởi vì những phần độc lập này là các lớp của mạng. Đặc biệt, tiền huấn luyện tham lam theo lớp thực thi một lớp tại mỗi thời điểm, huấn luyện lớp thứ k trong khi vẫn giữ lớp trước đó. Đặc biệt nữa là, các lớp thấp hơn (được huấn luyện trước tiên) không bị điều chỉnh khi các lớp trên được đưa vào làm việc. Nó được gọi là <strong>không giám sát</strong> (Unsupervised) bởi vì mỗi lớp được huấn luyện với một thuật toán học biểu diễn không giám sát. Tuy nhiên, nó cũng được gọi là <strong>tiền huấn luyện</strong> (pretraining) bởi vì nó chỉ được coi là bước đầu tiên trước khi áp dụng một thuật toán huấn luyện đồng thời để tinh chỉnh tất cả các lớp với nhau. Trong bối cảnh của một nhiệm vụ học tập có giám sát, nó có thể được xem như là một bộ kiểm soát (regularizer) (trong một số thực nghiệm, tiền huấn luyện giảm sai số kiểm thử mà không làm giảm sai số huấn luyện) và một dạng khởi tạo tham số.</p><p>Cụm từ “tiền huấn luyện” (pre-training) thường được sử dụng không những để chỉ đến chính giai đoạn tiền huấn luyện mà còn dùng cho toàn bộ giao thức hai pha kết hợp pha tiền huấn luyện và pha huấn luyện có giám sát. Pha học có giám sát có thể bao gồm cả việc huấn luyện một bộ phân loại đơn giản dựa trên các đặc trưng đã học được ở pha tiền huấn luyện, hoặc nó có thể bao gồm cả việc tinh chỉnh (fine-tuning) có giám sát của toàn bộ mạng học được trong pha tiền huấn luyện. Dù bạn sử dụng bất kể loại thuật toán học không giám sát hoặc loại mô hình nào, trong hầu hết các trường hợp, sơ đồ (scheme) chương trình huấn luyện tổng thể là gần như nhau. Trong khi việc lựa chọn thuật toán học không giám sát sẽ chắc chắn ảnh hưởng đến những chi tiết, hầu hết các ứng dụng của tiền huấn luyện không giám sát lại phụ thuộc giao thức cơ bản này.</p><p>Tiền huấn luyện tham lam theo lớp không giám sát cũng có thể sử dụng như một khởi tạo cho những thuật toán học không giám sát khác, như là: bộ tự mã hoá sâu (Hintonand Salakhutdinov, 2006) và mô hình xác suất với nhiều lớp biến ẩn. Các mô hình như vậy bao gồm các mạng phân phối đa tầng (deep belief) (Hinton và cộng sự, 2006) và máy Boltzmann đa tầng (deep Boltzmann machines) (Salakhutdinov và Hinton, 2009a). Chúng tôi sẽ mô tả những mô hình sinh mẫu đa tầng này trong chương 20.</p><p>Như đã thảo luận trong phần 8.7.4, cũng có thể thực hiện tiền huấn luyện tham lam theo lớp <em>có giám sát</em>. Cách này xây dựng trên cơ sở rằng: huấn luyện một mạng không sâu (shallow network) sẽ dễ dàng hơn huấn luyện mạng đa tầng (deep network), đây là điều dường như đã xác nhận trong một số điều kiện (Erhan và cộng sự, 2010).</p><h2 id="1511-Khi-nào-và-tại-sao-tiền-huấn-luyện-không-giám-sát-hoạt-động"><a class="anchor hidden-xs" href="#1511-Khi-nào-và-tại-sao-tiền-huấn-luyện-không-giám-sát-hoạt-động" title="1511-Khi-nào-và-tại-sao-tiền-huấn-luyện-không-giám-sát-hoạt-động"><span class="octicon octicon-link"></span></a>15.1.1 Khi nào và tại sao tiền huấn luyện không giám sát hoạt động?</h2><p>Trong nhiều tác vụ, tiền huấn luyện tham lam theo lớp không giám sát có thể tạo nên cải thiện đáng kể ở khâu kiểm thử cho các tác vụ phân loại. Quan sát này nguyên nhân của việc hồi sinh mối quan tâm của mọi người đối với mạng neural sâu bắt đầu năm 2006 (Hinton và cộng sự, 2006; Bengio và cộng sự, 2007; Ranzato và cộng sự, 2007a). Tuy nhiên, đối với nhiều tác vụ khác, tiền huấn luyện không giám sát không những không mang lại hiệu quả mà thậm chí còn gây ra thiệt hại đáng kể. Ma và cộng sự (2015) đã nghiên cứu hiệu quả của tiền huấn luyện trên mô hình học máy cho bài toán tiên đoán hoạt động hoá học và nhận thấy rằng, bình quân, tiền huấn luyện là hơi có hại, nhưng đối với nhiều tác vụ là hữu ích đáng kể. Bởi vì tiền huấn luyện thỉnh thoảng hữu ích nhưng thường có hại, cho nên điều quan trọng là phải hiểu khi nào và tại sao nó hoạt động để xác định xem nó có thể áp dụng cho một tác vụ cụ thể nào đó hay không.</p><p>Nhìn chung, điều quan trọng là ta cần làm rõ rằng: phần nhiều chúng ta đang thảo luận về tiền huấn luyện tham lam không giám sát. Có nhiều mẫu hình (paradigms) khác biệt hoàn toàn cũng có thể thực hiện học bán giám sát với mạng neural, chẳng hạn như huấn luyện đối kháng ảo (virtual adversarial training), mô tả trong phần 7.13. Nó cũng có thể huấn luyện một bộ tự mã hoá hoặc mô hình sinh mẫu cùng lúc với mô hình giám sát. Ví dụ cho cách tiếp cận kiểu giai đoạn đơn lẻ này bao gồm RBM phân biệt (discriminative RBM) (Larochelleand Bengio, 2008) và <em>mạng bậc thang</em> (ladder network) (Rasmus và cộng sự, 2015), trong đó, hàm mục tiêu cuối cùng là một tổng tường minh của 2 hàm mục tiêu (một cái sử dụng nhãn và một cái chỉ sử dụng đầu vào).</p><hr><p><strong>Thuật toán 15.1</strong> Giao thức tiền huấn luyện tham lam theo lớp không giám sát.</p><p>Thuận toán như sau.  Thuật toán tiền huấn luyện không giám sát <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-15"><span class="MJXp-mrow" id="MJXp-Span-16"><span class="MJXp-mi MJXp-cal" id="MJXp-Span-17">L</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-4-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-4">\mathcal { L }</script></span>, lấy một bộ mẫu huấn luyện và trả về một mã hoá hoặc hàm đặc trưng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-18"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-5-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-5">f</script></span>. Dữ liệu thô đầu vào là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-20"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-21">X</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-6-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-6">X</script></span>, với mỗi dòng là một mẫu, và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-22"><span class="MJXp-msubsup" id="MJXp-Span-23"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24" style="margin-right: 0.05em;">f</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-25" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-26">(</span><span class="MJXp-mn" id="MJXp-Span-27">1</span><span class="MJXp-mo" id="MJXp-Span-28">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-29" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30">X</span><span class="MJXp-mo" id="MJXp-Span-31" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-7-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-7">f^{(1)} (X)</script></span> là đầu ra của bộ mã hoá đầu tiên trên <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-32"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33">X</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-8-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-8">X</script></span>. Trong trường hợp sử dụng tinh chỉnh, chúng ta sử dụng người học <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-34"><span class="MJXp-mrow" id="MJXp-Span-35"><span class="MJXp-mi MJXp-cal" id="MJXp-Span-36">T</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-9-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-9">\mathcal{T}</script></span>, nó lấy một hàm khởi tạo <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-37"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-10-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-10">f</script></span>, mẫu đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-39"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-40">X</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-11-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-11">X</script></span> (Và trong trường hợp tinh chỉnh có giám sát, liên kết mục tiêu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-41"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-42">Y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-12-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-12">Y</script></span>), và trả về một hàm được điều chỉnh. Số lượng các giai đoạn là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-43"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44">m</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-13-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-13">m</script></span>.</p><hr><h2 id="f-leftarrow-Indentity-function-tilde--mathbf--X----mathbf--X--for-k--1-ldotsm-do-nbspnbspnbspnbspfk--mathcal--L-tilde--mathbf--X----nbspnbspnbspnbspf-leftarrow-ff-circ-f-nbspnbspnbspnbsptilde--mathbf--X---leftarrow-fftilde--mathbf--X----end-for-if-fine-tuning-then-nbspnbspnbspnbspf-leftarrow-mathcalTfmathbf--X-mathbf--Y--end-if-Return-f"><a class="anchor hidden-xs" href="#f-leftarrow-Indentity-function-tilde--mathbf--X----mathbf--X--for-k--1-ldotsm-do-nbspnbspnbspnbspfk--mathcal--L-tilde--mathbf--X----nbspnbspnbspnbspf-leftarrow-ff-circ-f-nbspnbspnbspnbsptilde--mathbf--X---leftarrow-fftilde--mathbf--X----end-for-if-fine-tuning-then-nbspnbspnbspnbspf-leftarrow-mathcalTfmathbf--X-mathbf--Y--end-if-Return-f" title="f-leftarrow-Indentity-function-tilde--mathbf--X----mathbf--X--for-k--1-ldotsm-do-nbspnbspnbspnbspfk--mathcal--L-tilde--mathbf--X----nbspnbspnbspnbspf-leftarrow-ff-circ-f-nbspnbspnbspnbsptilde--mathbf--X---leftarrow-fftilde--mathbf--X----end-for-if-fine-tuning-then-nbspnbspnbspnbspf-leftarrow-mathcalTfmathbf--X-mathbf--Y--end-if-Return-f"><span class="octicon octicon-link"></span></a><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-45"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46">f</span><span class="MJXp-mo" id="MJXp-Span-47" style="margin-left: 0.333em; margin-right: 0.333em;">←</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-14-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-14">f \leftarrow</script></span> Indentity function<br>
<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-48"><span class="MJXp-mrow" id="MJXp-Span-49"><span class="MJXp-munderover" id="MJXp-Span-50"><span><span class="MJXp-over"><span style="margin-bottom: -0.92em;"><span class="MJXp-mo" id="MJXp-Span-53" style="margin-left: 0px; margin-right: 0px;">˜</span></span><span><span class="MJXp-mrow" id="MJXp-Span-51"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-52">X</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-54" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mrow" id="MJXp-Span-55"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-56">X</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-15-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-15">\tilde { \mathbf { X } } = \mathbf { X }</script></span><br>
<strong>for</strong> <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-57"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">k</span><span class="MJXp-mo" id="MJXp-Span-59" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-60">1</span><span class="MJXp-mo" id="MJXp-Span-61" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mo" id="MJXp-Span-62" style="margin-left: 0em; margin-right: 0em;">…</span><span class="MJXp-mo" id="MJXp-Span-63" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-64">m</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-16-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-16">k = 1, \ldots,m</script></span> <strong>do</strong><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-65"><span class="MJXp-msubsup" id="MJXp-Span-66"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67" style="margin-right: 0.05em;">f</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-68" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-69">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70">k</span><span class="MJXp-mo" id="MJXp-Span-71">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-72" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mrow" id="MJXp-Span-73"><span class="MJXp-mi MJXp-cal" id="MJXp-Span-74">L</span></span><span class="MJXp-mo" id="MJXp-Span-75" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mrow" id="MJXp-Span-76"><span class="MJXp-munderover" id="MJXp-Span-77"><span><span class="MJXp-over"><span style="margin-bottom: -0.92em;"><span class="MJXp-mo" id="MJXp-Span-80" style="margin-left: 0px; margin-right: 0px;">˜</span></span><span><span class="MJXp-mrow" id="MJXp-Span-78"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-79">X</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-81" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-17-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-17">f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )</script></span><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-82"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83">f</span><span class="MJXp-mo" id="MJXp-Span-84" style="margin-left: 0.333em; margin-right: 0.333em;">←</span><span class="MJXp-msubsup" id="MJXp-Span-85"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-86" style="margin-right: 0.05em;">f</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-87" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-88">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">f</span><span class="MJXp-mo" id="MJXp-Span-90">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-91" style="margin-left: 0.267em; margin-right: 0.267em;">∘</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-92">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-18-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-18">f \leftarrow f^{(f)} \circ f</script></span><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-93"><span class="MJXp-mrow" id="MJXp-Span-94"><span class="MJXp-munderover" id="MJXp-Span-95"><span><span class="MJXp-over"><span style="margin-bottom: -0.92em;"><span class="MJXp-mo" id="MJXp-Span-98" style="margin-left: 0px; margin-right: 0px;">˜</span></span><span><span class="MJXp-mrow" id="MJXp-Span-96"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-97">X</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-99" style="margin-left: 0.333em; margin-right: 0.333em;">←</span><span class="MJXp-msubsup" id="MJXp-Span-100"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-101" style="margin-right: 0.05em;">f</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-102" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-103">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104">f</span><span class="MJXp-mo" id="MJXp-Span-105">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-106" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mrow" id="MJXp-Span-107"><span class="MJXp-munderover" id="MJXp-Span-108"><span><span class="MJXp-over"><span style="margin-bottom: -0.92em;"><span class="MJXp-mo" id="MJXp-Span-111" style="margin-left: 0px; margin-right: 0px;">˜</span></span><span><span class="MJXp-mrow" id="MJXp-Span-109"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-110">X</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-112" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-19-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-19">\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )</script></span><br>
<strong>end for</strong><br>
<strong>if</strong> <em>fine-tuning</em> <em><strong>then</strong></em><br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-113"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114">f</span><span class="MJXp-mo" id="MJXp-Span-115" style="margin-left: 0.333em; margin-right: 0.333em;">←</span><span class="MJXp-mrow" id="MJXp-Span-116"><span class="MJXp-mi MJXp-cal" id="MJXp-Span-117">T</span></span><span class="MJXp-mo" id="MJXp-Span-118" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119">f</span><span class="MJXp-mo" id="MJXp-Span-120" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mrow" id="MJXp-Span-121"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-122">X</span></span><span class="MJXp-mo" id="MJXp-Span-123" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mrow" id="MJXp-Span-124"><span class="MJXp-mi MJXp-bold" id="MJXp-Span-125">Y</span></span><span class="MJXp-mo" id="MJXp-Span-126" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-20-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-20">f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })</script></span><br>
<strong>end if</strong><br>
<strong>Return</strong> <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-127"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-128">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-21-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-21">f</script></span></h2><p>— Kết thúc phần dịch của Minh Vũ (524-528)–</p><p>— Minh Pham bắt đầu dịch từ đây (529-533)</p><p>Bước tiền huấn luyện không giám sát là kết hợp từ hai ý tưởng khác nhau. Thứ nhất, nó vận dụng ý tưởng rằng: việc lựa chọn các tham số khởi tạo cho một mô hình mạng neuron đa tầng có thể có hiệu quả hiệu chỉnh đáng kể lên mô hình (Và ở một phạm vi nhỏ hơn nào đó, điều này có thể cải thiện việc tối ưu hóa). Thứ hai, nó vận dụng một ý tưởng tổng quát hơn nữa, đó là việc học phân phối (distribution) của đầu vào có thể hỗ trợ cho quá trình học ánh xạ (mapping) từ đầu vào tới đầu ra.</p><p>Cả hai ý tưởng đều bao hàm nhiều tương tác phức tạp giữa một số thành phần của thuật toán học máy vốn ta vẫn chưa hiểu biết hoàn chỉnh.</p><p>Ý tưởng đầu tiên - rằng việc lựa chọn các tham số khởi tạo cho một mô hình mạng neuron đa tầng, nó là cái ít được hiểu rõ nhất. Ở thời điểm mà bước tiền huấn luyện trở nên phổ biến, nó từng được xem như là việc khởi tạo mô hình ở một vị trí nào đó mà sẽ làm cho mô hình tiếp cận với điểm cực tiểu cục bộ tốt hơn là ở những vị trị khác. Ngày nay, người ta không còn coi cực tiểu cục bộ là một vấn đề nghiêm trọng đối với việc tối ưu tham số của mạng neural nữa. Bây giờ, ta biết rằng các thao tác huấn luyện mạng neuron chuẩn mực thường không đi tới một điểm cực trị cục bộ hay toàn cục nào. Vẫn tồn tại khả năng rằng: bước tiền huấn luyện khởi tạo mô hình tại một vị trí nào đó mà ngược lại, sẽ không thể đạt tới được, chẳng hạn, một miền bị bao quanh bởi những vùng mà hàm chi phí thay đổi từ một điểm dữ liệu này tới điểm dữ liệu khác nhiều đến mức mà các mini-batch chỉ đưa ra được một giá trị gradient ước lượng rất nhiễu, hoặc một miền được bao quanh bởi những vùng mà ma trận Hessian ở trong điều kiện xấu đến mức phương pháp trượt dốc gradient phải dùng đến những bước rất nhỏ. Tuy nhiên, khả năng của chúng ta trong việc mô tả chính xác những khía cạnh nào của các tham số (đã được tiền huấn luyện) được giữ lại trong bước huấn luyện có giám sát là bị giới hạn. Đây là lý do mà các cách tiếp cận hiện đại thường sử dụng đồng thời việc huấn luyện không giám sát và có giám sát hơn là chia làm hai giai đoạn. Người ta có thể tránh phải vật lộn với những ý tưởng phức tạp về cách mà bước tối ưu trong pha huấn luyện có giám sát giữ lại thông tin từ pha huấn luyện không giám sát này bằng cách đơn giản là cố định các tham số trong bộ trích xuất đặc trưng và sử dụng huấn luyện có giám sát chỉ để thêm vào một bộ phân lớp lên trên những đặc trưng đã học được.</p><p>Một cách hiểu tốt hơn đó là: một thuật toán học nào đó có thể sử dụng thông tin học được trong pha học không giám sát để hoạt động tốt hơn trong pha học có giám sát. Ý tưởng cơ bản là một số đặc trưng hữu ích đối với một tác vụ học không giám sát cũng có thể hữu ích cho tác vụ học có giám sát. Ví dụ, nếu chúng ta huấn luyện một mô hình sinh mẫu cho các tấm ảnh ô tô và xe máy, nó sẽ cần hiểu về bánh xe, và về việc số lượng bánh xe nên có trong một bức ảnh. Nếu chúng ta may mắn, biểu diễn của các bánh xe sẽ được cho ở dạng mà các bộ học có giám sát có thể dễ dàng sử dụng. Điều này vẫn chưa được hiểu rõ ở mức toán học và lý thuyết, vì thế không phải lúc nào cũng có thể dự đoán những tác vụ nào sẽ hưởng lợi theo cách này từ việc học không giám sát. Rất nhiều khía cạnh của cách tiếp cận này phụ thuộc nhiều vào mô hình cụ thể được sử dụng. Ví dụ, nếu chúng ta muốn thêm vào một bộ phân lớp tuyến tính lên trên những đặc trưng đã được tiền huấn luyện, các đặc trưng đấy phải làm cho các lớp cơ sở của bộ phân lớp khả phân tuyết tính (linearly separable). Những thuộc tính này thường xuất hiện một cách tự nhiên nhưng không phải luôn luôn như vậy. Đây là một lý do khác nữa mà việc tiến hành đồng thời pha học không giám sát và có giám sát có thể được ưa chuộng hơn – các ràng buộc do tầng đầu ra áp đặt đã được hàm chứa một cách tự nhiên từ khi bắt đầu.</p><p>Từ quan điểm xem việc tiền huấn luyện không giám sát như là việc học một biểu diễn, chúng ta có thể hi vọng rằng việc tiền huấn luyện không giám sát hiệu quả hơn khi biểu diễn khởi đầu nghèo nàn. Một ví dụ cốt lõi cho quan điểm này chính là là việc sử dụng <em>phép nhúng từ</em> (word embeddings). Các từ được biểu diễn bằng các vector đơn-trội (one-hot) không có nhiều thông tin bởi vì hai vector đơn-trội khác nhau bất kỳ đều có cùng khoảng cách đến nhau (khoảng cách bằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-129"><span class="MJXp-mn" id="MJXp-Span-130">2</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-22-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-22">2</script></span> khi sử dụng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-131"><span class="MJXp-msubsup" id="MJXp-Span-132"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-133" style="margin-right: 0.05em;">L</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-134" style="vertical-align: 0.5em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-23-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-23">L^2</script></span>). Các từ nhúng học được đã được mã hóa mức độ giống nhau giữa các từ một cách tự nhiên bằng khoảng cách giữa các từ. Vì thế, việc tiền huấn luyện không giám sát đặc biệt hữu ích khi xử lý từ. Nó kém ữu ích hơn khi xử lý ảnh, có lẽ bởi vì ảnh đã nằm sẵn trong không gian vector cực kỳ nhiều chiều (rich vector space), ở đó, khoảng cách giữa các vector tự thân đã cung cấp một thang-đo độ-tương-tự chất-lượng-thấp.</p><p>Từ quan điểm coi việc tiền huấn luyện không giám sát như là một bộ hiệu chỉnh, chúng ta có thể mong đợi rằng việc tiền huấn luyện không giám sát hữu ích nhất khi số lượng các mẫu được gán nhãn là rất nhỏ. Bởi vì nguồn thông tin được thêm vào bởi quá trình tiền huấn luyện không giám sát là dữ liệu không có nhãn, chúng ta cũng có thể kỳ vọng rằng tiền huấn luyện không giám sát hoạt động tốt nhất khi số lượng mẫu không có nhãn là rất lớn. Ưu điểm của học bán giám sát thông qua tiền huấn luyện không giám sát với rất nhiều các mẫu không có nhãn và ít mẫu có nhãn đặc biệt được làm sáng tỏ vào năm 2011 thông qua sự kiện kỹ thuật tiền huấn luyện không giám sát thắng hai cuộc thi quốc tế về phương pháp <em>học chuyển giao</em> (transfer learning) (Mesnil et al, 2011; Goodfellow et và cộng sự, 2011), trong các cài đặt của bài toán có số lượng các mẫu đã gắn nhãn ở tác vụ đích rất nhỏ (từ số lượng ít cho tới vài chục mẫu trên mỗi loại). Những kết quả này được ghi lại cẩn thận trong các thực nghiệm được kiểm soát rất cẩn thận do Paine và cộng sự (2014) tiến hành.</p><p>Các yếu tố khác cũng có thể liên đới. Ví dụ, tiền huấn luyện không giám sát có thể tỏ ra hữu ích nhất khi hàm cần học rất phức tạp. Tiền huấn luyện không giám sát khác với các bộ hiệu chỉnh (regularizers) như kỹ thuật làm suy yếu trọng số bởi vì nó không làm thiên lệch giải thuật học (learner) theo hướng khám phá một hàm đơn giản mà hướng giải thuật học theo hướng khám phá những hàm đặc trưng có ích cho tác vụ học không giám sát. Nếu các hàm số cơ sở đúng thực sự phức tạp và được định hình bởi tính cân đối của phân bố đầu vào, thì học không giám sát có thể là một bộ hiệu chỉnh hợp lý hơn nữa.</p><p>Bỏ những dự báo này sang một bên, chúng ta bây giờ phân tích một số trường hợp thành công khi mà tiền xử lý không giám sát được biết đến như là nguyên nhân của sự cải tiến và giải thích cho những gì đã biết về việc tại sao sự cải tiến diễn ra. Tiền xử lý không giám sát đã luôn được dùng để cải tiến các bộ phân lớp và luôn hấp dẫn nhất từ quan điểm giảm lỗi trên tập test. Tuy nhiên, tiền xử lý không giám sát có thể trợ giúp các tác vụ khác ngoài bài toán phân loại và có thể cải tiến việc tối ưu hơn là việc chỉ đơn thuần là một bộ hiệu chỉnh. Ví dụ, nó có thể cải tiến cả việc huấn luyện và sai số khôi phục trên tập test cho các bộ tự động mã hóa nhiều tầng (deep autoencoder) (Hinton và Salakhutdinov, 2006).</p><p>Tác giả Erhan (2010) thực hiện nhiều thử nghiệm để lý giải một số thành công của tiền huấn luyện không giám sát. Những cải thiện đối với sai số huấn luyện và sai số kiểm tra đều được giải thích theo khía cạnh tiền huấn luyện không giám sát đưa các tham số tới một mức giá trị mà sẽ không đạt tới được nếu dùng cách khác. Huấn luyện mạng neural là không tất định và hội tụ tới một hàm số khác nhau tại mỗi lần huấn luyện. Việc huấn luyện có thể kết thúc tại một điểm mà giá trị đạo hàm nhỏ đi, hoặc một điểm mà sự kết thúc sớm (early stopping) chấm dứt việc huấn luyện để tránh vấn đề quá khớp (overfitting), hoặc tại một điểm mà đạo hàm lớn nhưng khó tìm ra bước đi xuống do các vấn đề như tính ngẫu nhiên hoặc do ma trận Hessian ở trạng thái xấu. Các mạng neural nhận bước tiền huấn luyện không giám sát dừng một cách nhất quán tại cùng một miền của không gian hàm số, trong khi các mạng neural không có tiền huấn luyện dừng một cách nhất quán tại các miền khác. Xem hình 15.1 để xem một minh họa cho hiện tượng này. Miền mà các mạng được tiền huấn luyện đạt tới nhỏ hơn, gợi ý rằng bước tiền huấn luyện giảm độ biến thiên của quá trình ước lượng, mà việc này có thể tiếp tục dẫn tới giảm nguy cơ việc huấn luyện bị quá khớp. Nói cách khác, tiền huấn luyện không giám sát khởi tạo các tham số của mạng tới một mức giá trị mà nó không vượt ra được, và các kết quả tiếp theo của việc khởi tạo này mà nhất quán hơn và ít khả năng nó trở nên tệ khi không có việc khởi tạo này.</p><p>Tác giả Erhan (2010) cũng cung cấp một vài câu trả lời cho việc khi nào thì bước tiền huấn luyện hoạt động tốt nhất – tức là khi giá trị trung bình và phương sai của sai số kiểm tra giảm nhiều nhất bởi quá trình tiền huấn luyện mạng neural nhiều tầng. Hãy chú ý rằng những thí nghiệm được thực hiện trước phát kiến và sự phổ biến của các kỹ thuật hiện đại cho việc huấn luyện mạng neural rất nhiều tầng (hàm chính quy hóa tuyến tính, cơ chế tắt ngẫu nhiên và batch normalization) vì thế rất ít điều được hiểu rõ về hiệu quả của bước tiền huấn luyện không giám sát khi đi dùng chung với các cách tiếp cận đương thời.</p><p>Một câu hỏi quan trọng là cách mà tiền huấn luyện không giám sát có thể đóng vai trò như là một bộ hiệu chỉnh. Một giả thuyết là tiền huấn luyện giúp cho các thuật toán học khám phá các đặc trưng liên quan đến những nguyên nhân được ưu tiên sinh ra dữ liệu quan sát. Đây là một ý tưởng quan trọng dẫn tới rất nhiều thuật toán khác bên cạnh kỹ thuật tiền huấn luyện không giám sát và được mô hình nhiều hơn ở phần 15.3.</p><p><img src="https://i.imgur.com/M9eAavp.png" alt=""></p><p><strong>Hình 15.1</strong>: Sự trực quan hóa thông qua phép chiếu không tuyến tính của các chiến lược học của các mạng neural khác nhau trong <em>không gian hàm số</em> (không phải không gian tham số, để tránh vấn đề ánh xạ đa biến-tới-một biến từ các vector tham số tới các hàm), với việc khởi tạo ngẫu nhiên và có hoặc không có tiền huấn luyện không giám sát. Mỗi điểm tương tứng với một mạng neural khác nhau tại một thời điểm cụ thể trong quá trình huấn luyện. Hình vẽ này được chỉnh sửa thích hợp với sự cho phép từ Erhan và cộng sự (2010). Một tọa độ trong không gian hàm số là một vector vô hạn chiều kết hợp mỗi đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-135"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-136">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-24-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-24">x</script></span> với đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-137"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-138">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-25-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-25">y</script></span>. Erhan và cộng sự (2010) đã làm một phép chiếu tuyến tính tới không gian nhiều chiều bằng cách nối đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-139"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-26-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-26">y</script></span> cho nhiều điểm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-141"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-27-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-27">x</script></span> cụ thể. Họ sau đó làm thêm một phép chiếu phi tuyến tới không gian 2 chiều bằng Isomap (Tenenbaum và cộng sự, 2000). Màu sắc biểu thị thời gian. Tất cả mạng được khởi tạo gần trung tâm của hình vẽ (tương ứng với miền của các hàm mà tạo ra các phân phối chuẩn trên nhãn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-143"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-144">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-28-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-28">y</script></span> của hầu hết các đầu vào). Theo thời gian, việc học đưa các hàm xuất hiện, tới các điểm mà có khả năng dự đoán mạnh. Việc huấn luyện kết thúc một cách nhất quán ở một miền khi sử dụng tiền huấn luyện và ở miền khác, không trùng lên nhau khi không sử dụng tiền huấn luyện. Isomap cố gắng giữ lại các khoảng cách tương đối toàn cục (và do đó giữ lại thể tích) vì thế miền nhỏ tương ứng với các mô hình tiến huấn luyện biểu thị rằng các bộ ước lượng dựa trên tiền huấn luyện đã giảm độ biến thiên.</p><p>So với các dạng khác của tiền học không giám sát, tiền huấn luyện không giám sát có <em>nhược điểm</em> khi cần thực hiện với hai pha huấn luyện riêng biệt. Nhiều chiến lược hiệu chỉnh có ưu điểm về việc cho phép người dùng kiểm soát sức mạnh của việc hiệu chỉnh bằng cách điều chỉnh giá trị của từng siêu tham số (hyperparameter). Tiền huấn luyện không giám sát không đưa ra một phương pháp rõ ràng nào để điều chỉnh sức mạnh của sự hiệu chỉnh xuất hiện ở giai đoạn học không giám sát. Thay vào đó, có rất nhiều siêu tham số mà hiệu quả của chúng được đo sau khi có kết quả nhưng thường khó dự đoán trước. Khi chúng ta tiến hành học không giám sát và có giám sát một cách đồng thời, thay vì sử dụng chiến lược tiền huấn luyện, có một siêu tham số đơn lẻ, thường là một hệ số gắn với hàm chi phí không giám sát, mà xác định hàm mục tiêu không giám sát sẽ hiệu chỉnh mô hình có giám sát mạnh như thế nào. Chúng ta có thể luôn đạt được sự hiệu chỉnh ít hơn bằng cách giảm hệ số này. Trong tiền huấn luyện không giám sát, không có cách nào để thay đổi một cách thích hợp sức mạnh của sự hiệu chỉnh – mô hình có giám sát hoặc là được khởi tạo bằng các tham số tiền huấn luyện hoặc là không.</p><p>Một nhược điểm khác của việc có hai pha huấn luyện tách rời là mỗi pha có các siêu tham số riêng của chúng. Sự thực hiện của pha thứ hai thường không thể dự đoán được trong pha thứ nhất, vì thế có một khoảng trễ dài giữa việc đưa ra siêu tham số cho pha thứ nhất và việc có thể cập nhật chúng sử dụng phản hồi từ pha thứ hai. Cách tiếp cận có tính nguyên lý nhất, là sử dụng sai số trên tập xác thực trong pha học có giám sát để chọn các siêu tham số trong pha tiền huấn luyện, đã được thảo luận trong bài báo của Larochelle (2009). Trong thực hành, một số siêu tham số, như là số vòng tiền huấn luyện, thường được thiết lập trong quá trình tiến huấn luyện, sử dụng kỹ thuật dừng sớm (early stopping) trên hàm mục tiêu không giám sát, tuy không lý tưởng nhưng thường hiệu quả hơn về mặt tính toán hơn so với sử dụng hàm mục tiêu có giám sát.</p><p>Ngày nay, tiền huấn luyện không giám sát thường bị bỏ qua ngoại trừ lĩnh vực xử lý ngôn ngữ tự nhiên, nơi mà biểu diễn tự nhiên của từ như các vector đơn-trội không hàm chứa thông tin về độ tương tự và số lượng lớn các tập dữ liệu không gán nhãn là sẵn có. Trong trường hợp đó, ưu điểm của việc tiền huấn luyện là một người có thể tiền huấn luyện một lần trên một tập dữ liệu lớn không gán nhãn (ví dụ với một tập dữ liệu có hàng tỉ từ), học ra một biểu diễn tốt (thường là các từ, nhưng cũng có thể là các câu), và sau đó sử dụng biểu diễn này hoặc tinh chỉnh chúng cho tác vụ học có giám sát mà tập huấn luyện có chứa ít các ví dụ hơn một cách đáng kể. Cách tiếp cận này được tiên phong bởi Collobert và Weston (2008b), Turian (2010) và Collobert và cộng sự (2011a) và hiện đang là cách sử dụng phổ biến.</p><p>Các kỹ thuật học sâu dựa trên học máy có giám sát, được hiệu chỉnh với dropout hoặc batch normalization, có thể đạt được chất lượng ngang với con người trên nhiều tác vụ, nhưng chỉ với các tập dữ liệu gán nhãn cực kỳ lớn. Cùng các kỹ thuật này vượt trội kỹ thuật tiền huấn luyện không giám sát trên các tập dữ liệu có kích thước trung bình như CIFAR-10 và MNIST, mà có khoảng 5000 mẫu có gán nhãn trên mỗi loại. Trên những tập dữ liệu rất nhỏ, như là tập dữ liệu <em>alternative splicing</em>, các phương pháp Bayes vượt trội các phương pháp dựa trên tiền huấn luyện không giám sát (Srivastava, 2013). Vì những lý do này, sự phổ biến của tiền huấn luyện không giám sát giảm đi. Tuy nhiên, tiền huấn luyện không giám sát luôn là một cột mốc quan trọng trong lịch sử nghiên cứu học sâu và tiếp tục ảnh hưởng đến các cách tiếp cận đương thời. Ý tưởng tiền huấn luyện được tổng quát hóa thành tiền huấn luyện có giám sát, như đã được thảo luận ở phần 8.7.4, như là một cách tiếp cận chung cho học chuyển dịch. Tiền huấn luyện có giám sát cho học chuyển dịch là phổ biến (Oquab và cộng sự, 2014; Yosinski và cộng sự, 2014) với việc sử dụng mạng neural tích chập được tiền huấn luyện trên tập dữ liệu ImageNet. Các nhà thực hành xuất bản công khai các tham số của những mạng đã được huấn luyện cho mục đích này, giống như là việc công khai các vector từ đã được học trước, cho các bài toán xử lý ngôn ngữ (Collobert và cộng sự, 2011a; Mikolov và cộng sự, 2013a).</p><p>— Kết thúc phần dịch của Minh Pham (529-533)-----</p><p>— Huy Thai bắt đầu dịch từ đây (pg 534 - 538)-----</p><h1 id="152-Học-chuyển-tiếp-và-thích-ứng-miền"><a class="anchor hidden-xs" href="#152-Học-chuyển-tiếp-và-thích-ứng-miền" title="152-Học-chuyển-tiếp-và-thích-ứng-miền"><span class="octicon octicon-link"></span></a>15.2 Học chuyển tiếp và thích ứng miền</h1><p>Học chuyển tiếp và thích ứng miền đề cập tới tình huống khi mô hình được học theo một cấu hình (chẳng hạn, phân phối <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-msubsup" id="MJXp-Span-146"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-147" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-148" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-29-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-29">P_1</script></span>) và cần được tổng quát hoá trong một cấu hình khác (phân phối <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-149"><span class="MJXp-msubsup" id="MJXp-Span-150"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-151" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-152" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-30-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-30">P_2</script></span>). Điều này tổng quát hoá ý tưởng được trình bày trong phần trước, khi ta chuyển tiếp các biểu diễn giữa tác vụ học không giám sát và tác vụ học giám sát.</p><p>Trong học chuyển tiếp, bộ học phải tiến hành từ 2 tác vụ trở lên, nhưng ta giả sử có nhiều nhân tố lí giải sự thay đổi trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-153"><span class="MJXp-msubsup" id="MJXp-Span-154"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-156" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-31-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-31">P_1</script></span> liên quan đến sự thay đổi cần được mô tả trong kết quả học <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-157"><span class="MJXp-msubsup" id="MJXp-Span-158"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-159" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-160" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-32-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-32">P_2</script></span>. Điều này thường được hiểu trong bối cảnh của học có giám sát, khi đầu vào là như nhau nhưng kết quả có thể tuân theo một quy luật khác. Ví dụ, khi học về bài toán phân loại chủ đề hình ảnh, giả sử chó và mèo trong sắp đặt đầu tiên nhưng khi chuyển qua sự sắp đặt thứ hai, ta cần đáp ứng cho các chủ đề khác, chẳng hạn kiến và ong. Nếu sự sắp đặt thứ nhất có lượng dữ liệu nhiều hơn đáng kể, nó có thể học biểu diễn hữu ích để nhanh chóng tổng quát hoá từ rất ít mẫu lấy từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-161"><span class="MJXp-msubsup" id="MJXp-Span-162"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-163" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-164" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-33-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-33">P_2</script></span>. Rất nhiều chủ đề hình ảnh có chung khái niệm cấp thấp của cạnh và hình dạng, các thay đổi hình học, thay đổi về ánh sáng. Nói chung, học chuyển tiếp, học đa nhiệm và thích ứng miền có thể đạt được thông qua học biểu diễn khi tồn tại các đặc trưng hữu ích cho các cấu hình và tác vụ khác nhau, ứng với các nhân tố cơ bản xuất hiện trong nhiều sự sắp đặt. Điều này được minh họa trong hình 7.2, với các tầng thấp dùng chung và các tầng cao hơn đặc thù cho tác vụ.</p><p>Có đôi khi, mặc dù, những gì được các tác vụ chia sẻ không phải là ngữ nghĩa của đầu vào mà là ngữ nghĩa của đầu ra. Ví dụ, một hệ thống nhận diện giọng nói cần tạo ra các câu dịch có nghĩa ở tầng đầu ra, nhưng các tầng trước gần với đầu vào cần nhận ra các phiên bản khác nhau cùng âm vị khi những người khác nhau nói. Trong những trường hợp này, sẽ có ý nghĩa hơn khi chia sẻ các tầng trên (gần đầu ra) của mạng neural và có tiền xử lí đặc thù theo tác vụ, như được minh họa trong hình 15.2.</p><p><img src="https://i.imgur.com/3MEKIEN.png" alt=""></p><p><strong>Hình 15.2:</strong> Ví dụ: Kiến trúc cho đa tác vụ hoặc học chuyển tiếp khi biết đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-165"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-34-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-34">y</script></span> có cùng ngữ nghĩa trong tất cả tác vụ trong khi biến đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-167"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-168">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-35-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-35">x</script></span> mang nghĩa khác (và thậm chí khác về chiều) trong mỗi tác vụ (hoặc theo mỗi người dùng), giả sử là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-169"><span class="MJXp-msubsup" id="MJXp-Span-170"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-171" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-172" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-173">(</span><span class="MJXp-mn" id="MJXp-Span-174">1</span><span class="MJXp-mo" id="MJXp-Span-175">)</span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-36-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-36">x^{(1)}</script></span>, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-176"><span class="MJXp-msubsup" id="MJXp-Span-177"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-179" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-180">(</span><span class="MJXp-mn" id="MJXp-Span-181">2</span><span class="MJXp-mo" id="MJXp-Span-182">)</span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-37-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-37">x^{(2)}</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-183"><span class="MJXp-msubsup" id="MJXp-Span-184"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-185" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-186" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-187">(</span><span class="MJXp-mn" id="MJXp-Span-188">3</span><span class="MJXp-mo" id="MJXp-Span-189">)</span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-38-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-38">x^{(3)}</script></span>. Các mức thấp phụ thuộc vào tác vụ, còn các mức cao được dùng chung. Các mức thấp học để dịch từ đầu vào ứng với từng tác vụ trở thành tập đặc trưng dùng chung.</p><p>Trong những trường hợp liên quan tới thích ứng miền, tác vụ (và ánh xạ tối ưu từ đầu ra đến đầu vào) giữ nguyên giữa các cấu hình, nhưng phân phối đầu vào hơi khác nhau. Ví dụ, xét tác vụ phân tích cảm tình bao gồm xác định một câu nhận xét mang nghĩa tích cực hay tiêu cực. Những nhận xét được đăng trên các trang web đến từ rất nhiều chủ đề. Bối cảnh thích ứng miền xuất hiện khi một bộ dự đoán <em>cảm tình</em> được huấn luyện trên các nhận định cho nội dung đại chúng như sách, video và âm nhạc được áp dụng đối với đồ điện tử tiêu dùng như tivi hoặc điện thoại thông minh. Có thể hình dung rằng có một hàm cơ bản có chức năng đánh giá một câu là tích cực, trung lập hoặc tiêu cực nhưng tất nhiên từ ngữ và văn phong có thể khác nhau tuỳ vào miền, vậy nên khó tổng quát hoá trên các miền khác nhau. Tiền huấn luyện không giám sát đơn giản (với khử nhiễu bộ tự mã hoá) rất tốt cho phân tích cảm tình với thích ứng miền.</p><p>Một vấn đề liên quan là chuyển dịch khái niệm (concept drift), có thể xem như là một dạng học chuyển tiếp vì có sự thay đổi dần dần của phân phối dữ liệu theo thời gian. Cả chuyển dịch khái niệm và học chuyển tiếp có thể xem là các dạng cụ thể của học đa tác vụ. Trong khi từ “học đa tác vụ” thường liên quan đến tác vụ học giám sát, khái niệm tổng quát của học chuyển tiếp có thể được áp dụng cho học không giám sát cũng như học tăng cường.</p><p>Trong những trường hợp này, mục tiêu là tận dụng dữ liệu trong cấu hình đầu tiên để trích xuất thông tin có ích khi học hoặc thậm chí trực tiếp đưa ra dự báo trong cấu hình thứ hai. Ý tưởng cốt lõi của học biểu diễn là cùng một biểu diễn hữu ích trong cả hai cấu hình. Việc dùng cùng một biểu diễn trong hai cấu hình cho phép mang lại lợi ích từ dữ liệu huấn luyện sẵn có cho cả hai tác vụ.</p><p>Như đã đề cập, học không giám sát cho học chuyển tiếp thường đạt được thành công trong các cuộc thi học máy (Mesnil và cộng sự, 2011; Goodfellow và cộng sự, 2011). Trong lần đầu tiên tổ chức các cuộc thi này, thí nghiệm được cài đặt như sau: Mỗi người tham gia được cung cấp một tập dữ liệu từ cấu hình gốc (phân phối <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-190"><span class="MJXp-msubsup" id="MJXp-Span-191"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-193" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-39-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-39">P_1</script></span>), minh họa ý nghĩa của một vài chủ đề rồi sử dụng dữ liệu này để học một không gian đặc trưng (ánh xạ input thành một biểu diễn nào đó) sao cho khi áp dụng ánh xạ này cho input từ cấu hình chuyển tiếp (phân phối <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-194"><span class="MJXp-msubsup" id="MJXp-Span-195"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-197" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-40-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-40">P_2</script></span>) thì một bộ phân lớp tuyến tính có thể được huấn luyện và tổng quát hoá tốt từ lượng nhỏ dữ liệu đã được gán nhãn. Một kết quả nổi bật từ cuộc thi này là khi một kiến trúc sử dụng biểu diễn càng sâu (được học hoàn toàn không giám sát từ dữ liệu thu thập với cấu hình ban đầu, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-198"><span class="MJXp-msubsup" id="MJXp-Span-199"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-200" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-201" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-41-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-41">P_1</script></span>) thì đường cong của việc học về các chủ đề trong cấu hình sau (<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-202"><span class="MJXp-msubsup" id="MJXp-Span-203"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204" style="margin-right: 0.05em;">P</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-205" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-42-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-42">P_2</script></span>) trở nên tốt hơn nhiều. Với các biểu diễn sâu, cần ít mẫu có nhãn hơn để đạt được hiệu suất tiệm cận tổng quát rõ ràng.</p><p>Hai dạng đặc biệt (extreme) của học chuyển tiếp là one-shot learning và zero-shot learning (còn gọi là học phi dữ liệu). Chỉ có một mẫu đã gán nhãn của tác vụ chuyển tiếp được cung cấp cho one-shot learning, thậm chí là không mẫu nào đối với zero-shot learning.</p><p>One-shot learning (Fei-Fei và cộng sự, 2006) là khả thi vì biểu diễn đã phân chia rạch ròi các lớp cơ bản trong giai đoạn đầu. Trong giai đoạn học chuyển tiếp, chỉ cần một mẫu có nhãn để suy ra nhãn của các mẫu xung quanh nó trong không gian biểu diễn. Kết quả này được mở rộng cho các nhân tố sai số liên quan đến bất biến đã được phân tách rõ ràng trong không gian biểu diễn, và ta đã học một cách nào đó để biết nhân tố có ảnh hưởng hay không ảnh hưởng đến phân biệt các đối tượng của một số chủ đề.</p><p>Ví dụ cho zero-shot learning, xét bài toán có một bộ học đọc qua một tập văn bản lớn và giải bài toán nhận diện đối tượng. Có thể nhận ra một đối tượng cụ thể mà không cần xem qua bất kì hình ảnh nào của nó nếu đoạn mô tả về đối tượng đó đủ tốt. Ví dụ, khi đã biết mèo có bốn chân và có tai nhọn, bộ học có thể đoán được một hình ảnh là mèo dù chưa từng nhìn qua hình ảnh của mèo.</p><p>Học phi dữ liệu (Larochelle và cộng sự, 2008) và zero-shot learning (Palatucci và cộng sự, 2009; Socher và cộng sự, 2013b) chỉ khả thi khi thông tin bổ sung đã được khai thác trong quá trình huấn luyện. Ta xem như diễn biến của zero-shot learning bao gồm 3 biến ngẫu nhiên: Input truyền thống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-206"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-43-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-43">x</script></span>, output truyền thống <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-208"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-209">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-44-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-44">y</script></span> và biến ngẫu nhiên bổ sung mô tả tác vụ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-210"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-211">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-45-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-45">T</script></span>. Mô hình được huấn luyện để ước lượng phân phối có điều kiện <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-212"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-213">p</span><span class="MJXp-mo" id="MJXp-Span-214" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">y</span><span class="MJXp-mrow" id="MJXp-Span-216"><span class="MJXp-mo" id="MJXp-Span-217" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-218">x</span><span class="MJXp-mo" id="MJXp-Span-219" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">T</span><span class="MJXp-mo" id="MJXp-Span-221" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-46-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-46">p(y|x,T)</script></span>, trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-222"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-47-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-47">T</script></span> mô tả về tác vụ mà ta muốn mô hình thực thi. Trong ví dụ về nhận diện mèo sau khi đọc về mèo, output là biến nhị phân y, giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-224"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-225">y</span><span class="MJXp-mo" id="MJXp-Span-226" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-227">1</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-48-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-48">y = 1</script></span> nghĩa là “đúng” và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-228"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">y</span><span class="MJXp-mo" id="MJXp-Span-230" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-231">0</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-49-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-49">y = 0</script></span> nghĩa là “sai”. Biến tác vụ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-232"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-233">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-50-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-50">T</script></span> biểu diễn câu hỏi, chẳng hạn “Có mèo trong bức hình không?” Nếu ta có tập huấn luyện chứa các mẫu không giám sát của các đối tượng trong cùng không gian với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-234"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-51-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-51">T</script></span>, ta có thể suy ra nghĩa của các thực thể chưa thấy trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-236"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-237">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-52-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-52">T</script></span>. Trong ví dụ về nhận diện mèo, điều quan trọng là dữ liệu văn bãn chưa có nhãn phải có các câu như “mèo có 4 chân” hoặc “mèo có tai nhọn”.</p><p>Zero-shot learning đòi hỏi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-238"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-239">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-53-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-53">T</script></span> được biểu diễn một cách hữu ích cho việc tổng quát hoá. Ví dụ, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-240"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-241">T</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-54-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-54">T</script></span> không thể là <em>vector đơn trội</em> (one-hot vector) xác định chủ đề. Thay vào đó, Socher và cộng sự (2013b) đề xuất một biểu diễn phân phối của các chủ đề cho đối tượng bằng cách sử dụng một phép nhúng từ đã học cho các từ vựng liên kết của từng chủ đề.</p><p>Hiện tượng tương tự xảy ra trong dịch máy (Klementiev và cộng sự, 2012; Mikolov và cộng sự, 2013b; Gouws và cộng sự, 2014): ta có từ vựng trong một ngôn ngữ, quan hệ giữa các từ có thể được học từ một tập văn bản đơn ngôn ngữ; bên cạnh đó, ta dịch những câu liên quan các từ trong một ngôn ngữ ra ngôn ngữ khác. Mặc dù không cần gán nhãn các mẫu dịch có nhiệm vụ dịch từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-242"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">A</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-55-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-55">A</script></span> trong ngôn ngữ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-244"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245">X</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-56-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-56">X</script></span> sang từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-246"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-247">B</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-57-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-57">B</script></span> ngôn ngữ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-248"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-249">Y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-58-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-58">Y</script></span>, ta có thể tổng quát hoá và đoán nghĩa của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-250"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">A</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-59-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-59">A</script></span> nhờ vào biểu diễn phân phối từ vựng ngôn ngữ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-252"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-253">X</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-60-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-60">X</script></span> và biểu diễn phân phối từ vựng ngôn ngữ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-254"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-255">Y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-61-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-61">Y</script></span>, rồi tạo ra liên kết (có thể là hai chiều) giữa hai không gian thông qua huấn luyện các mẫu là các cặp câu của 2 ngôn ngữ. Việc chuyển tiếp này thành công nhất khi tất cả ba thành phần (hai biểu diễn và các quan hệ của chúng) được học cùng nhau.</p><p>Zero-shot learning là một trường hợp cụ thể của học chuyển tiếp. Nguyên tắc tương tự giải thích làm sao người ta có thể thực thi việc học đa phương thức, nghĩa là lấy một biểu diễn trong một phương thức, một biểu diễn trong một phương thức khác và quan hệ (nói chung là một phân phối kết hợp) giữa các cặp <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-256"><span class="MJXp-mo" id="MJXp-Span-257" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-258">x</span><span class="MJXp-mo" id="MJXp-Span-259" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260">y</span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-62-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-62">(x, y)</script></span> gồm một quan sát <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-262"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-63-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-63">x</script></span> chứa trong một phương thức và một quan sát <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-264"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-265">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-64-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-64">y</script></span> chứa trong phương thức còn lại (Srivastava and Salakhutdinov 2012). Bằng cách học cả ba tập tham số (từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-266"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-267">x</span><span class="MJXp-mo" id="MJXp-Span-268" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-65-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-65">x, y</script></span> đến các biểu diễn tương ứng của nó và quan hệ giữa hai biểu diễn), các khái niệm trong một biểu diễn được giữ trong biểu diễn kia, điều này làm cho khái niệm được tổng quát hoá cho các cặp mới. Tiến trình được minh họa trong hình 15.3.</p><p><img src="https://i.imgur.com/MpnrHE2.png" alt=""></p><p><strong>Hình 15.3:</strong> Học chuyển tiếp giữa hai miền <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-270"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-66-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-66">x</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-272"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-67-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-67">y</script></span> cho phép zero-shot learning. Các mẫu đã có hoặc chưa có nhãn của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-274"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-68-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-68">x</script></span> cho phép học một hàm biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-276"><span class="MJXp-msubsup" id="MJXp-Span-277"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-279" style="vertical-align: -0.4em;">x</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-69-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-69">f_x</script></span>, tương tự với các mẫu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-280"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-70-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-70">y</script></span> để học hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-282"><span class="MJXp-msubsup" id="MJXp-Span-283"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-284" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-285" style="vertical-align: -0.4em;">y</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-71-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-71">f_y</script></span>. Mỗi áp dụng của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-286"><span class="MJXp-msubsup" id="MJXp-Span-287"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-289" style="vertical-align: -0.4em;">x</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-72-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-72">f_x</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-290"><span class="MJXp-msubsup" id="MJXp-Span-291"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-293" style="vertical-align: -0.4em;">y</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-73-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-73">f_y</script></span> thể hiện bằng mũi tên hướng lên. Khoảng cách trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-294"><span class="MJXp-msubsup" id="MJXp-Span-295"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-297" style="vertical-align: -0.4em;">x</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-74-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-74">h_x</script></span> cho thấy thông tin về độ tương tự của từng cặp điểm trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-298"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-299">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-75-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-75">x</script></span> có thể có ý nghĩa hơn khoảng cách trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-300"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-76-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-76">x</script></span>. Tương tự, khoảng cách trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-302"><span class="MJXp-msubsup" id="MJXp-Span-303"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-305" style="vertical-align: -0.4em;">y</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-77-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-77">h_y</script></span> cho thấy độ tương tự giữa một cặp điểm bất kì trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-306"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-78-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-78">y</script></span>. Hai hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-308"><span class="MJXp-msubsup" id="MJXp-Span-309"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-310" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-311" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mo" id="MJXp-Span-312" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-msubsup" id="MJXp-Span-313"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-315" style="vertical-align: -0.4em;">y</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-79-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-79">f_x, f_y</script></span> được thể hiện bằng mũi tên chấm hai chiều. Các mẫu có nhãn (đường ngang đứt) là các cặp <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-316"><span class="MJXp-mo" id="MJXp-Span-317" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">x</span><span class="MJXp-mo" id="MJXp-Span-319" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">y</span><span class="MJXp-mo" id="MJXp-Span-321" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-80-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-80">(x, y)</script></span> cho phép học ánh xạ theo một chiều hoặc hai chiều (mũi tên hai chiều nét liền) giữa biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-322"><span class="MJXp-msubsup" id="MJXp-Span-323"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-325" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mo" id="MJXp-Span-326" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">x</span><span class="MJXp-mo" id="MJXp-Span-328" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-81-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-81">f_x(x)</script></span> và biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-329"><span class="MJXp-msubsup" id="MJXp-Span-330"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-331" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-332" style="vertical-align: -0.4em;">y</span></span><span class="MJXp-mo" id="MJXp-Span-333" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">y</span><span class="MJXp-mo" id="MJXp-Span-335" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-82-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-82">f_y(y)</script></span> và giữ các biểu diễn này với nhau. Học phi dữ liệu được kích hoạt như sau. Một bộ học có thể kết hợp hình ảnh <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-336"><span class="MJXp-msubsup" id="MJXp-Span-337"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-338" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-339" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-340">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-341">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-342">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-343">t</span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-83-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-83">x_{test}</script></span> và từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-344"><span class="MJXp-msubsup" id="MJXp-Span-345"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-346" style="margin-right: 0.05em;">y</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-347" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-348">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-349">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-350">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-351">t</span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-84-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-84">y_{test}</script></span> thậm chí khi chưa hình ảnh nào của từ đó được giới thiệu, đơn giản bởi vì biểu diễn của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-352"><span class="MJXp-msubsup" id="MJXp-Span-353"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-355" style="vertical-align: -0.4em;">y</span></span><span class="MJXp-mo" id="MJXp-Span-356" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-357"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358" style="margin-right: 0.05em;">y</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-359" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-363">t</span></span></span><span class="MJXp-mo" id="MJXp-Span-364" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-85-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-85">f_y(y_{test})</script></span> và biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-365"><span class="MJXp-msubsup" id="MJXp-Span-366"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-368" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mo" id="MJXp-Span-369" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-370"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-371" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-372" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-373">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-375">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-376">t</span></span></span><span class="MJXp-mo" id="MJXp-Span-377" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-86-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-86">f_x(x_{test})</script></span> cho thấy sự liên quan thông quan ánh xạ giữa các không gian biểu diễn. Nó hoạt động dù hình ảnh và từ tương ứng chưa bao giờ được ghép cặp với nhau, tuy nhiên biểu diễn vector đặc trưng tương ứng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-378"><span class="MJXp-msubsup" id="MJXp-Span-379"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-381" style="vertical-align: -0.4em;">x</span></span><span class="MJXp-mo" id="MJXp-Span-382" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-383"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-384" style="margin-right: 0.05em;">x</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-385" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-386">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-388">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-389">t</span></span></span><span class="MJXp-mo" id="MJXp-Span-390" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-87-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-87">f_x(x_{test})</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-391"><span class="MJXp-msubsup" id="MJXp-Span-392"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393" style="margin-right: 0.05em;">f</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-394" style="vertical-align: -0.4em;">y</span></span><span class="MJXp-mo" id="MJXp-Span-395" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-396"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-397" style="margin-right: 0.05em;">y</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-398" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-399">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-401">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-402">t</span></span></span><span class="MJXp-mo" id="MJXp-Span-403" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-88-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-88">f_y(y_{test})</script></span> có liên hệ với nhau. Hình được đưa vào từ gợi ý của Hrant Khachatrian.<br>
— Kết thúc phần dịch của Huy Thai-----</p><p>— Trang Le bắt đầu dịch từ đây(trang 439)—</p><h1 id="153-Học-bán-giám-sát-gỡ-rối-các-yếu-tố-nhân-quả-Semi-Supervised-Disentangling-of-Causal-Factors"><a class="anchor hidden-xs" href="#153-Học-bán-giám-sát-gỡ-rối-các-yếu-tố-nhân-quả-Semi-Supervised-Disentangling-of-Causal-Factors" title="153-Học-bán-giám-sát-gỡ-rối-các-yếu-tố-nhân-quả-Semi-Supervised-Disentangling-of-Causal-Factors"><span class="octicon octicon-link"></span></a>15.3 Học bán giám sát gỡ rối các yếu tố nhân quả (Semi-Supervised Disentangling of Causal Factors)</h1><p>Một câu hỏi quan trọng về học biểu diễn là: Những gì làm cho một biểu diễn tốt hơn những biểu diễn khác? Giả thuyết về một biểu diễn lý tưởng là một biểu diễn mà trong đó những đặc trưng <strong>trong biểu diễn</strong> tương ứng với nguyên nhân cơ bản của <strong>dữ liệu quan sát</strong>, với các đặc trưng hoặc các hướng riêng biệt trong <strong>không gian đặc trưng</strong> tương ứng với các nguyên nhân khác nhau. Giả thiết này thúc đẩy các hướng tiếp cận trong đó chúng ta trước tiên tìm kiếm một biểu diễn tốt cho <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-404"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-405">p</span><span class="MJXp-mo" id="MJXp-Span-406" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-407">x</span><span class="MJXp-mo" id="MJXp-Span-408" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-89-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-89">p(\boldsymbol x)</script></span>. Biểu diễn đó cũng có thể là một biểu diễn tốt cho việc tính toán <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-409"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-410">p</span><span class="MJXp-mo" id="MJXp-Span-411" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-412">y</span><span class="MJXp-mspace" id="MJXp-Span-413" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-414"><span class="MJXp-mo" id="MJXp-Span-415" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-416" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-417">x</span><span class="MJXp-mo" id="MJXp-Span-418" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-90-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-90">p(\boldsymbol y\, |\, \boldsymbol x )</script></span> nếu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-419"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-420">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-91-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-91">\boldsymbol y</script></span> là một trong những tác nhân nổi bật nhất của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-421"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-422">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-92-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-92">\boldsymbol x</script></span>. Ý tưởng này đã dẫn dắt một lượng lớn những nghiên cứu về học sâu ít nhất là từ những năm 1990s [Becker và Hinton, 1992; Hinton và Sejnowski, 1990] một cách chi tiết hơn. Đối với các lập luận khác về thời gian học bán giám sát có thể hoạt động tốt hơn học giám sát thuần túy, chúng tôi giới thiệu người đọc đến phần 1.2 của [Chapelle và cộng sự, 2006].</p><p>Trong các phương pháp tiếp cận khác để học biểu diễn, chúng ta thường quan tâm đến một biểu diễn dễ mô hình hóa — ví dụ, một biểu diễn có các biểu diễn thưa thớt hoặc độc lập với nhau. Một đại diện phân tách rõ ràng các yếu tố nhân quả cơ bản có thể không nhất thiết phải là một yếu tố dễ mô hình hóa. Tuy nhiên, một phần nữa của giả thuyết thúc đẩy việc học bán giám sát thông qua việc học biểu diễn không giám sát là đối với nhiều tác vụ AI (trí tuệ nhân tạo), hai đặc tính này trùng khớp nhau: Một khi chúng ta có thể có được những lời giải thích cơ bản cho những gì chúng ta quan sát, chúng ta dễ dàng cô lập những thuộc tính riêng lẻ với những thuộc tính khác . Đặc biệt, nếu một biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-423"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-424">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-93-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-93">\boldsymbol h</script></span> đại diện cho nhiều nguyên nhân cơ bản của quan sát <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-425"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-426">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-94-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-94">\boldsymbol x</script></span>, và đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-427"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-428">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-95-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-95">\boldsymbol y</script></span> là một trong những nguyên nhân nổi bật nhất, thì rất dễ dự đoán <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-429"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-430">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-96-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-96">\boldsymbol y</script></span> từ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-431"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-432">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-97-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-97">\boldsymbol h</script></span>.</p><p>Đầu tiên, chúng ta hãy xem cách học bán giám sát có thể thất bại vì học không giám sát của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-433"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-434">p</span><span class="MJXp-mo" id="MJXp-Span-435" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-436">x</span><span class="MJXp-mo" id="MJXp-Span-437" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-98-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-98">p(\text x)</script></span> không giúp đỡ cho việc học <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-438"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-439">p</span><span class="MJXp-mo" id="MJXp-Span-440" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-441">y</span><span class="MJXp-mspace" id="MJXp-Span-442" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-443"><span class="MJXp-mo" id="MJXp-Span-444" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-445" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-446">x</span><span class="MJXp-mo" id="MJXp-Span-447" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-99-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-99">p(\text y\, |\, \text x )</script></span>. Xem xét ví dụ, trong trường hợp mà <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-448"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-449">p</span><span class="MJXp-mo" id="MJXp-Span-450" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-451">x</span><span class="MJXp-mo" id="MJXp-Span-452" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-100-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-100">p(\text x)</script></span> là một phân bố đồng đều và chúng ta muốn học  <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-453"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-454">f</span><span class="MJXp-mo" id="MJXp-Span-455" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-456">x</span><span class="MJXp-mo" id="MJXp-Span-457" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-458" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mrow" id="MJXp-Span-459"><span class="MJXp-mi undefined" id="MJXp-Span-460">E</span></span><span class="MJXp-mrow" id="MJXp-Span-461"><span class="MJXp-mo" id="MJXp-Span-462" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-463">y</span><span class="MJXp-mspace" id="MJXp-Span-464" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-465"><span class="MJXp-mo" id="MJXp-Span-466" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-467" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-468">x</span><span class="MJXp-mo" id="MJXp-Span-469" style="margin-left: 0em; margin-right: 0em;">]</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-101-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-101">f(\boldsymbol x) = \mathbb{E} \left[\boldsymbol y\,|\,\boldsymbol x \right]</script></span>. Quan sát tập huấn luyện của các giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-470"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-471">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-102-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-102">\boldsymbol x</script></span> riêng lẻ không cho chúng ta bất cứ thông tin nào về <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-472"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-473">p</span><span class="MJXp-mo" id="MJXp-Span-474" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-475">y</span><span class="MJXp-mspace" id="MJXp-Span-476" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-477"><span class="MJXp-mo" id="MJXp-Span-478" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-479" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-480">x</span><span class="MJXp-mo" id="MJXp-Span-481" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-103-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-103">p(\boldsymbol y\, |\, \boldsymbol x )</script></span>.</p><p><img src="https://i.imgur.com/Xaeus3I.png" alt=""></p><blockquote>
<p>Hình 15.4: Mô hình hỗn hợp. Ví dụ về mật độ trên <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-482"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-483">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-104-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-104">x</script></span> là hỗn hợp trên ba thành phần. Thành phần đặc tính là một yếu tố giải thích cơ bản, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-484"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-485">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-105-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-105">y</script></span>. Bởi vì thành phần hỗn hợp (ví dụ: Các lớp đối tượng tự nhiên trong dữ liệu hình ảnh) có tính nổi bật về mặt thống kê, chỉ cần mô hình hóa <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-486"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-487">p</span><span class="MJXp-mo" id="MJXp-Span-488" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-489">x</span><span class="MJXp-mo" id="MJXp-Span-490" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-106-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-106">p(x)</script></span> theo các không giám sát và ví dụ không được gắn nhãn đã làm lộ yếu tố <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-491"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-107-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-107">y</script></span>.</p>
</blockquote><p>Tiếp theo, chúng ta hãy xem xét một ví dụ đơn giản về các học bán giám sát có thể thành công. Xem xét tình huống trong đó <strong>x</strong> phát sinh từ một hỗn hợp, với một thành phần hỗn hợp trên mỗi giá trị của <strong>y</strong>, như minh họa trong hình 15.4. Nếu các thành phần hỗn hợp được tách biệt tốt, thì mô hình <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-493"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-494">p</span><span class="MJXp-mo" id="MJXp-Span-495" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-496">x</span><span class="MJXp-mo" id="MJXp-Span-497" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-108-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-108">p(\text x)</script></span> cho thấy chính xác vị trí của mỗi thành phần, và một ví dụ có nhãn của mỗi lớp sẽ đủ để học hoàn toàn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-498"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-499">p</span><span class="MJXp-mo" id="MJXp-Span-500" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-501">y</span><span class="MJXp-mspace" id="MJXp-Span-502" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-503"><span class="MJXp-mo" id="MJXp-Span-504" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-505" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-506">x</span><span class="MJXp-mo" id="MJXp-Span-507" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-109-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-109">p(\text y \, |\,\text  x)</script></span>. Nhưng nói chung, điều gì có thể liên kết  <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-508"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-509">p</span><span class="MJXp-mo" id="MJXp-Span-510" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-511">y</span><span class="MJXp-mspace" id="MJXp-Span-512" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-513"><span class="MJXp-mo" id="MJXp-Span-514" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-515" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-516">x</span><span class="MJXp-mo" id="MJXp-Span-517" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-110-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-110">p(\text y \, |\,\text  x)</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-518"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-519">p</span><span class="MJXp-mo" id="MJXp-Span-520" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-521">x</span><span class="MJXp-mo" id="MJXp-Span-522" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-111-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-111">p(\text x)</script></span> với nhau?</p><p>Nếu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-523"><span class="MJXp-mtext" id="MJXp-Span-524">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-112-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-112">\text y</script></span> liên kết chặt chẽ với các yếu tố nhân quả của <strong>x</strong>, thì <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-525"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-526">p</span><span class="MJXp-mo" id="MJXp-Span-527" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-528">x</span><span class="MJXp-mo" id="MJXp-Span-529" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-113-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-113">p(\text x)</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-530"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-531">p</span><span class="MJXp-mo" id="MJXp-Span-532" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-533">y</span><span class="MJXp-mspace" id="MJXp-Span-534" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-535"><span class="MJXp-mo" id="MJXp-Span-536" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-537" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-538">x</span><span class="MJXp-mo" id="MJXp-Span-539" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-114-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-114">p(\text y \, |\,\text  x)</script></span> sẽ liên kết chặt chẽ, và học biểu diễn không giám sát cố gắng giải quyết các yếu tố cơ bản của các biến thể có thể hữu ích như một chiến lược học bán giám sát.</p><p>Xem xét giả định rằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-540"><span class="MJXp-mtext" id="MJXp-Span-541">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-115-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-115">\text y</script></span> là một trong những yếu tố cơ bản của <strong>x</strong>, và để <strong>h</strong> biểu diễn cho tất cả những yếu tố đó. Tiến trình sinh mẫu chính xác có thể hiểu như cấu trúc theo mô hình đồ họa có hướng này, với <strong>h</strong> sinh ra của <strong>x</strong>:</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-542"><span class="MJXp-mtable" id="MJXp-Span-543"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-544" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-545" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-546">p</span><span class="MJXp-mo" id="MJXp-Span-547" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-548">h</span><span class="MJXp-mo" id="MJXp-Span-549" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mtext" id="MJXp-Span-550">x</span><span class="MJXp-mo" id="MJXp-Span-551" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-552" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-553">p</span><span class="MJXp-mo" id="MJXp-Span-554" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-555">x</span><span class="MJXp-mspace" id="MJXp-Span-556" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-557"><span class="MJXp-mo" id="MJXp-Span-558" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-559" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-560">h</span><span class="MJXp-mo" id="MJXp-Span-561" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-562">p</span><span class="MJXp-mo" id="MJXp-Span-563" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-564">h</span><span class="MJXp-mo" id="MJXp-Span-565" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-566" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><div class="MathJax_Display MathJax_Processing"><span class="MathJax" id="MathJax-Element-116-Frame" tabindex="0"></span></div><script type="math/tex; mode=display" id="MathJax-Element-116">
p(\text h,\text x) = p(\text x \,|\,\text h) p(\text h).
\tag{15.1}</script></span></p><p>Do đó, dữ liệu có xác suất biên</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-567"><span class="MJXp-mtable" id="MJXp-Span-568"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-569" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-570" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-571">p</span><span class="MJXp-mo" id="MJXp-Span-572" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-573">x</span><span class="MJXp-mo" id="MJXp-Span-574" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-575" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-576"><span class="MJXp-mrow" id="MJXp-Span-577" style="margin-right: 0.05em;"><span class="MJXp-mi undefined" id="MJXp-Span-578">E</span></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-579" style="vertical-align: -0.4em;">h</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-580">p</span><span class="MJXp-mo" id="MJXp-Span-581" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-582">x</span><span class="MJXp-mspace" id="MJXp-Span-583" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-584"><span class="MJXp-mo" id="MJXp-Span-585" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-586" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-587">h</span><span class="MJXp-mo" id="MJXp-Span-588" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-589" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><div class="MathJax_Display MathJax_Processing"><span class="MathJax" id="MathJax-Element-117-Frame" tabindex="0"></span></div><script type="math/tex; mode=display" id="MathJax-Element-117">
p(\boldsymbol x) = \mathbb{E}_h p(\boldsymbol x \,|\,\boldsymbol h).
\tag{15.2}</script></span></p><p>Từ quan sát đơn giản này, chúng ta kết luận rằng mô hình tốt nhất có thể của <strong>x</strong> (từ quan điểm tổng quát) là mô hình khám phá cấu trúc “đúng” ở trên, với <strong>h</strong> là biến ẩn giải thích các biến thể quan sát được trong <strong>x</strong>. Việc học biểu diễn “lý tưởng” được thảo luận ở trên do đó sẽ phục hồi các yếu tố ẩn này. Nếu <strong>y</strong> là một trong số này (hoặc liên quan chặt chẽ với một trong số chúng), thì sẽ dễ dàng học cách dự đoán <strong>y</strong> từ một biểu diễn như vậy. Chúng ta cũng thấy rằng sự phân bố có điều kiện của <strong>y</strong> cho <strong>x</strong> được ràng buộc bởi quy luật Bayes với các thành phần trong phương trình:</p><p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-590"><span class="MJXp-mtable" id="MJXp-Span-591"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-592" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-593" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-594">p</span><span class="MJXp-mo" id="MJXp-Span-595" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-596">y</span><span class="MJXp-mspace" id="MJXp-Span-597" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-598"><span class="MJXp-mo" id="MJXp-Span-599" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-600" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-601">x</span><span class="MJXp-mo" id="MJXp-Span-602" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-603" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mstyle" id="MJXp-Span-604"><span class="MJXp-mfrac" id="MJXp-Span-605" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-606">p</span><span class="MJXp-mo" id="MJXp-Span-607" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-608">x</span><span class="MJXp-mspace" id="MJXp-Span-609" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-610"><span class="MJXp-mo" id="MJXp-Span-611" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-612" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-613">y</span><span class="MJXp-mo" id="MJXp-Span-614" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-615">p</span><span class="MJXp-mo" id="MJXp-Span-616" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-617">y</span><span class="MJXp-mo" id="MJXp-Span-618" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-619">p</span><span class="MJXp-mo" id="MJXp-Span-620" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-621">x</span><span class="MJXp-mo" id="MJXp-Span-622" style="margin-left: 0em; margin-right: 0em;">)</span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_Display MathJax_Processing"><span class="MathJax" id="MathJax-Element-118-Frame" tabindex="0"></span></div><script type="math/tex; mode=display" id="MathJax-Element-118"> p(\text y \,|\, \text x) = \dfrac{p(\text x \,|\, \text y) p(\text y)}{p(\text x)}
\tag{15.3}</script></span></p><p>Do đó, xác suất biên <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-623"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-624">p</span><span class="MJXp-mo" id="MJXp-Span-625" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-626">x</span><span class="MJXp-mo" id="MJXp-Span-627" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-119-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-119">p(\text x)</script></span> liên kết mật thiết với xác suất có điều kiện <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-628"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-629">p</span><span class="MJXp-mo" id="MJXp-Span-630" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-631">y</span><span class="MJXp-mspace" id="MJXp-Span-632" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-633"><span class="MJXp-mo" id="MJXp-Span-634" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-635" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-636">x</span><span class="MJXp-mo" id="MJXp-Span-637" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-120-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-120">p(\text y \,|\, \text x)</script></span>, và sự hiểu biết về cấu trúc của vấn đề trước có thể hữu ích để học sau này. Do đó, trong những tình huống tôn trọng những giả định này, học bán giám sát sẽ cải thiện hiệu suất.</p><p>Một vấn đề nghiên cứu quan trọng liên quan đến thực tế là hầu hết các quan sát được hình thành bởi một số lượng lớn các nguyên nhân cơ bản. Giả sử <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-638"><span class="MJXp-mtext" id="MJXp-Span-639">y</span><span class="MJXp-mo" id="MJXp-Span-640" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-641"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-642" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-643" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-121-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-121">\text y = h_i</script></span>, nhưng người học không giám sát không biết <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-644"><span class="MJXp-msubsup" id="MJXp-Span-645"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-646" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-647" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-122-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-122">h_i</script></span>. Giải pháp brute force là cho người học không giám sát tìm hiểu một biểu diễn nắm bắt tất cả các yếu tố sinh mẫu hợp lý <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-648"><span class="MJXp-msubsup" id="MJXp-Span-649"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-650" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-651" style="vertical-align: -0.4em;">j</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-123-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-123">h_j</script></span> và tách chúng ra khỏi nhau, do đó làm cho nó dễ dàng dự đoán <strong>y</strong> từ <strong>h</strong>, không chú ý tới trường hợp <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-652"><span class="MJXp-msubsup" id="MJXp-Span-653"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-654" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-655" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-124-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-124">h_i</script></span> được kết hợp với <strong>y</strong>.</p><p>Trong thực tế, giải pháp brute force là không khả thi bởi vì không thể nắm bắt được tất cả hoặc hầu hết các yếu tố biến thiên ảnh hướng đến một quan sát. Ví dụ, trong một cảnh trực quan, có luôn luôn nên biểu diễn mã hóa tất cả các đối tượng nhỏ nhất trong nền? Đó là một hiện tượng tâm lý được ghi chép đầy đủ rằng con người không nhận thức được những thay đổi trong môi trường của họ mà không liên quan ngay đến công việc mà họ đang thực hiện — xem, ví dụ [Simons và Levin, 1998]. Một giới hạn nghiên cứu quan trọng trong học bán giám sát là xác định những gì cần mã hóa trong từng tình huống. Hiện tại, hai chiến lược chính để xử lý một số lượng lớn nguyên nhân cơ bản là sử dụng tín hiệu học giám sát cùng lúc với tín hiệu học không giám sát để mô hình sẽ chọn nắm bắt các yếu tố có liên quan nhất của biến thể hoặc sử dụng biểu diễn lớn hơn nhiều nếu sử dụng học không giám sát hoàn toàn.</p><p>Một chiến lược mới nổi cho học không giám sát là chỉnh sửa việc xác định nguyên nhân cơ bản nào nổi bật nhất. Trong lịch sử, các bộ tự mã hóa và các mô hình sinh mẫu đã được đào tạo để tối ưu hóa một tiêu chí được xác định, thường tương tự với sai số bình phương trung bình. Những tiêu chí này xác định nguyên nhân nào được coi là nổi bật. Ví dụ, lỗi bình phương trung bình được áp dụng cho các điểm ảnh của một hình ảnh ngầm định rằng nguyên nhân cơ bản chỉ là nổi bật nếu nó thay đổi đáng kể độ sáng của một số lượng lớn điểm ảnh. Điều này có thể có vấn đề nếu tác vụ chúng ta muốn giải quyết liên quan đến việc tương tác với các đối tượng nhỏ. Xem hình 15.5 để biết ví dụ về tác vụ robot trong đó bộ tự mã hóa đã thất bại trong việc học để mã hóa một quả bóng bàn nhỏ. Robot này có khả năng tương tác thành công với các vật thể lớn hơn, chẳng hạn như quả bóng chày, nó nổi bật hơn theo sai số bình phương trung bình.</p><p><img src="https://i.imgur.com/HjAPwbw.png" alt=""></p><blockquote>
<p>Hình 15.5: Một bộ tự mã hóa được huấn luyện với sai số bình phương trung bình cho một tác vụ robot đã thất bại trong việc xây dựng lại một quả bóng bàn. Sự tồn tại của quả bóng bàn và tất cả các tọa độ không gian của nó là các yếu tố nhân quả quan trọng tạo ra hình ảnh và có liên quan đến tác vụ robot. Thật không may, bộ tự mã hóa có dung lượng giới hạn và việc đào tạo với sai số bình phương trung bình không xác định được quả bóng bàn là đủ nổi bật để mã hóa. Hình ảnh được cung cấp bởi Chelsea Finn.</p>
</blockquote><p>Có thể có các định nghĩa khác của sự nổi bật. Ví dụ: nếu một nhóm điểm ảnh theo sau một mẫu có thể nhận biết cao, ngay cả khi mẫu đó không liên quan đến độ sáng hoặc độ tối cực độ thì mẫu đó có thể được xem là cực kỳ nổi bật. Một cách để nêu ra một sự định nghĩa của sự kiên định đó là sử dụng một phương pháp được phát triển gần đây, gọi là các mạng đối kháng sinh mẫu [Goodfellow và cộng sự, 2014c]. Trong cách tiếp cận này, một mô hình sinh mẫu được đào tạo để đánh lừa một bộ phân loại lan truyền thuận. Bộ phân loại lan truyền thuận nỗ lực để nhận ra tất cả các mẫu từ mô hình sinh mẫu như là giả và tất cả các mẫu từ tập huấn luyện là thực. Trong khuôn khổ này, bất kỳ mẫu có cấu trúc nào mà mạng lan truyền thuận có thể nhận ra là rất nổi bật. Mạng đối kháng sinh mẫu được mô tả chi tiết hơn trong phần 20.10.4. Với mục đích của việc thảo luận hiện tại, nó là đủ để hiểu rằng các mạng học làm thế nào để xác định những gì là nổi bật. [Lotter và cộng sự, 2015] cho thấy rằng các mô hình được đào tạo để tạo ra hình ảnh của một đầu người sẽ thường bỏ bê việc tạo ra tai người khi được đào tạo với sai số trung bình bình phương, nhưng sẽ thành công tạo ra tai khi được đào tạo với khuôn mẫu đối kháng. Bởi vì tai không phải là cực kỳ sáng hoặc tối so với da xung quanh, chúng không đặc biệt nổi bật theo sai số bình phương, nhưng hình dạng dễ nhận biết và vị trí nhất quán của chúng có nghĩa là mạng lan truyền thuận có thể dễ dàng học cách phát hiện chúng nổi bật dưới khuôn khổ đối kháng sinh mẫu. Xem hình 15.6 để biết các hình ảnh ví dụ. Mạng đối kháng sinh mẫu chỉ là một bước để xác định các yếu tố nào cần được thể hiện. Chúng tôi hy vọng rằng nghiên cứu trong tương lai sẽ khám phá những cách thức tốt hơn để xác định các yếu tố để đại diện và phát triển cơ chế biểu diễn các yếu tố khác nhau tùy thuộc vào tác vụ.<br>
/---- Trần Hoàng Sơn đã review đến đây ---------</p><p><img src="https://i.imgur.com/mPR8OHL.png" alt=""></p><blockquote>
<p>Hình 15.6: Mạng sinh mẫu tiên đoán cung cấp một ví dụ về tầm quan trọng của việc học các đặc trưng nổi bật. Trong ví dụ này, mạng sinh mẫu tiên đoán đã được huấn luyện để dự đoán sự xuất hiện của một mô hình 3-D của một đầu người ở một góc nhìn đặc biệt. (Trái) ảnh thực: đây là hình ảnh chính xác mà mạng neuron nên sinh ra. (Trung tâm) Hình ảnh được tạo ra bởi một mạng sinh mẫu tiên đoán được đào tạo chỉ với sai số bình phương trung bình. Bởi vì tai không gây ra hiện tượng cực đoan về độ sáng so với làn da lân cận, chúng không phải là một sự kiện nổi bật cho mô hình để học cách đại diện cho chúng. (Phải) Hình ảnh được tạo ra bởi một mô hình được đào tạo với sự kết hợp của sai số bình phương trung bình và sai số đối kháng. Sử dụng hàm chi phí đã học này, tai là nổi bật bởi vì chúng tuân theo một mẫu có thể đoán trước được. Học hỏi những nguyên nhân cơ bản nào là quan trọng và có liên quan, đủ để mô hình là một lĩnh vực nghiên cứu quan trọng. Hình ảnh được cung cấp bởi [Lotter et al, 2015].</p>
</blockquote><p>Một lợi ích của việc học các yếu tố nhân quả cơ bản, như được chỉ ra bởi [Scholkopf et al, 2012], là nếu quá trình sinh mẫu thực sự có <strong>x</strong> là một kết quả, và <strong>y</strong> là một nguyên nhân, thì mô hình <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-656"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-657">p</span><span class="MJXp-mo" id="MJXp-Span-658" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-659">x</span><span class="MJXp-mspace" id="MJXp-Span-660" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-661"><span class="MJXp-mo" id="MJXp-Span-662" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-663" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-664">y</span><span class="MJXp-mo" id="MJXp-Span-665" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-125-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-125">p(\text x\,|\, \text y)</script></span> mạnh mẽ với những thay đổi trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-666"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667">p</span><span class="MJXp-mo" id="MJXp-Span-668" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-669">y</span><span class="MJXp-mo" id="MJXp-Span-670" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-126-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-126">p(\text y)</script></span>. Nếu mối quan hệ nguyên nhân - kết quả bị đảo ngược, điều này sẽ không đúng, do theo quy tắc Bayes, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-671"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-672">p</span><span class="MJXp-mo" id="MJXp-Span-673" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-674">x</span><span class="MJXp-mspace" id="MJXp-Span-675" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-676"><span class="MJXp-mo" id="MJXp-Span-677" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-678" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mtext" id="MJXp-Span-679">y</span><span class="MJXp-mo" id="MJXp-Span-680" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-127-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-127">p(\text x\,|\, \text y)</script></span> sẽ nhạy cảm với những thay đổi trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-681"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-682">p</span><span class="MJXp-mo" id="MJXp-Span-683" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mtext" id="MJXp-Span-684">y</span><span class="MJXp-mo" id="MJXp-Span-685" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-128-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-128">p(\text y)</script></span>. Thông thường, khi chúng ta xem xét những thay đổi trong phân phối do các miền khác nhau, sự không ổn định thời gian, hoặc thay đổi bản chất của tác vụ, <em>các cơ chế nhân quả vẫn bất biến</em> (the causal mechanisms remain invariant) (“luật của vũ trụ là hằng số”), trong khi phân phối biên trên nguyên nhân cơ bản có thể thay đổi. Do đó, tổng quát hơn và mạnh mẽ hơn cho tất cả các loại thay đổi có thể được mong đợi thông qua việc học một mô hình sinh mẫu cố gắng phục hồi các yếu tố nhân quả <strong>h</strong> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-686"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-687">p</span><span class="MJXp-mo" id="MJXp-Span-688" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-689">x</span><span class="MJXp-mspace" id="MJXp-Span-690" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mrow" id="MJXp-Span-691"><span class="MJXp-mo" id="MJXp-Span-692" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mspace" id="MJXp-Span-693" style="width: 0.167em; height: 0em;"></span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-694">h</span><span class="MJXp-mo" id="MJXp-Span-695" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-129-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-129">p(\boldsymbol x\,|\, \boldsymbol h)</script></span>.</p><h1 id="154-Biểu-diễn-phân-phối-Distributed-Representation"><a class="anchor hidden-xs" href="#154-Biểu-diễn-phân-phối-Distributed-Representation" title="154-Biểu-diễn-phân-phối-Distributed-Representation"><span class="octicon octicon-link"></span></a>15.4 Biểu diễn phân phối (Distributed Representation)</h1><p>Những biểu diễn phân phối của những <em>biểu diễn khái niệm</em> (concepts-representation) bao gồm nhiều phần tử có thể thiết lập riêng biệt với nhau là một trong những công cụ quan trọng nhất đối với học biểu diễn. Những biểu diễn phân phối rất mạnh vì chúng có thể sử dụng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-696"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-697">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-130-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-130">n</script></span> đặc trưng với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-698"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-699">k</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-131-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-131">k</script></span> giá trị để mô tả <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-700"><span class="MJXp-msubsup" id="MJXp-Span-701"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-702" style="margin-right: 0.05em;">k</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-703" style="vertical-align: 0.5em;">n</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-132-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-132">k^n</script></span> khái niệm khác nhau. Như chúng ta đã thấy trong suốt cuốn sách này, mạng neuron với nhiều đơn vị ẩn và mô hình xác suất với nhiều biến ẩn đều sử dụng chiến lược của biểu diễn phân phối. Bây giờ, chúng tôi giới thiệu một quan sát bổ sung. Nhiều thuật toán học sâu được thúc đẩy bởi giả thuyết rằng các đơn vị ẩn có thể học cách biểu diễn những yếu tố nhân quả cơ bản giải thích dữ liệu, như đã thảo luận trong phần 15.3. Những biểu diễn phân phối là tự nhiên cho cách tiếp cận này, vì mỗi hướng trong không gian biểu diễn có thể tương ứng với giá trị của một biến cấu hình cơ bản khác nhau.<br>
Một ví dụ của biểu diễn phân phối là một vector của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-704"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-705">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-133-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-133">n</script></span> đặc trưng nhị phân, có thể lấy <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-706"><span class="MJXp-msubsup" id="MJXp-Span-707"><span class="MJXp-mn" id="MJXp-Span-708" style="margin-right: 0.05em;">2</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-709" style="vertical-align: 0.5em;">n</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-134-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-134">2^n</script></span> cấu hình, mỗi khả năng tương ứng với một cùng khác nhau trong không gian đầu vào, như minh họa trong hình 15.7. Điều này có thể so sánh với một biểu diễn <em>tượng trưng</em> (symbolic representation), trong đó đầu vào được liên kết với một biểu tượng hoặc thể loại. Nếu có <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-710"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-711">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-135-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-135">n</script></span> biểu tượng trong từ điển, người ta có thể liên tưởng đến <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-712"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-713">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-136-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-136">n</script></span> <em>bộ dò đặc trưng</em> (feature detector), mỗi bộ dò tương ứng với việc phát hiện sự hiện diện của một thể loại đặc trưng. Trong trường hợp đó, chỉ có thể có <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-714"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-715">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-137-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-137">n</script></span> cấu hình khác nhau của không gian biểu diễn, tương ứng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-716"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-717">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-138-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-138">n</script></span> vùng khác nhau trong không gian đầu vào, như minh họa trong hình 15.8. Biểu diễn tượng trưng như vậy cũng được gọi là <em>biểu diễn đơn trội</em> (one-hot representation), vì nó có thể nắm bắt bởi một vector nhị phân với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-718"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-719">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-139-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-139">n</script></span> bits loại trừ lẫn nhau (chỉ một trong số chúng có thể hoạt động). Một biểu diễn tượng trưng là một ví dụ cụ thể của lớp khái quát của biểu diễn không phân phối, là những biểu diễn có nhiều mục nhập nhưng không có sự kiểm soát riêng biệt có ý nghĩa đối với mỗi mục nhập.</p><p><img src="https://i.imgur.com/rDfkihX.png" alt=""></p><blockquote>
<p>Hình 15.7: Minh họa về cách một thuật toán học dựa trên biểu diễn phân phối chia nhỏ không gian đầu vào thành các vùng. Trong ví dụ này, có ba đặc trưng nhị phân <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-720"><span class="MJXp-msubsup" id="MJXp-Span-721"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-722" style="margin-right: 0.05em;">h</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-723" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-140-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-140">h_1</script></span>, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-724"><span class="MJXp-msubsup" id="MJXp-Span-725"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-726" style="margin-right: 0.05em;">h</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-727" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-141-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-141">h_2</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-728"><span class="MJXp-msubsup" id="MJXp-Span-729"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-730" style="margin-right: 0.05em;">h</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-731" style="vertical-align: -0.4em;">3</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-142-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-142">h_3</script></span>. Mỗi đặc trưng được định nghĩa bằng cách lấy ngưỡng của đầu ra của phép biến đổi tuyến tính đã học. Mỗi đặc trưng chia <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-732"><span class="MJXp-msubsup" id="MJXp-Span-733"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-734" style="margin-right: 0.05em;">R</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-735" style="vertical-align: 0.5em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-143-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-143">R^2</script></span> thành hai <em>nửa mặt phẳng</em> (half-planes). Gọi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-736"><span class="MJXp-msubsup" id="MJXp-Span-737"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-738" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-740">+</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-739">i</span></span></span></span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-144-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-144">h^+_i</script></span> là tập các điểm đầu vào mà <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-741"><span class="MJXp-msubsup" id="MJXp-Span-742"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-743" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-744" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-745" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-746">1</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-145-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-145">h_i = 1</script></span>, và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-747"><span class="MJXp-msubsup" id="MJXp-Span-748"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-749" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-751">−</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-750">i</span></span></span></span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-146-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-146">h^-_i</script></span> là một các điểm đầu vào mà <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-752"><span class="MJXp-msubsup" id="MJXp-Span-753"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-754" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-755" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-756" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-757">0</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-147-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-147">h_i = 0</script></span>. Trong hình minh hoạ này, mỗi dòng biểu diễn một biên quyết định cho một <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-758"><span class="MJXp-msubsup" id="MJXp-Span-759"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-760" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-761" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-148-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-148">h_i</script></span>, với mũi tên tương ứng trỏ đến phía <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-762"><span class="MJXp-msubsup" id="MJXp-Span-763"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-764" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-766">+</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-765">i</span></span></span></span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-149-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-149">h^+_i</script></span> của biên. Các đại diện như một tổng thể mang một giá trị duy nhất tại mỗi giao điểm có thể có của các nửa mặt phẳng này. Ví dụ, giá trị biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-767"><span class="MJXp-mo" id="MJXp-Span-768" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mn" id="MJXp-Span-769">1</span><span class="MJXp-mo" id="MJXp-Span-770" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-771">1</span><span class="MJXp-mo" id="MJXp-Span-772" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mn" id="MJXp-Span-773">1</span><span class="MJXp-msubsup" id="MJXp-Span-774"><span class="MJXp-mo" id="MJXp-Span-775" style="margin-left: 0em; margin-right: 0.05em;">]</span><span class="MJXp-mi MJXp-script" id="MJXp-Span-776" style="vertical-align: 0.5em;">⊤</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-150-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-150">[1,1,1]^\top</script></span> tương ứng với vùng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-777"><span class="MJXp-msubsup" id="MJXp-Span-778"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-779" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-781">+</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mn" id="MJXp-Span-780">1</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-782" style="margin-left: 0.267em; margin-right: 0.267em;">∩</span><span class="MJXp-msubsup" id="MJXp-Span-783"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-784" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-786">+</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mn" id="MJXp-Span-785">2</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-787" style="margin-left: 0.267em; margin-right: 0.267em;">∩</span><span class="MJXp-msubsup" id="MJXp-Span-788"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-789" style="margin-right: 0.05em;">h</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class="MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mo" id="MJXp-Span-791">+</span></span></span></span><span class="MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mn" id="MJXp-Span-790">3</span></span></span></span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-151-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-151">h^+_1 \cap h^+_2 \cap h^+_3</script></span>. So sánh điều này với các biểu diễn không phân phối trong hình 15.8. Trong trường hợp thông thường của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-792"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-793">d</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-152-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-152">d</script></span> chiều của đầu vào, mỗi biểu diễn phân phối chia <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-794"><span class="MJXp-msubsup" id="MJXp-Span-795"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-796" style="margin-right: 0.05em;">R</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-797" style="vertical-align: 0.5em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-153-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-153">R^d</script></span> bằng cách giao nhau một nửa không gian thay vì một nửa mặt phẳng. Biểu diễn phân phối với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-798"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-799">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-154-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-154">n</script></span> đặc trưng gán các mã duy nhất cho <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-800"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-801">O</span><span class="MJXp-mo" id="MJXp-Span-802" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-803"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-804" style="margin-right: 0.05em;">n</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-805" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-806" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-155-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-155">O(n^d)</script></span> vùng khác nhau, trong khi thuật toán lân cận gần nhất với các ví dụ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-807"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-808">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-156-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-156">n</script></span> gán các mã duy nhất cho chỉ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-809"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-810">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-157-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-157">n</script></span> vùng. Do đó, so với biểu diễn không phân phối, biểu diễn phân phối có thể biểu biểu diễn nhiều vùng hơn theo cấp số nhân. Hãy nhớ rằng, không phải tất cả các giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-811"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-812">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-158-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-158">\boldsymbol h</script></span> đều khả thi (không có <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-813"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-814">h</span><span class="MJXp-mo" id="MJXp-Span-815" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-816">0</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-159-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-159">\boldsymbol h = 0</script></span> trong ví dụ này), và một bộ phân loại tuyến tính ở trên đỉnh của biểu diễn phân phối không thể gán các lớp định danh khác nhau cho mọi vùng lân cận; thậm chí một <em>mạng ngưỡng tuyến tính sâu</em> (deep linear-threshold network) có VC chiều của chỉ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-817"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-818">O</span><span class="MJXp-mo" id="MJXp-Span-819" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-820">w</span><span class="MJXp-mi" id="MJXp-Span-821">log</span><span class="MJXp-mo" id="MJXp-Span-822" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-823">w</span><span class="MJXp-mo" id="MJXp-Span-824" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-160-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-160">O(w \log w)</script></span>, trong đó <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-825"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-826">w</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-161-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-161">w</script></span> là số lượng trọng số [Sontag, 1998]. Sự kết hợp của một lớp biểu diễn mạnh với một bộ phân loại yếu có thể là một cơ chế kiểm soát mạnh. Ví dụ, một bộ phân loại cố gắng để học khái niệm “người” so với “không phải người” thì nó không cần phải gán một lớp khác cho biểu diễn đầu vào như “người phụ nữ đeo kính” hơn là “người đàn ông đeo kính”. Ràng buộc khả năng khuyến khích mỗi bộ phân loại tập trung vào vài <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-827"><span class="MJXp-msubsup" id="MJXp-Span-828"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-829" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-830" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-162-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-162">h_i</script></span> và khuyến khích <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-831"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-832">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-163-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-163">\boldsymbol h</script></span> học cách biểu diễn các lớp theo một cách phân biệt tuyến tính.</p>
</blockquote><p><img src="https://i.imgur.com/jRM00QI.png" alt=""></p><blockquote>
<p>Hình 15.8: Minh họa cách thuật toán lân cận gần nhất phá vỡ không gian đầu vào thành các vùng khác nhau. Thuật toán lân cận gần nhất cung cấp một ví dụ về thuật toán học dựa trên biểu diễn không phân phối. Các thuật toán không phân phối riêng có thể có dạng hình học khác nhau, nhưng chúng thường phá vỡ không gian đầu vào thành các vùng với một bộ tham số riêng biệt cho mỗi vùng. Ưu điểm của cách tiếp cận không phân phối là, với đủ các tham số, nó không thể huấn luyện mà không giải quyết thuật toán tối ưu hóa khó, bởi vì rất đơn giản để chọn đầu ra riêng biệt độc lập cho từng vùng. Điểm bất lợi là các mô hình không phân phối như vậy chỉ khái quát hóa cục bộ thông qua độ mịn trước, làm cho nó trở nên phức tạp để tìm hiểu một hàm phức tạp với nhiều đỉnh và chỗ lõm hơn số ví dụ có sẵn. Tương phản điều này với một biểu diễn phân phối, hình 15.7.</p>
</blockquote><p>Các ví dụ sau về thuật toán học dựa trên biểu diễn không phân phối:</p><ul>
<li>Cây quyết định: Chỉ một lá (và các nút trên đường dẫn từ gốc đến lá) được kích hoạt khi một đầu vào được đưa ra.</li>
<li>Hỗn hợp Gauss và hỗn hợp của các chuyên gia: các mẫu (trung tâm cụm) hoặc các chuyên gia hiện được kết hợp với một mức độ kích hoạt. Giống như thuật toán lân cận gần nhất, mỗi đầu vào được biểu diễn bằng nhiều giá trị, nhưng những giá trị đó không thể dễ dàng được kiểm soát riêng biệt với nhau.</li>
<li>Mô hình hàm lõi với hàm lõi Gauss (hoặc một hàm lõi cục bộ khác tương tự): mặc dù mức độ kích hoạt của mỗi “vector hỗ trợ” hoặc ví dụ mẫu hiện nay có giá trị liên tục, cùng một vấn đề phát sinh như với hỗn hợp Gauss.</li>
<li>Các mô hình ngôn ngữ hoặc dịch dựa trên n-grams: tập hợp các bối cảnh (chuỗi các ký hiệu) được phân đoạn theo cấu trúc cây của các giá trị. Ví dụ: một chiếc lá có thể tương ứng với hai từ cuối cùng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-833"><span class="MJXp-msubsup" id="MJXp-Span-834"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-835" style="margin-right: 0.05em;">w</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-836" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-164-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-164">w_1</script></span> và <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-837"><span class="MJXp-msubsup" id="MJXp-Span-838"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-839" style="margin-right: 0.05em;">w</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-840" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-165-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-165">w_2</script></span>. Các thông số riêng biệt được ước tính cho mỗi lá của cây (với một số chia sẻ có thể).</li>
</ul><p>Đối với một số thuật toán không phân phối này, đầu ra không phải là hằng số theo các phần mà thay vào đó là nội suy giữa các vùng lân cận. Mối quan hệ giữa số lượng tham số (hoặc ví dụ) và số vùng mà chúng có thể xác định là tuyến tính.</p><p>Một khái niệm quan trọng liên quan đến việc phân biệt một biểu diễn phân phối từ một biểu tượng là tổng quát phát sinh do các thuộc tính được chia sẻ giữa các khái niệm khác nhau. Như các biểu tượng thuần túy, “mèo” và “chó” là xa nhau như bất kỳ hai biểu tượng khác. Tuy nhiên, nếu liên kết chúng với một đại diện phân phối có ý nghĩa, thì nhiều điều có thể nói về mèo có thể khái quát hóa cho chó và ngược lại. Ví dụ, đại diện phân phối của ta có thể chứa các mục như “có_lông” hoặc “số_ chân” có cùng giá trị cho việc nhúng cả “mèo” và “chó”. Mô hình ngôn ngữ neuron hoạt động trên các biểu diễn được phân phối của các từ tổng quát tốt hơn nhiều các mô hình khác hoạt động trực tiếp trên các biểu diễn đơn trội của các từ, như được thảo luận trong phần 12.4. Biểu diễn phân phối tạo ra một không gian tương tự phong phú, trong đó khái niệm gần đúng ngữ nghĩa (hoặc đầu vào) nằm gần nhau, một thuộc tính vắng mặt từ các biểu diễn thuần túy tượng trưng.</p><p>Khi nào và tại sao có thể có một lợi thế thống kê từ việc sử dụng biểu diễn phân phối như là một phần của thuật toán học tập? Các biểu diễn phân phối có thể có một lợi thế thống kê khi một cấu trúc rõ ràng phức tạp có thể được biểu diễn nhỏ gọn bằng cách sử dụng một số lượng nhỏ các tham số. Một số thuật toán học không phân phối truyền thống tổng quát chỉ do giả thiết dễ dàng, trong đó nói rằng nếu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-841"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-842">u</span><span class="MJXp-mo" id="MJXp-Span-843" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-844">v</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-166-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-166">u \approx v</script></span>, thì hàm mục tiêu <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-845"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-846">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-167-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-167">f</script></span> được học có thuộc tính <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-847"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-848">f</span><span class="MJXp-mo" id="MJXp-Span-849" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-850">u</span><span class="MJXp-mo" id="MJXp-Span-851" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-852" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-853">f</span><span class="MJXp-mo" id="MJXp-Span-854" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-855">v</span><span class="MJXp-mo" id="MJXp-Span-856" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-168-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-168">f (u) \approx f (v)</script></span> nói chung. Có rất nhiều cách để chính thức hóa một giả định như vậy, nhưng kết quả cuối cùng là nếu ta có một ví dụ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-857"><span class="MJXp-mo" id="MJXp-Span-858" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-859">x</span><span class="MJXp-mo" id="MJXp-Span-860" style="margin-left: 0em; margin-right: 0.222em;">,</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-861">y</span><span class="MJXp-mo" id="MJXp-Span-862" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-169-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-169">(x,y)</script></span> mà ta biết rằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-863"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-864">f</span><span class="MJXp-mo" id="MJXp-Span-865" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-866">x</span><span class="MJXp-mo" id="MJXp-Span-867" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-868" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-869">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-170-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-170">f(x) \approx y</script></span>, sau đó ta chọn một công cụ ước tính <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-870"><span class="MJXp-mrow" id="MJXp-Span-871"><span class="MJXp-munderover" id="MJXp-Span-872"><span><span class="MJXp-over"><span style="margin-bottom: -0.92em;"><span class="MJXp-mo" id="MJXp-Span-874" style="margin-left: 0px; margin-right: 0px;">ˆ</span></span><span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-873">f</span></span></span></span></span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-171-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-171">\hat{f}</script></span> mà nó xấp xỉ các ràng buộc này khi có thay đổi càng ít càng tốt khi ta di chuyển đến một đầu vào lân cận <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-875"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-876">x</span><span class="MJXp-mo" id="MJXp-Span-877" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-878">ϵ</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-172-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-172">x + \epsilon</script></span>. Giả thiết này rõ ràng hữu ích nhưng nó bị vấn đề <em>hiểm họa số chiều lớn</em> (the curse of dimensionality): để học một hàm mục tiêu tăng và giảm nhiều lần ở nhiều vùng khác nhau <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-879"><span class="MJXp-msubsup" id="MJXp-Span-880"><span class="MJXp-mi" id="MJXp-Span-881" style="margin-right: 0.05em;"></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-882" style="vertical-align: 0.5em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-173-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-173">^1</script></span>, chúng ta có thể cần một số ví dụ mà ít nhất là nó đủ lớn như số lượng các vùng phân biệt. Người ta có thể nghĩ về từng vùng như là một loại hoặc biểu tượng: bằng cách có một mức độ tự do riêng biệt cho mỗi biểu tượng (hoặc vùng), chúng ta có thể tìm hiểu một bộ giải mã tùy ý ánh xạ từ ký hiệu đến giá trị. Tuy nhiên, điều này không cho phép ta khái quát hóa các biểu tượng mới cho các vùng mới.</p><blockquote>
<p><span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-883"><span class="MJXp-msubsup" id="MJXp-Span-884"><span class="MJXp-mi" id="MJXp-Span-885" style="margin-right: 0.05em;"></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-886" style="vertical-align: 0.5em;">1</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-174-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-174">^1</script></span> Có khả năng, ta muốn học một hàm mà hành vi khác biệt theo cấp số nhân trong nhiều vùng: trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-887"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-888">d</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-175-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-175">d</script></span> chiều với ít nhất 2 giá trị khác nhau để phân biệt mỗi chiều, ta có thể muốn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-889"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-890">f</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-176-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-176">f</script></span> khác biệt trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-891"><span class="MJXp-msubsup" id="MJXp-Span-892"><span class="MJXp-mn" id="MJXp-Span-893" style="margin-right: 0.05em;">2</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-894" style="vertical-align: 0.5em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-177-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-177">2^d</script></span> vùng khác nhau, đòi hỏi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-895"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-896">O</span><span class="MJXp-mo" id="MJXp-Span-897" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-898"><span class="MJXp-mn" id="MJXp-Span-899" style="margin-right: 0.05em;">2</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-900" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-901" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-178-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-178">O(2^d)</script></span> ví dụ huấn luyện.</p>
</blockquote><p>Nếu ta may mắn, có thể có một số tính quy tắc trong hàm mục tiêu, ngoài việc được làm mịn. Ví dụ, một mạng tích chập với phép gộp cực đại có thể nhận ra một đối tượng bất kể vị trí của nó trong hình ảnh, mặc dù bản dịch không gian của đối tượng có thể không tương ứng với biến đổi bằng phẳng trong không gian đầu vào.</p><p>Ta hãy xem xét một trường hợp đặc biệt của thuật toán học tập biểu diễn phân phối, trích xuất các đặc trưng nhị phân bằng cách làm tròn các hàm tuyến tính của đầu vào. Mỗi đặc trưng nhị phân trong biểu diễn này chia <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-902"><span class="MJXp-msubsup" id="MJXp-Span-903"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-904" style="margin-right: 0.05em;">R</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-905" style="vertical-align: 0.5em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-179-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-179">R^d</script></span> thành một cặp của <em>nửa không gian</em>(half-spaces), như mình họa trong hình 15.7.  Số lượng lớn các điểm giao nhau của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-906"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-907">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-180-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-180">n</script></span> của nửa không gian tương ứng xác định có bao nhiêu vùng mà người học biểu diễn phân phối này có thể phân biệt. Có bao nhiêu vùng được tạo ra bởi một sự sắp xếp của n <em>siêu phẳng</em> (hyperplanes) trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-908"><span class="MJXp-msubsup" id="MJXp-Span-909"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-910" style="margin-right: 0.05em;">R</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-911" style="vertical-align: 0.5em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-181-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-181">R^d</script></span>? Bằng cách áp dụng một kết quả chung liên quan đến giao điểm của siêu phẳng [Zaslavsky, 1975], người ta có thể hiển thị [Pascanu et al, 2014b] rằng số vùng mà biểu diễn đặc trưng nhị phân này có thể phân biệt là<br>
<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-912"><span class="MJXp-mtable" id="MJXp-Span-913"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-914" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-915" style="text-align: center;"><span class="MJXp-munderover" id="MJXp-Span-916"><span><span class="MJXp-over"><span class="MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-922" style="margin-right: 0px; margin-left: 0px;">d</span></span><span><span class="MJXp-mo" id="MJXp-Span-917" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop">∑</span></span></span></span></span><span class="MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-918" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-919">j</span><span class="MJXp-mo" id="MJXp-Span-920">=</span><span class="MJXp-mn" id="MJXp-Span-921">0</span></span></span></span><span class="MJXp-mrow" id="MJXp-Span-923"><span class="MJXp-mo" id="MJXp-Span-924" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.528em;"><span class="MJXp-right MJXp-scale5" style="font-size: 3.111em; margin-left: -0.13em;">(</span></span><span class="MJXp-mtable" id="MJXp-Span-925"><span><span class="MJXp-mtr" id="MJXp-Span-926" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-927" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-928">n</span></span></span><span class="MJXp-mtr" id="MJXp-Span-929" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-930" style="padding-top: 0.4em; text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-931">j</span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-932" style="margin-left: 0em; margin-right: 0em; vertical-align: -0.528em;"><span class="MJXp-right MJXp-scale5" style="font-size: 3.111em; margin-left: -0.13em;">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-933" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-934">O</span><span class="MJXp-mo" id="MJXp-Span-935" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-936"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-937" style="margin-right: 0.05em;">n</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-938" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-939" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-940" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span></span></span></span></span><div class="MathJax_Display MathJax_Processing"><span class="MathJax" id="MathJax-Element-182-Frame" tabindex="0"></span></div><script type="math/tex; mode=display" id="MathJax-Element-182"> \sum^d_{j=0} \left( \begin{matrix} n \\ j\end{matrix}  \right )= O(n^d).
\tag{15.4}</script></span><br>
Vì vậy, chúng ta thấy một sự tăng trưởng là theo cấp số mũ trong kích thước đầu vào và đa thức trong số lượng các đơn vị ẩn.</p><p>— Trang Le kết thúc phần dịch tại đây(trang548)</p><p>-------------------------------------------------------------------------<br>
------------ Thomas Nguyen bắt đầu dich từ đây (trang 549 đến 554)</p><p>Phương pháp này cung cấp một lập luận về mặt hình học để giải thích về tính tổng quát của phương pháp biểu diễn phân tán: với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-941"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-942">O</span><span class="MJXp-mo" id="MJXp-Span-943" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-944">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-945">d</span><span class="MJXp-mo" id="MJXp-Span-946" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-183-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-183">O(nd)</script></span> tham số (<span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-947"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-948">n</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-184-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-184">n</script></span> là số đặc trưng ngưỡng tuyến tính trong không gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-949"><span class="MJXp-msubsup" id="MJXp-Span-950"><span class="MJXp-mrow" id="MJXp-Span-951" style="margin-right: 0.05em;"><span class="MJXp-mi undefined" id="MJXp-Span-952">R</span></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-953" style="vertical-align: -0.4em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-185-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-185">\mathbb{R}_d</script></span>), chúng ta có thể biểu diễn phạm vi của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-954"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-955">O</span><span class="MJXp-mo" id="MJXp-Span-956" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-957"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-958" style="margin-right: 0.05em;">n</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-959" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-960" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-186-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-186">O(n^d)</script></span> trong không gian đầu vào. Nếu ta không đưa ra các giả sử về dữ liệu và chỉ biểu diễn dữ liệu bởi một ký tự duy nhất cho mỗi vùng, chia tách các tham số theo mỗi ký tự để nhận biết nó tương ứng với các vùng trong không gian nào trong <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-961"><span class="MJXp-msubsup" id="MJXp-Span-962"><span class="MJXp-mrow" id="MJXp-Span-963" style="margin-right: 0.05em;"><span class="MJXp-mi undefined" id="MJXp-Span-964">R</span></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-965" style="vertical-align: 0.5em;">d</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-187-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-187">\mathbb{R}^d</script></span>, thì một vùng cụ thể <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-966"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-967">O</span><span class="MJXp-mo" id="MJXp-Span-968" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-969"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-970" style="margin-right: 0.05em;">n</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-971" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-972" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-188-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-188">O(n^d)</script></span> sẽ cần <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-973"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-974">O</span><span class="MJXp-mo" id="MJXp-Span-975" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-976"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-977" style="margin-right: 0.05em;">n</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-978" style="vertical-align: 0.5em;">d</span></span><span class="MJXp-mo" id="MJXp-Span-979" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-189-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-189">O(n^d)</script></span> mẫu. Tổng quát hơn, các lập luận ủng hộ việc biểu diễn phân tán có thể được mở rộng trong trường hợp mà thay vì sử dụng các unit tuyến tính, ta sử dụng bộ trích xuất đặc trưng không tuyến tính, có tính liên tục cho mỗi thuộc tính của phép biểu diễn phân tán. Các lập luận trong trường hợp này, nếu một phép biến đổi với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-980"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-981">k</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-190-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-190">k</script></span> tham số có thể học về <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-982"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-983">r</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-191-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-191">r</script></span> vùng trong không gian đầu vào, với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-984"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-985">k</span><span class="MJXp-mo" id="MJXp-Span-986" style="margin-left: 0.333em; margin-right: 0.333em;">≪</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-987">r</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-192-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-192">k \ll r</script></span>, thì ta có thể tổng quát hóa theo cách này tốt hơn nhiều so với thiết lập không phân tán mà chúng ta cần <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-988"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-989">O</span><span class="MJXp-mo" id="MJXp-Span-990" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-991">r</span><span class="MJXp-mo" id="MJXp-Span-992" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-193-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-193">O(r)</script></span> mẫu để thu được các đặc trung giống nhau và các phần kết hợp trong không gian đầu vào với <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-993"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-994">r</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-194-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-194">r</script></span> vùng. Sử dụng ít tham số hơn để biểu diễn có nghĩa là ta có ít tham số hơn để khớp hơn, do vậy cần số mẫu huấn luyện ít hơn nhiều để đạt được kết quả tổng quát hóa tốt.</p><p>Một phần nữa của lập luận là tại sao các mô hình được xây dựng trên các đại diện phân tán có tính tổng quát hóa tốt, đó là khả năng của chúng vẫn còn hạn chế mặc dù chúng có thể mã hóa rõ ràng các vùng khác nhau. Ví dụ, các chiều VC của một mạng neuron có unit ngưỡng tuyến tính chỉ là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-995"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-996">O</span><span class="MJXp-mo" id="MJXp-Span-997" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-998">w</span><span class="MJXp-mtext" id="MJXp-Span-999">&nbsp;</span><span class="MJXp-mrow" id="MJXp-Span-1000"><span class="MJXp-mi undefined" id="MJXp-Span-1001">l</span><span class="MJXp-mi undefined" id="MJXp-Span-1002">o</span><span class="MJXp-mi undefined" id="MJXp-Span-1003">g</span></span><span class="MJXp-mtext" id="MJXp-Span-1004">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1005">w</span><span class="MJXp-mo" id="MJXp-Span-1006" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-195-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-195">O(w\space\mathbb{log}\space w)</script></span>, ở đây <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1007"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1008">w</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-196-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-196">w</script></span> là số lượng các trọng số. Giới hạn này phát sinh bởi khi ta ấn định nhiều mã cho không gian biểu diễn, ta không thể sử dụng toàn bộ tất cả các không gian mã hóa, cũng như không thể học cách lập đồ thị hàm tùy ý từ không gian biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1009"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1010">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-197-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-197">\boldsymbol{h}</script></span> đến đầu ra <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1011"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1012">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-198-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-198">\boldsymbol{y}</script></span> sử dụng bộ phân lớp tuyến tính. Sử dụng biểu diễn phân tán kết hợp với một bộ phân lớp tuyến tính biểu diễn một tiên nghiệm mà các lớp được nhận dạng là tách biệt tuyến tính như một hàm các yếu tố kết quả căn bản cho bởi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1013"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1014">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-199-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-199">\boldsymbol{h}</script></span>. Chúng ta thông thường muốn học các danh mục như một bộ ảnh dữ liệu về các vật thể màu xanh lá cây hoặc một bộ ảnh dữ liệu về các loại xe nhưng không phải các danh mục yêu cầu logic XOR phi tuyến. Ví dụ, ta thường không muốn phân chia dữ liệu xe màu đỏ và xe tải màu xanh vào cùng một lớp và xe màu xanh, xe tải màu đỏ vào một lớp khác.</p><p>Ý tưởng đã thảo luận ở trên khá là trừu tượng, tuy nhiên chúng có thể xác thực bằng thực nghiệm. Zhou et al. (2015) đã tìm ra rằng các unit lớp ẩn trong mạng tích chập đã huấn luyện với dữ liệu tiêu chuẩn ImageNet và Place sẽ học các đặc trưng có thể hiểu được, tương ứng với các nhãn mà con người ấn định một cách tự nhiên. Trong thực tế thì không phải luôn có các trường hợp mà các unit lớp ẩn học một thứ gì đó có tên về mặt ngữ pháp đơn giản, mà thú vị ở chỗ sự phát triển này gần với mức độ cao nhất của các mạng học sâu tốt nhất áp dụng trong thị giác máy tính. Những đặc trưng như vậy có điểm chung là chúng có thể học từ một trong số chúng mà không cần quan tâm đến cấu hình của toàn bộ các đặc trưng khác. Radford et al. (2015) đã giải thích rằng một mô hình sinh mẫu có thể học có thể học cách biểu diễn mặt người dưới dạng ảnh, với các hướng tách biệt trong không gian biểu diễn chụp dưới các điều kiện cơ bản khác nhau. Hình 15.9 đưa ra giải thích về một hướng trong không gian biểu diễn tương ứng với giới tính của một người, trong khi hướng khác lại tương ứng với việc người đó có đeo kính hay không.</p><p><img src="https://i.imgur.com/b64lFGi.png" alt=""></p><blockquote>
<p>Hình 15.9: Ví dụ về một mô hình sinh mẫu đã học cách biểu diễn phân tán về khái niệm giới tính của người từ khái niệm về việc đeo kính hay không. Nếu ta bắt đầu với việc biểu diễn khái niệm về người đàn ông đeo kính, sau đó trừ đi vector biểu diễn người đàn ông không đeo kính, sau đó cộng với vector biểu diễn một người phụ nữa không đeo kính, ta sẽ nhận được một vector biểu diễn người phụ nữa đeo kính. Mô hình sinh mẫu đã mã hóa chính xác về việc biểu diễn vector dưới dạng ảnh mô tả việc đeo kính hay không.</p>
</blockquote><p>Các đặc trưng được tìm một cách tự động chứ không dựa trên việc cố định một tiên nghiệm. Do đó không cần phải gắn nhãn cho các bộ phân lớp unit ẩn: thuật toán giảm theo độ dốc trên một hàm đối tượng sẽ học một cách tự nhiên các đặc trưng cần thiết, hoặc các vấn đề yêu cầu các đặc trưng như vậy. Chúng có thể tự học về sự khác nhau về giới tính con người hoặc các đặc trưng biểu diễn về việc đeo kính hay không, mà không cần phải đặc trưng hóa các tham số biểu diễn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1015"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1016">n</span><span class="MJXp-mo" id="MJXp-Span-1017" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mn" id="MJXp-Span-1018">1</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-200-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-200">n-1</script></span> các đặc trưng còn lại. Đây là một dạng của sự chia tách dựa trên thống kê cho phép ta có thể tổng quát một tham số mới cho các đặc trưng của một người mà chưa từng xuất hiện trong tập huấn luyện.</p><h1 id="155-Mức-độ-cải-thiện-theo-hàm-mũ-thu-được-từ-độ-sâu-của-mô-hình"><a class="anchor hidden-xs" href="#155-Mức-độ-cải-thiện-theo-hàm-mũ-thu-được-từ-độ-sâu-của-mô-hình" title="155-Mức-độ-cải-thiện-theo-hàm-mũ-thu-được-từ-độ-sâu-của-mô-hình"><span class="octicon octicon-link"></span></a>15.5 Mức độ cải thiện theo hàm mũ thu được từ độ sâu của mô hình</h1><p>Từ mục 6.4.1, một perception đa lớp là một mô hình xấp xỉ phổ quát, và các đặc trưng đó có thể được biểu diễn bằng các mạng học sâu nhỏ hơn theo dạng lũy thừa khi so sánh với các mô hình mạng không sâu (shallow network). Việc giảm về kích thước đã dẫn đến việc cải thiện tính hiệu quả về mặt thống kê. Trong phần này, ta sẽ đi tìm hiểu <em>bằng cách nào</em> các kết quả giống nhau được áp dụng  chung cho các mô hình khác nhau với phương pháp biểu diễn lớp ẩn phân tán.</p><p>Trong mục 15.4, ta đã xét một ví dụ về một mô hình sinh mẫu học về các yếu tố cơ bản về mặt một người dựa trên ảnh, bao gồm giới tính của người đó và xét xem liệu họ có đeo kính hay không. Mô hình sinh mẫu trong ví dụ này dựa trên một mô hình mạng nơ ron sâu. Sẽ là không hợp lý nếu ta sử dụng một mô hình mạng không sâu, ví dụ như một mạng tuyến tính, để học về các mối quan hệ phức tạp giữa các yếu tố mang tính trừu tượng với các pixel trên ảnh. Trong ví dụ này và trong các bài toán về AI khác, các yếu tố được lựa chọn sẽ độc lập với nhau nhưng chúng vẫn tương ứng với các đầu vào ở mức cao và liên quan một cách phi tuyến với đầu vào. Ta đã chỉ ra điều này yêu cầu việc biểu diễn phân tán có chiều sâu mà các đặc trưng ở mức cao (có thể coi như một hàm đầu vào) hoặc các yếu tố (coi như nguyên nhân sinh mẫu) thu được thông qua nhiều thành phần phi tuyến.</p><p>Điều này đã được chứng minh trong nhiều thiết lập khác nhau mà việc tính toán thông qua các thành phần phi tuyến và một phân cấp các đặc trưng có thể làm nâng cao hiệu quả thống kê theo cấp số nhân, và hiệu quả nhất khi sử dụng biểu diễn phân tán. Có rất nhiều các mô hình mạng (với các thành phần phi tuyến kết hợp, cổng dạng boolen, tổng/tích, hoặc khối RBF) với chỉ một lớp ẩn có thể được biểu diễn dưới dạng mô hình xấp xỉ phổ quát. Một họ các mô hình dạng xấp xỉ phổ quát có thể xấp xỉ số lượng lớn lớp các hàm (bao gồm tất cả các hàm liên tục) về bất kỳ mức dung sai khác không nào. Tuy nhiên thì số lượng các unit ẩn yêu cầu sẽ rất lớn. Kết quả về mặt lý thuyết về khả năng triển khai các cấu trúc chiều sâu chỉ ra rằng một họ các hàm số có thể được biểu diễn hiệu quả bởi một cấu trúc có chiều sâu là <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1019"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1020">k</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-201-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-201">k</script></span> nhưng nó sẽ yêu cầu số lượng các unit ẩn dạng cấp số nhân (với kích thước đầu vào hợp lý) với chiều sâu không xác định (chiều sâu là 2 hoặc <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1021"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1022">k</span><span class="MJXp-mo" id="MJXp-Span-1023" style="margin-left: 0.267em; margin-right: 0.267em;">−</span><span class="MJXp-mn" id="MJXp-Span-1024">1</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-202-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-202">k-1</script></span>).</p><p>Trong mục 6.4.1, chúng ta đã thấy rằng một mô hình mạng học sâu lan truyền thuận là các hàm xấp xỉ phổ quát của các hàm. Nhiều mô hình xác suất có cấu trúc với một lớp các biến tiềm ẩn, bao gồm các máy Boltzmann bị hạn chế và mạng phân phối đa tầng (deep belief networks), là các xấp xỉ phổ biến của phân phối xác suất (Le Rouxand Bengio, 2008, 2010; Montúfar và Ay, 2011; Montúfar, 2014; 2013).</p><p>Cũng trong mục 6.4.1, chúng ta thấy rằng một mô hình mạng sâu hoàn toàn có thể có lợi thế theo cấp số nhân so với một mạng không sâu. Kết quả tương tự như vậy cũng có thể đạt được với các mô hình xác suất. Một trong số các mô hình mạng như vậy là <strong>mạng tổng-tích</strong>, hoặc SPN (Poon and Domingos, 2011). Mô hình này sử dụng mạch đa thức để tính toán phân phối xác suất của các biến ngẫu nhiên. Delalleau and Bengio (2011) chỉ ra rằng tồn tại một phân phối xác suất trong đó yêu cầu độ sâu cực tiểu của mạng SPN để tránh việc sử dụng một mô hình có độ lớn dạng hàm mũ. Tiếp theo đó, Martens and Medabalimi (2014) chỉ ra rằng tồn tại sự khác nhau rõ rệt giữa bất kỳ hai giá trị chiều sâu của mô hình mạng SPN, và có một số ràng buộc được sử dụng để làm cho mạng SPN dễ vận dụng có thể sẽ giới hạn khả năng của chúng.</p><p>Một tiến bộ thú vị khác đó là một bộ các kết quả về mặt lý thuyết cho khả năng triển khai của mạch có độ sâu liên quan đến mạng tích chập (families of deep circuits related to convolutional nets), làm nổi bật lên ưu điểm của mạch có chiều sâu, ngay cả khi một mạch không sâu chỉ cho phép xấp xỉ tính toán các hàm bằng một mạch có độ sâu (Cohen et al.,2015). Bằng phép so sánh, các nghiên cứu trước đây về mặt lý thuyết đã đưa ra các tuyên bố liên quan đến trường hợp mạch không sâu phải mô phỏng chính xác các hàm số cụ thể.</p><h1 id="156-Cung-cấp-các-dẫn-chứng-để-tìm-hiểu-nguyên-nhân-sâu-hơn"><a class="anchor hidden-xs" href="#156-Cung-cấp-các-dẫn-chứng-để-tìm-hiểu-nguyên-nhân-sâu-hơn" title="156-Cung-cấp-các-dẫn-chứng-để-tìm-hiểu-nguyên-nhân-sâu-hơn"><span class="octicon octicon-link"></span></a>15.6 Cung cấp các dẫn chứng để tìm hiểu nguyên nhân sâu hơn</h1><p>Để kết thúc chương này, chúng ta sẽ lần ngược về câu hỏi phần đầu chương: điều gì làm cho một phép biểu diễn tốt hơn các phép biểu diễn khác? Đáp án được đưa ra tại mục 15.3 đó là, phép biểu diễn lý tưởng là một phép biểu diễn các yếu tố nguyên nhân cơ bản của phép biến đổi tạo ra dữ liệu, đặc biệt là các yếu tố đó liên quan đến việc áp dụng của chúng ta. Hầu hết các chiến lược để học các biểu diễn đó là dựa trên việc đưa ra các dẫn chứng giúp cho việc tìm ra những yếu tố cơ bản của các biến thể. Các dẫn chứng giúp cho bộ học chia tách các yếu tố quan sát được. Học giám sát cung cấp cho chúng ta một dẫn chứng rất chắc chắn: một nhãn <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1025"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1026">y</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-203-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-203">\boldsymbol{y}</script></span> đại diện cho mỗi giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1027"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1028">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-204-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-204">\boldsymbol{x}</script></span> sẽ xác định giá trị cụ thể của ít nhất một trong các yếu tố của các biến thể một cách trực tiếp. Tổng quát hơn, để sử dụng nhiều dữ liệu không được gán nhãn, học biểu diễn được sử dụng trong các trường hợp khác, là một gợi ý ít trực tiếp hơn về các yếu tố cơ bản. Những gợi ý này đã tạo nên khuôn dạng của các tiên nghiệm mà chúng ta, những người thiết kế các thuật toán học máy, áp dụng để hướng dẫn cho các bộ học. Kết quả giống như định lý về <em>không có bữa trưa nào miễn phí</em> (the no free lunch theorem) đã chỉ ra rằng các chiến lược chính quy hóa là cần thiết để thu được kết quả có tổng quát hóa cao. Trong khi không thể tìm ra một chiến lược chính quy hóa tốt, một trong số mục tiêu của mô hình học sâu đó là tìm ra được một bộ các chiến lược chính quy hóa chung có tính ứng dụng cho nhiều bài toán về AI, tương tự với các bài toán mà con người cũng như động vật có thể giải quyết được.</p><p>Ở đây chúng ta đưa ra một danh sách các chiến lược chính quy hóa chung. Danh sách này không phải đầy đủ tuy nhiên vẫn đưa ra được các ví dụ đúc kết được về việc làm cách nào các thuật toán học máy có thể khám phá tìm ra các đặc trưng tương ứng với các yếu tố nền tảng. Danh sách này được đưa ra tại mục 3.1 của Bengio et al. (2013d) và đã được mở rộng:</p><ul>
<li>
<p>Độ mượt (Smoothness): Đây là một giả sử rằng <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1029"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1030">f</span><span class="MJXp-mo" id="MJXp-Span-1031" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1032">x</span><span class="MJXp-mo" id="MJXp-Span-1033" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1034">ϵ</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1035">d</span><span class="MJXp-mo" id="MJXp-Span-1036" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-1037">&nbsp;</span><span class="MJXp-mo" id="MJXp-Span-1038" style="margin-left: 0.333em; margin-right: 0.333em;">≈</span><span class="MJXp-mtext" id="MJXp-Span-1039">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1040">f</span><span class="MJXp-mo" id="MJXp-Span-1041" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1042">x</span><span class="MJXp-mo" id="MJXp-Span-1043" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-205-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-205">f(\boldsymbol{x}+\epsilon \boldsymbol{d}) \space \approx \space f(\boldsymbol{x})</script></span> cho unit <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1044"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1045">d</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-206-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-206">\boldsymbol{d}</script></span> và giá trị <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1046"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1047">ϵ</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-207-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-207">\epsilon</script></span> đủ nhỏ. Giả sử này cho phép các bộ học  tổng quát hóa từ mẫu huấn luyện đến điểm gần nhất trong không gian đầu vào. Rất nhiều các thuật toán học máy đã dựa trên ý tưởng này, tuy nhiên nó không đủ để vượt qua các trở ngại về chiều dữ liệu.</p>
</li>
<li>
<p>Tính tuyến tính: Rất nhiều các thuật toán trong máy học đưa ra các giả sử về mối quan hệ giữa các biến là tuyến tính. Điều này cho phép các thuật toán có thể đưa ra các dự đoán khác xa với các dữ liệu quan sát được, tuy nhiên đôi khi nó dẫn đến các kết quả dự đoán khá cực đoan. Hầu hết các thuật toán học máy cơ bản đều không đưa ra giả thiết về độ mượt thay vì đưa ra giả thiết về tính tuyến tính. Trong thực tế có các giả sử khác nhau - hàm tuyến tính với các trọng số lớn được áp dụng với không gian nhiều chiều có thể sẽ không mượt. Tham khảo Goodfellow et al.(2014b) về các thảo luận chi tiết về giới hạn của các giả thiết tuyến tính.</p>
</li>
<li>
<p>Các yếu tố có nhiều nghĩa (Multiple explanatory factors): Rất nhiều các thuật toán học máy được thúc đẩy bởi các giả sử về dữ liệu được tạo ra từ các yếu tố nền tảng nhiều nghĩa, và hầu hết các bài toán được giải quyết dễ dàng đều đưa ra giả định về các yếu tố này. Mục 15.3 mô tả về việc làm cách nào quan điểm này thúc đẩy phương pháp học bán giám sát thông qua học biểu diễn. Học về cấu trúc của <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1048"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1049">p</span><span class="MJXp-mo" id="MJXp-Span-1050" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1051">x</span><span class="MJXp-mo" id="MJXp-Span-1052" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-208-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-208">p(\boldsymbol{x})</script></span> yêu cầu học một vài các đặc trưng giống nhau hữu ích cho việc mô hình hóa <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1053"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1054">p</span><span class="MJXp-mo" id="MJXp-Span-1055" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1056">y</span><span class="MJXp-mtext" id="MJXp-Span-1057">&nbsp;</span><span class="MJXp-mrow" id="MJXp-Span-1058"><span class="MJXp-mo" id="MJXp-Span-1059" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mtext" id="MJXp-Span-1060">&nbsp;</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1061">x</span><span class="MJXp-mo" id="MJXp-Span-1062" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-209-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-209">p(\boldsymbol{y} \space | \space \boldsymbol{x})</script></span> bởi cả hai đều tham chiếu đến các yếu tố cơ bản giống nhau. Mục 15.4 giải thích về việc làm cách nào quan điểm này thúc đẩy việc sử dụng biểu diễn phân tán với các hướng tách biệt trong không gian biểu diễn tương ứng với các yếu tố tách biệt của biến thể.</p>
</li>
<li>
<p>Yếu tố nhân quả (Causal factors): Mô hình được xây dựng theo cách này coi các yếu tố được mô tả bằng phép biểu diễn đã học <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1063"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1064">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-210-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-210">\boldsymbol{h}</script></span> như một hệ quả của dữ liệu quan sát được <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1065"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1066">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-211-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-211">\boldsymbol{x}</script></span>. Như đã thảo luận ở mục 15.3, đây là một ưu điểm cho mạng bán giám sát và làm cho mô hình đã học mạnh hơn khi các phân phối trên các thay đổi nguyên nhân cơ bản hoặc khi ta sử dụng mô hình cho nhiều nhiệm vụ.</p>
</li>
<li>
<p>Các yếu tố được chia sẻ giữa các nhiệm vụ: Khi chúng ta có nhiều nhiệm vụ tương ứng với các biến <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1067"><span class="MJXp-msubsup" id="MJXp-Span-1068"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1069" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-1070" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-212-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-212">y_i</script></span> khác nhau cùng chia sẻ một đầu vào <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1071"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1072">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-213-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-213">\boldsymbol{x}</script></span> hoặc khi mỗi nhiệm vụ được kết hợp với một tập con hoặc 1 hàm <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1073"><span class="MJXp-msubsup" id="MJXp-Span-1074"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1075" style="margin-right: 0.05em;">f</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-1076" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-1077">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1078">i</span><span class="MJXp-mo" id="MJXp-Span-1079">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-1080" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1081">x</span><span class="MJXp-mo" id="MJXp-Span-1082" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-214-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-214">f^{(i)}(\boldsymbol{x})</script></span> của các giá trị đầu vào toàn cục <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1083"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1084">x</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-215-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-215">\boldsymbol{x}</script></span>. Giả sử rằng mỗi <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1085"><span class="MJXp-msubsup" id="MJXp-Span-1086"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1087" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-1088" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-216-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-216">y_i</script></span> được kết hợp với một tập con khác nhau trong một tập chung các yêu tố tương quan <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1089"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1090">h</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-217-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-217">\boldsymbol{h}</script></span>. Bởi các tập con này là chồng lên nhau, việc học toàn bộ <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1091"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1092">P</span><span class="MJXp-mo" id="MJXp-Span-1093" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-1094"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1095" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-1096" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mtext" id="MJXp-Span-1097">&nbsp;</span><span class="MJXp-mrow" id="MJXp-Span-1098"><span class="MJXp-mo" id="MJXp-Span-1099" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mtext" id="MJXp-Span-1100">&nbsp;</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1101">x</span><span class="MJXp-mo" id="MJXp-Span-1102" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-218-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-218">P(y_i\space | \space \boldsymbol{x})</script></span> thông qua một phép biểu diễn trung gian <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1103"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1104">P</span><span class="MJXp-mo" id="MJXp-Span-1105" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1106">h</span><span class="MJXp-mtext" id="MJXp-Span-1107">&nbsp;</span><span class="MJXp-mrow" id="MJXp-Span-1108"><span class="MJXp-mo" id="MJXp-Span-1109" style="margin-left: 0.167em; margin-right: 0.167em;">|</span></span><span class="MJXp-mtext" id="MJXp-Span-1110">&nbsp;</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1111">x</span><span class="MJXp-mo" id="MJXp-Span-1112" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-219-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-219">P(\boldsymbol{h}\space | \space \boldsymbol{x})</script></span> cho phép chia sẻ các điểm mạnh về mặt thống kê giữa các nhiệm vụ.</p>
</li>
<li>
<p>Đa tạp (Manifolds): Khối xác suất tập trung, và phạm vi của nó trong đó nó tập trung tại các liên kết cục bộ và chiếm một số lượng nhỏ. Trong trường hợp liên tục, phạm vi này có thể được xấp xỉ bằng một đa tạp có số chiều nhỏ, với kích thước nhỏ hơn nhiều so với vùng không gian cơ sở của dữ liệu. Có nhiều thuật toán máy học chỉ hoạt động một cách hợp lý trên các đa tạp này (Goodfellow et al., 2014b). Một vài thuật toán học máy, đặc biệt là bộ mã hóa tự động dùng để tìm hiểu cấu trúc đa tạp một cách rõ ràng.</p>
</li>
<li>
<p>Phân lớp tự nhiên: Nhiều thuật toán máy học giả sử rằng mỗi đa tạp liên kết trong một không gian đầu vào có thể được ấn định vào một lớp cụ thể. Dữ liệu có thể nằm trên nhiều đa tạp không liên kết, nhưng các lớp vẫn không thay đổi. Giả sử này đã thúc đẩy rất nhiều các thuật toán học máy bao gồm lan truyền tiếp tuyến (tangent propagation), lan truyền ngược kép (double backprop), bộ phân lớp tiếp tuyến đa tạp (the manifoldtangent classiﬁer), và phương pháp huấn luyện đối nghịch (adversarial training).</p>
</li>
<li>
<p>Tính gắn kết về không gian và thời gian: phép phân tích đặc trưng chậm và các thuật toán liên quan giả sử rằng các yếu tố giải thích quan trọng nhất sẽ thay đổi theo thời gian, hoặc ít nhất sẽ dễ dự đoán chính xác các yếu tố giải thích cơ bản hơn so với dự đoán về dữ liệu quan sát ở dạng thô như giá trị pixel của ảnh. Xem mục 13.3 để hiểu chi tiết hơn về hướng tiếp cận này.</p>
</li>
<li>
<p>Tính rải rác: Hầu hết các đặc trưng có lẽ không liên quan đến các mô tả dữ liệu đầu vào, do đó không cần phải sử dụng một đặc trưng để phát hiện vòi của một con voi khi biểu diễn một ảnh về một con mèo. Do đó, sẽ hợp lý khi áp đặt một tiên nghiệm mà bất kỳ đặc trưng nào cũng có thể được giải thích như việc “có mặt” hay “vắng mặt” hầu hết không xuất hiện.</p>
</li>
<li>
<p>Tính đơn giản của các yếu tố phụ thuộc: trong phép biểu diễn tốt ở mức cao, các yếu tố được liên kết với các yếu tố khác thông qua các phụ thuộc đơn giản. Đơn giản nhất có thể là các phụ thuộc cận biên, <span class="mathjax"><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1113"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1114">P</span><span class="MJXp-mo" id="MJXp-Span-1115" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1116">h</span><span class="MJXp-mo" id="MJXp-Span-1117" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-1118" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-1119"><span class="MJXp-mo" id="MJXp-Span-1120" style="margin-left: 0.111em; margin-right: 0.05em;">∏</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-1121" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-1122">P</span><span class="MJXp-mo" id="MJXp-Span-1123" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-1124"><span class="MJXp-mi MJXp-bold MJXp-italic" id="MJXp-Span-1125" style="margin-right: 0.05em;">h</span><span class="MJXp-mi MJXp-bold MJXp-italic MJXp-script" id="MJXp-Span-1126" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-1127" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax MathJax_Processing" id="MathJax-Element-220-Frame" tabindex="0"></span><script type="math/tex" id="MathJax-Element-220">P(\boldsymbol{h}) = \prod_i P(\boldsymbol{h_i})</script></span>, tuy nhiên thì các phụ thuộc tuyến tính cũng là các giả sử hợp lý. Điều này có thể được thấy thông qua các định luật vật lý và được giả sử khi đưa một mô hình dự đoán tuyến tính hoặc một tiên nghiệm ở phía đầu của phép biểu diễn đã học.</p>
</li>
</ul><p>Khái niệm về học biểu diễn ràng buộc tất cả các dạng của mô hình học sâu với nhau. Mạng lan truyền thuận và mạng hồi quy, bộ mã hóa tự động và mô hình xác suất chiều sâu, tất cả đều học và triển khai phép biểu diễn. Học cách biểu diễn tốt nhất có thể vẫn là một trong những mảng nghiên cứu thú vị.</p><p>---------- Thomas Nguyen kết thúc dịch ở đây --------------<br>
---------- Xuan-Son Vu đã hiệu chỉnh đến đây --------------</p></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-15-Học-biểu-diễn-Representation-Learning" title="Chương 15: Học biểu diễn (Representation Learning)">Chương 15: Học biểu diễn (Representation Learning)</a></li>
<li><a href="#151-Tiền-huấn-luyện-tham-lam-theo-lớp-không-giám-sát-Greedy-Layer-Wise-Unsupervised-Pretraining" title="15.1 Tiền huấn luyện tham lam theo lớp không giám sát (Greedy Layer-Wise Unsupervised Pretraining)">15.1 Tiền huấn luyện tham lam theo lớp không giám sát (Greedy Layer-Wise Unsupervised Pretraining)</a><ul class="nav">
<li><a href="#1511-Khi-nào-và-tại-sao-tiền-huấn-luyện-không-giám-sát-hoạt-động" title="15.1.1 Khi nào và tại sao tiền huấn luyện không giám sát hoạt động?">15.1.1 Khi nào và tại sao tiền huấn luyện không giám sát hoạt động?</a></li>
<li><a href="#f-leftarrow-Indentity-function-tilde--mathbf--X----mathbf--X--for-k--1-ldotsm-do-nbspnbspnbspnbspfk--mathcal--L-tilde--mathbf--X----nbspnbspnbspnbspf-leftarrow-ff-circ-f-nbspnbspnbspnbsptilde--mathbf--X---leftarrow-fftilde--mathbf--X----end-for-if-fine-tuning-then-nbspnbspnbspnbspf-leftarrow-mathcalTfmathbf--X-mathbf--Y--end-if-Return-f" title="f \leftarrowf \leftarrow Indentity function
\tilde { \mathbf { X } } = \mathbf { X }\tilde { \mathbf { X } } = \mathbf { X }
for k = 1, \ldots,mk = 1, \ldots,m do
&nbsp;&nbsp;&nbsp;&nbsp;f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow f^{(f)} \circ ff \leftarrow f^{(f)} \circ f
&nbsp;&nbsp;&nbsp;&nbsp;\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )
end for
if fine-tuning then
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })
end if
Return ff">f \leftarrowf \leftarrow Indentity function
\tilde { \mathbf { X } } = \mathbf { X }\tilde { \mathbf { X } } = \mathbf { X }
for k = 1, \ldots,mk = 1, \ldots,m do
&nbsp;&nbsp;&nbsp;&nbsp;f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow f^{(f)} \circ ff \leftarrow f^{(f)} \circ f
&nbsp;&nbsp;&nbsp;&nbsp;\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )
end for
if fine-tuning then
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })
end if
Return ff</a></li>
</ul>
</li>
<li><a href="#152-Học-chuyển-tiếp-và-thích-ứng-miền" title="15.2 Học chuyển tiếp và thích ứng miền">15.2 Học chuyển tiếp và thích ứng miền</a></li>
<li><a href="#153-Học-bán-giám-sát-gỡ-rối-các-yếu-tố-nhân-quả-Semi-Supervised-Disentangling-of-Causal-Factors" title="15.3 Học bán giám sát gỡ rối các yếu tố nhân quả (Semi-Supervised Disentangling of Causal Factors)">15.3 Học bán giám sát gỡ rối các yếu tố nhân quả (Semi-Supervised Disentangling of Causal Factors)</a></li>
<li><a href="#154-Biểu-diễn-phân-phối-Distributed-Representation" title="15.4 Biểu diễn phân phối (Distributed Representation)">15.4 Biểu diễn phân phối (Distributed Representation)</a></li>
<li><a href="#155-Mức-độ-cải-thiện-theo-hàm-mũ-thu-được-từ-độ-sâu-của-mô-hình" title="15.5 Mức độ cải thiện theo hàm mũ thu được từ độ sâu của mô hình">15.5 Mức độ cải thiện theo hàm mũ thu được từ độ sâu của mô hình</a></li>
<li><a href="#156-Cung-cấp-các-dẫn-chứng-để-tìm-hiểu-nguyên-nhân-sâu-hơn" title="15.6 Cung cấp các dẫn chứng để tìm hiểu nguyên nhân sâu hơn">15.6 Cung cấp các dẫn chứng để tìm hiểu nguyên nhân sâu hơn</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;"  >
        <div class="toc"><ul class="nav">
<li class=""><a href="#Chương-15-Học-biểu-diễn-Representation-Learning" title="Chương 15: Học biểu diễn (Representation Learning)">Chương 15: Học biểu diễn (Representation Learning)</a></li>
<li><a href="#151-Tiền-huấn-luyện-tham-lam-theo-lớp-không-giám-sát-Greedy-Layer-Wise-Unsupervised-Pretraining" title="15.1 Tiền huấn luyện tham lam theo lớp không giám sát (Greedy Layer-Wise Unsupervised Pretraining)">15.1 Tiền huấn luyện tham lam theo lớp không giám sát (Greedy Layer-Wise Unsupervised Pretraining)</a><ul class="nav">
<li><a href="#1511-Khi-nào-và-tại-sao-tiền-huấn-luyện-không-giám-sát-hoạt-động" title="15.1.1 Khi nào và tại sao tiền huấn luyện không giám sát hoạt động?">15.1.1 Khi nào và tại sao tiền huấn luyện không giám sát hoạt động?</a></li>
<li><a href="#f-leftarrow-Indentity-function-tilde--mathbf--X----mathbf--X--for-k--1-ldotsm-do-nbspnbspnbspnbspfk--mathcal--L-tilde--mathbf--X----nbspnbspnbspnbspf-leftarrow-ff-circ-f-nbspnbspnbspnbsptilde--mathbf--X---leftarrow-fftilde--mathbf--X----end-for-if-fine-tuning-then-nbspnbspnbspnbspf-leftarrow-mathcalTfmathbf--X-mathbf--Y--end-if-Return-f" title="f \leftarrowf \leftarrow Indentity function
\tilde { \mathbf { X } } = \mathbf { X }\tilde { \mathbf { X } } = \mathbf { X }
for k = 1, \ldots,mk = 1, \ldots,m do
&nbsp;&nbsp;&nbsp;&nbsp;f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow f^{(f)} \circ ff \leftarrow f^{(f)} \circ f
&nbsp;&nbsp;&nbsp;&nbsp;\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )
end for
if fine-tuning then
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })
end if
Return ff">f \leftarrowf \leftarrow Indentity function
\tilde { \mathbf { X } } = \mathbf { X }\tilde { \mathbf { X } } = \mathbf { X }
for k = 1, \ldots,mk = 1, \ldots,m do
&nbsp;&nbsp;&nbsp;&nbsp;f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )f^{(k)} = \mathcal { L }(\tilde { \mathbf { X } } )
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow f^{(f)} \circ ff \leftarrow f^{(f)} \circ f
&nbsp;&nbsp;&nbsp;&nbsp;\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )\tilde { \mathbf { X } }  \leftarrow f^{(f)}(\tilde { \mathbf { X } } )
end for
if fine-tuning then
&nbsp;&nbsp;&nbsp;&nbsp;f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })f \leftarrow \mathcal{T}(f,\mathbf { X },\mathbf { Y })
end if
Return ff</a></li>
</ul>
</li>
<li><a href="#152-Học-chuyển-tiếp-và-thích-ứng-miền" title="15.2 Học chuyển tiếp và thích ứng miền">15.2 Học chuyển tiếp và thích ứng miền</a></li>
<li><a href="#153-Học-bán-giám-sát-gỡ-rối-các-yếu-tố-nhân-quả-Semi-Supervised-Disentangling-of-Causal-Factors" title="15.3 Học bán giám sát gỡ rối các yếu tố nhân quả (Semi-Supervised Disentangling of Causal Factors)">15.3 Học bán giám sát gỡ rối các yếu tố nhân quả (Semi-Supervised Disentangling of Causal Factors)</a></li>
<li><a href="#154-Biểu-diễn-phân-phối-Distributed-Representation" title="15.4 Biểu diễn phân phối (Distributed Representation)">15.4 Biểu diễn phân phối (Distributed Representation)</a></li>
<li><a href="#155-Mức-độ-cải-thiện-theo-hàm-mũ-thu-được-từ-độ-sâu-của-mô-hình" title="15.5 Mức độ cải thiện theo hàm mũ thu được từ độ sâu của mô hình">15.5 Mức độ cải thiện theo hàm mũ thu được từ độ sâu của mô hình</a></li>
<li><a href="#156-Cung-cấp-các-dẫn-chứng-để-tìm-hiểu-nguyên-nhân-sâu-hơn" title="15.6 Cung cấp các dẫn chứng để tìm hiểu nguyên nhân sâu hơn">15.6 Cung cấp các dẫn chứng để tìm hiểu nguyên nhân sâu hơn</a></li>
</ul>
</div><div class="toc-menu"><a class="expand-toggle" href="#">Expand all</a><a class="back-to-top" href="#">Back to top</a><a class="go-to-bottom" href="#">Go to bottom</a></div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
